### Exploring Fine Tuning with Unsloth

This repository contains Jupyter notebooks for fine-tuning large language models using [Unsloth](https://github.com/unslothai/unsloth), an open-source library that provides 2x faster training with significantly less VRAM than standard approaches.

## Claude Prompt

The initial prompt submitted to Claude Opus 4.6:


    Create a notebook the explores finetuning a large language model using Unsloth. The Unsloth repository can be found at https://github.com/unslothai/unsloth. The creators of Unsloth have a repository of notebooks that demonstrate how to properly use Unsloth for many different models and use cases. 
    The model I want you to focus on can be found in the notebook https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb. I want you to use this notebook as your guide on how to properly fine tune the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" model. 

    The fine tuning data consists of books written by Jordan Peterson all in pdf format. These books can be found in the "FineTuning/Books/JordanPeterson" sub folder. I want you to create a notebook in the "FineTuning/NoteBooks/JordanPeterson" sub folder that reads from all of the books to fine tune the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" model to create a new fine tuned model. 

    I know very little about how to do this, so be very detailed in explaining everything you do and why you did it. 

    The python environment '.finetuning' has been created using uv for all of this work and already has unsloth installed to it, plus other resources that may be needed. If any other resources are needed for this effort, use this environment to install those packages. 
    The 'FineTuning' folder is a github repository that has been cloned from 'git@github.com:rkaunismaa/FineTuning.git'


## Setup

The project uses a `uv`-managed Python 3.12 virtual environment (`.finetuning`) with Unsloth and its dependencies pre-installed.

**Hardware**: NVIDIA RTX 4090 (24GB VRAM)

## Notebooks

| Notebook | Model | Data Source | Description |
|----------|-------|-------------|-------------|
| [GPT-OSS 20B Jordan Peterson — Fine-Tuning](NoteBooks/JordanPeterson/GPT_OSS_20B_JordanPeterson_FineTuning.ipynb) | `unsloth/gpt-oss-20b-unsloth-bnb-4bit` | Jordan Peterson's books (4 PDFs) | Fine-tunes GPT-OSS 20B using LoRA on text extracted from Peterson's writings. Covers PDF extraction, dataset creation, training, inference, and model saving. |
| [GPT-OSS 20B Jordan Peterson — Comparison](NoteBooks/JordanPeterson/GPT_OSS_20B_JordanPeterson_Comparison.ipynb) | Base vs. fine-tuned | Same 4 PDFs (held-out passages) | Quantitative comparison between the base and fine-tuned models across 6 metrics (perplexity, TF-IDF similarity, keyword density, TTR, response length, word clouds) with 7 visualizations. |
| [Qwen3-14B Jordan Peterson — Fine-Tuning](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_FineTuning.ipynb) | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | Jordan Peterson's books (4 PDFs) | Fine-tunes Qwen3-14B using LoRA. Demonstrates Qwen3's ChatML format, `enable_thinking` parameter, and dual inference modes (chat + thinking). ~23 min training, loss 2.44. |
| [Qwen3-14B Jordan Peterson — Comparison](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_Comparison.ipynb) | Base vs. fine-tuned | Same 4 PDFs (held-out passages) | Quantitative comparison of Qwen3-14B base vs. fine-tuned across 6 metrics. Perplexity drops 33% (strong domain adaptation); other metrics reflect 1-epoch training artifacts. 7 visualizations. |
| [All Models — Cross-Architecture Comparison](NoteBooks/JordanPeterson/AllModels_JordanPeterson_Comparison.ipynb) | GPT-OSS 20B (base + FT) vs. Qwen3-14B (base + FT) | Same 4 PDFs (held-out passages) | Full cross-architecture comparison of all 4 model variants side-by-side. Covers intra-architecture fine-tuning effectiveness, inter-architecture base and fine-tuned comparisons, and training efficiency analysis. 7 visualizations. |
| [Qwen3-14B Jordan Peterson — Fine-Tuning V2](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_V2_FineTuning.ipynb) | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | Synthetic Q&A pairs from 4 PDFs (generated by Claude Haiku) | Version 2 fine-tuning: addresses V1's passage-regurgitation problem with synthetic Q&A training data, 3 epochs, and LoRA r=32. Includes detailed post-mortem of V1 failures and full technical rationale for every hyperparameter. |

## Project Structure

```
FineTuning/
├── Books/                          # Training data (PDFs, gitignored)
│   └── JordanPeterson/             # 4 books by Jordan Peterson
├── NoteBooks/                      # Fine-tuning notebooks
│   └── JordanPeterson/             # Peterson-focused fine-tuning
│       ├── outputs/                # Saved LoRA adapters (gitignored)
│       ├── qa_dataset/             # Synthetic Q&A cache (gitignored)
│       ├── comparison_cache*/      # Inference pkl caches (gitignored)
│       └── comparison_figures*/    # Generated chart PNGs
├── .gitignore
├── CLAUDE.md                       # AI assistant context file
└── README.MD                       # This file
```

## Approach

### Fine-Tuning Workflow

#### V1 — Passage Completion (GPT-OSS 20B, Qwen3-14B)

1. **Extract text** from source PDFs using PyMuPDF
2. **Chunk** into ~350-word passages with 50-word overlap
3. **Format** as passage-completion conversations: `(user: passage fragment) → (assistant: continuation)`
4. **Load** a 4-bit quantized model via Unsloth's `FastLanguageModel`
5. **Add LoRA adapters** (r=16) for parameter-efficient fine-tuning
6. **Train** 1 epoch using `SFTTrainer` with response-only masking
7. **Test** and **save** the LoRA adapter

**V1 limitation**: training on passage completion teaches the model to *continue* book text, not to *answer questions*. After 1 epoch the model regurgitates passages instead of generating coherent responses.

#### V2 — Synthetic Q&A (Qwen3-14B V2)

1. **Extract text** from PDFs — same as V1
2. **Generate questions** using Claude Haiku API: 2 questions per passage that the passage answers (~$1–3 total)
3. **Format** as instruction pairs: `(system: Peterson persona) + (user: question) + (assistant: verbatim passage)`
4. **Load** Qwen3-14B with LoRA r=32 (doubled capacity vs V1)
5. **Train** for **3 epochs** — crosses from memorisation into stylistic generalisation
6. **Test** and **save** the adapter to `outputs/qwen3_14b_peterson_v2_lora/`

The key insight: the *questions* are synthetic (generated by Claude Haiku) but the *answers* are verbatim Peterson — authentic, unmodified text from his books. The training task now matches the inference task.

### Comparison Workflow

#### Claude Prompt

The comparison prompt submitted to Claude Opus 4.6:

    Create a notebook that compares the performance of the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" base model and the model fine tuned on the 4 books from Jordan Peterson. Use whatever quantitative measurements are available that will be able to visually display how the fine tuned model is more Jordan Peterson like than the base model. Be detailed in your explanations and provide visuals. 

The comparison notebook measures domain adaptation across six independent axes:

| Metric | What It Measures |
|--------|-----------------|
| **Perplexity** | How surprised the model is by Peterson's actual prose (lower = better) |
| **TF-IDF Cosine Similarity** | Vocabulary overlap between responses and real Peterson passages |
| **Keyword Density** | Frequency of Peterson's characteristic terms (chaos, order, meaning, hero…) |
| **Type-Token Ratio** | Vocabulary richness — fraction of unique words per response |
| **Response Length** | Average words per response (Peterson is verbose) |
| **Word Cloud** | Visual of which words dominate each model's responses |

Both models are loaded and evaluated sequentially (memory constraint: two 20B models can't fit in 24GB simultaneously). Results are cached to `comparison_cache/` as pickle files so inference only runs once.

### Training Results Summary

| Model | Data | Epochs | Steps | Time | Loss | Peak VRAM |
|-------|------|--------|-------|------|------|-----------|
| GPT-OSS 20B V1 | Passage completion | 1 | 641 | 73.3 min | 3.01 | 13.8 GB |
| Qwen3-14B V1 | Passage completion | 1 | 321 | 23.3 min | 2.44 | 13.4 GB |
| Qwen3-14B V2 | Synthetic Q&A | 3 | ~900 | ~90 min | TBD | ~13.5 GB |

## References

- [Unsloth](https://github.com/unslothai/unsloth) - Fast fine-tuning library
- [Unsloth Notebooks](https://github.com/unslothai/notebooks) - Official example notebooks
- [GPT-OSS 20B Fine-tuning Reference](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb) - Reference notebook used as the basis for our implementation
- [Qwen3 Fine-tuning Reference](https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb) - Reference notebook used for Qwen3 implementation
