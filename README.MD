### Exploring Fine Tuning with Unsloth

This repository contains Jupyter notebooks for fine-tuning large language models using [Unsloth](https://github.com/unslothai/unsloth), an open-source library that provides 2x faster training with significantly less VRAM than standard approaches.

## Claude Prompt

The initial prompt submitted to Claude Opus 4.6:


    Create a notebook the explores finetuning a large language model using Unsloth. The Unsloth repository can be found at https://github.com/unslothai/unsloth. The creators of Unsloth have a repository of notebooks that demonstrate how to properly use Unsloth for many different models and use cases. 
    The model I want you to focus on can be found in the notebook https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb. I want you to use this notebook as your guide on how to properly fine tune the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" model. 

    The fine tuning data consists of books written by Jordan Peterson all in pdf format. These books can be found in the "FineTuning/Books/JordanPeterson" sub folder. I want you to create a notebook in the "FineTuning/NoteBooks/JordanPeterson" sub folder that reads from all of the books to fine tune the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" model to create a new fine tuned model. 

    I know very little about how to do this, so be very detailed in explaining everything you do and why you did it. 

    The python environment '.finetuning' has been created using uv for all of this work and already has unsloth installed to it, plus other resources that may be needed. If any other resources are needed for this effort, use this environment to install those packages. 
    The 'FineTuning' folder is a github repository that has been cloned from 'git@github.com:rkaunismaa/FineTuning.git'


## Setup

The project uses a `uv`-managed Python 3.12 virtual environment (`.finetuning`) with Unsloth and its dependencies pre-installed.

**Hardware**: NVIDIA RTX 4090 (24GB VRAM)

## Notebooks

| Notebook | Model | Data Source | Description |
|----------|-------|-------------|-------------|
| [GPT-OSS 20B Jordan Peterson — Fine-Tuning](NoteBooks/JordanPeterson/GPT_OSS_20B_JordanPeterson_FineTuning.ipynb) | `unsloth/gpt-oss-20b-unsloth-bnb-4bit` | Jordan Peterson's books (4 PDFs) | Fine-tunes GPT-OSS 20B using LoRA on text extracted from Peterson's writings. Covers PDF extraction, dataset creation, training, inference, and model saving. |
| [GPT-OSS 20B Jordan Peterson — Comparison](NoteBooks/JordanPeterson/GPT_OSS_20B_JordanPeterson_Comparison.ipynb) | Base vs. fine-tuned | Same 4 PDFs (held-out passages) | Quantitative comparison between the base and fine-tuned models across 6 metrics (perplexity, TF-IDF similarity, keyword density, TTR, response length, word clouds) with 7 visualizations. |
| [Qwen3-14B Jordan Peterson — Fine-Tuning](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_FineTuning.ipynb) | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | Jordan Peterson's books (4 PDFs) | Fine-tunes Qwen3-14B using LoRA. Demonstrates Qwen3's ChatML format, `enable_thinking` parameter, and dual inference modes (chat + thinking). ~23 min training, loss 2.44. |
| [Qwen3-14B Jordan Peterson — Comparison](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_Comparison.ipynb) | Base vs. fine-tuned | Same 4 PDFs (held-out passages) | Quantitative comparison of Qwen3-14B base vs. fine-tuned across 6 metrics. Perplexity drops 33% (strong domain adaptation); other metrics reflect 1-epoch training artifacts. 7 visualizations. |
| [All Models — Cross-Architecture Comparison](NoteBooks/JordanPeterson/AllModels_JordanPeterson_Comparison.ipynb) | GPT-OSS 20B (base + FT) vs. Qwen3-14B (base + FT) | Same 4 PDFs (held-out passages) | Full cross-architecture comparison of all 4 model variants side-by-side. Covers intra-architecture fine-tuning effectiveness, inter-architecture base and fine-tuned comparisons, and training efficiency analysis. 7 visualizations. |
| [Qwen3-14B Jordan Peterson — Fine-Tuning V2](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_V2_FineTuning.ipynb) | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | Synthetic Q&A pairs from 4 PDFs (generated by Claude Haiku) | Version 2 fine-tuning: addresses V1's passage-regurgitation problem with synthetic Q&A training data, 3 epochs, and LoRA r=32. Includes detailed post-mortem of V1 failures and full technical rationale for every hyperparameter. |

## Project Structure

```
FineTuning/
├── Books/                  # Training data (PDFs, gitignored)
│   └── JordanPeterson/     # 4 books by Jordan Peterson
├── NoteBooks/              # Fine-tuning notebooks
│   └── JordanPeterson/     # Peterson-focused fine-tuning
├── .gitignore
├── CLAUDE.md               # AI assistant context file
└── README.MD               # This file
```

## Approach

### Fine-Tuning Workflow

Each fine-tuning notebook follows this general workflow:

1. **Extract text** from source PDFs using PyMuPDF
2. **Create a training dataset** by chunking text into passages and formatting as conversations
3. **Load a quantized model** (4-bit via bitsandbytes) using Unsloth's `FastLanguageModel`
4. **Add LoRA adapters** for parameter-efficient fine-tuning (~0.04% of parameters trained)
5. **Train** using `SFTTrainer` with response-only masking
6. **Test** the fine-tuned model with relevant prompts
7. **Save** the LoRA adapters for later use

### Comparison Workflow

#### Claude Prompt

The comparison prompt submitted to Claude Opus 4.6:

    Create a notebook that compares the performance of the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" base model and the model fine tuned on the 4 books from Jordan Peterson. Use whatever quantitative measurements are available that will be able to visually display how the fine tuned model is more Jordan Peterson like than the base model. Be detailed in your explanations and provide visuals. 

The comparison notebook measures domain adaptation across six independent axes:

| Metric | What It Measures |
|--------|-----------------|
| **Perplexity** | How surprised the model is by Peterson's actual prose (lower = better) |
| **TF-IDF Cosine Similarity** | Vocabulary overlap between responses and real Peterson passages |
| **Keyword Density** | Frequency of Peterson's characteristic terms (chaos, order, meaning, hero…) |
| **Type-Token Ratio** | Vocabulary richness — fraction of unique words per response |
| **Response Length** | Average words per response (Peterson is verbose) |
| **Word Cloud** | Visual of which words dominate each model's responses |

Both models are loaded and evaluated sequentially (memory constraint: two 20B models can't fit in 24GB simultaneously). Results are cached to `comparison_cache/` as pickle files so inference only runs once.

## References

- [Unsloth](https://github.com/unslothai/unsloth) - Fast fine-tuning library
- [Unsloth Notebooks](https://github.com/unslothai/notebooks) - Official example notebooks
- [GPT-OSS 20B Fine-tuning Reference](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb) - Reference notebook used as the basis for our implementation
