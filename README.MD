### Exploring Fine Tuning with Unsloth

This repository contains Jupyter notebooks for fine-tuning large language models using [Unsloth](https://github.com/unslothai/unsloth), an open-source library that provides 2x faster training with significantly less VRAM than standard approaches.

## Claude Prompt

The initial prompt submitted to Claude Opus 4.6:


    Create a notebook the explores finetuning a large language model using Unsloth. The Unsloth repository can be found at https://github.com/unslothai/unsloth. The creators of Unsloth have a repository of notebooks that demonstrate how to properly use Unsloth for many different models and use cases. 
    The model I want you to focus on can be found in the notebook https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb. I want you to use this notebook as your guide on how to properly fine tune the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" model. 

    The fine tuning data consists of books written by Jordan Peterson all in pdf format. These books can be found in the "FineTuning/Books/JordanPeterson" sub folder. I want you to create a notebook in the "FineTuning/NoteBooks/JordanPeterson" sub folder that reads from all of the books to fine tune the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" model to create a new fine tuned model. 

    I know very little about how to do this, so be very detailed in explaining everything you do and why you did it. 

    The python environment '.finetuning' has been created using uv for all of this work and already has unsloth installed to it, plus other resources that may be needed. If any other resources are needed for this effort, use this environment to install those packages. 
    The 'FineTuning' folder is a github repository that has been cloned from 'git@github.com:rkaunismaa/FineTuning.git'


## Setup

The project uses a `uv`-managed Python 3.12 virtual environment (`.finetuning`) with Unsloth and its dependencies pre-installed.

**Hardware**: NVIDIA RTX 4090 (24GB VRAM)

## Notebooks

| Notebook | Model | Data Source | Description |
|----------|-------|-------------|-------------|
| [GPT-OSS 20B Jordan Peterson — Fine-Tuning](NoteBooks/JordanPeterson/GPT_OSS_20B_JordanPeterson_FineTuning.ipynb) | `unsloth/gpt-oss-20b-unsloth-bnb-4bit` | Jordan Peterson's books (4 PDFs) | Fine-tunes GPT-OSS 20B using LoRA on text extracted from Peterson's writings. Covers PDF extraction, dataset creation, training, inference, and model saving. |
| [GPT-OSS 20B Jordan Peterson — Comparison](NoteBooks/JordanPeterson/GPT_OSS_20B_JordanPeterson_Comparison.ipynb) | Base vs. fine-tuned | Same 4 PDFs (held-out passages) | Quantitative comparison between the base and fine-tuned models across 6 metrics (perplexity, TF-IDF similarity, keyword density, TTR, response length, word clouds) with 7 visualizations. |
| [Qwen3-14B Jordan Peterson — Fine-Tuning](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_FineTuning.ipynb) | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | Jordan Peterson's books (4 PDFs) | Fine-tunes Qwen3-14B using LoRA. Demonstrates Qwen3's ChatML format, `enable_thinking` parameter, and dual inference modes (chat + thinking). ~23 min training, loss 2.44. |
| [Qwen3-14B Jordan Peterson — Comparison](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_Comparison.ipynb) | Base vs. fine-tuned | Same 4 PDFs (held-out passages) | Quantitative comparison of Qwen3-14B base vs. fine-tuned across 6 metrics. Perplexity drops 33% (strong domain adaptation); other metrics reflect 1-epoch training artifacts. 7 visualizations. |
| [All Models — Cross-Architecture Comparison](NoteBooks/JordanPeterson/AllModels_JordanPeterson_Comparison.ipynb) | GPT-OSS 20B (base + FT) vs. Qwen3-14B (base + FT) | Same 4 PDFs (held-out passages) | Full cross-architecture comparison of all 4 model variants side-by-side. Covers intra-architecture fine-tuning effectiveness, inter-architecture base and fine-tuned comparisons, and training efficiency analysis. 7 visualizations. |
| [Qwen3-14B Jordan Peterson — Fine-Tuning V2](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_V2_FineTuning.ipynb) | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | Synthetic Q&A pairs from 4 PDFs (generated by Claude Haiku) | Version 2 fine-tuning: addresses V1's passage-regurgitation problem with synthetic Q&A training data, 3 epochs, and LoRA r=32. Includes detailed post-mortem of V1 failures and full technical rationale for every hyperparameter. |
| [Qwen3-32B Jordan Peterson — Fine-Tuning](NoteBooks/JordanPeterson/Qwen3_32B_JordanPeterson_FineTuning.ipynb) | `unsloth/Qwen3-32B-unsloth-bnb-4bit` | Synthetic Q&A pairs from 4 PDFs (shared cache with V2) | Scales the V2 approach to Qwen3's 32B dense model. Uses conservative VRAM settings (max_seq_length=1024, batch_size=1, grad_accum=8) to fit within 24 GB. Includes pre-load VRAM check, OOM handler with fallback options, and a conclusions section with instructions for adding 32B to the AllModels comparison. |
| [Qwen3-14B Jordan Peterson — Fine-Tuning V3](NoteBooks/JordanPeterson/Qwen3_14B_JordanPeterson_V3_FineTuning.ipynb) | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | Cleaner Q&A cache from DataPrep notebook (4,867 pairs, front-matter removed) | Version 3: standalone fine-tuning notebook (no dataset generation). Trains on the DataPrep notebook's cleaner Q&A cache. Same hyperparameters as V2 (r=32, 3 epochs). Results: 1,827 steps, loss 1.5258, 136.9 min, 15.3 GB peak VRAM. |

## Project Structure

```
FineTuning/
├── Books/                          # Training data (PDFs, gitignored)
│   └── JordanPeterson/             # 4 books by Jordan Peterson
├── NoteBooks/                      # Fine-tuning notebooks
│   └── JordanPeterson/             # Peterson-focused fine-tuning
│       ├── outputs/                # Saved LoRA adapters (gitignored)
│       ├── qa_dataset/             # Synthetic Q&A cache (gitignored)
│       ├── comparison_cache*/      # Inference pkl caches (gitignored)
│       └── comparison_figures*/    # Generated chart PNGs
├── .gitignore
├── CLAUDE.md                       # AI assistant context file
└── README.MD                       # This file
```

## Approach

### Fine-Tuning Workflow

#### V1 — Passage Completion (GPT-OSS 20B, Qwen3-14B)

1. **Extract text** from source PDFs using PyMuPDF
2. **Chunk** into ~350-word passages with 50-word overlap
3. **Format** as passage-completion conversations: `(user: passage fragment) → (assistant: continuation)`
4. **Load** a 4-bit quantized model via Unsloth's `FastLanguageModel`
5. **Add LoRA adapters** (r=16) for parameter-efficient fine-tuning
6. **Train** 1 epoch using `SFTTrainer` with response-only masking
7. **Test** and **save** the LoRA adapter

**V1 limitation**: training on passage completion teaches the model to *continue* book text, not to *answer questions*. After 1 epoch the model regurgitates passages instead of generating coherent responses.

#### V2 — Synthetic Q&A (Qwen3-14B V2)

1. **Extract text** from PDFs — same as V1
2. **Generate questions** using Claude Haiku API: 2 questions per passage that the passage answers (~$1–3 total)
3. **Format** as instruction pairs: `(system: Peterson persona) + (user: question) + (assistant: verbatim passage)`
4. **Load** Qwen3-14B with LoRA r=32 (doubled capacity vs V1)
5. **Train** for **3 epochs** — crosses from memorisation into stylistic generalisation
6. **Test** and **save** the adapter to `outputs/qwen3_14b_peterson_v2_lora/`

The key insight: the *questions* are synthetic (generated by Claude Haiku) but the *answers* are verbatim Peterson — authentic, unmodified text from his books. The training task now matches the inference task.

#### V3 — Cleaner Data (Qwen3-14B V3)

V3 separates dataset preparation from fine-tuning and trains on the cleaner output of the
standalone `JordanPeterson_DataPrep.ipynb` notebook (front-matter pages removed):

1. **Run DataPrep first** to generate `qa_dataset/peterson_qa.jsonl` (4,867 pairs)
2. **Load** the cache — no PDF extraction or question generation in this notebook
3. **Train** with identical hyperparameters to V2 (r=32, 3 epochs, 2e-4 LR)
4. **Save** adapter to `outputs/qwen3_14b_peterson_v3_lora/`

**What changed vs V2**: The dataset shrank by 162 pairs (5,029 → 4,867), all from boilerplate
pages. The biggest reduction was We Who Wrestle with God (−103 pairs) and 12 Rules for Life
(−48 pairs); Maps of Meaning was nearly unchanged (−5 pairs).

**Results**: Loss 1.5258 (vs V2's 1.5058 — negligible difference), 136.9 min (vs 144.0 min),
peak VRAM 15.3 GB (vs 15.5 GB). Training metrics are essentially identical to V2.

**Inference quality**: Four of five evaluation prompts produced substantive, on-topic passages
from Peterson's works — appropriate content but still passage-like rather than direct answers.
One prompt (personal responsibility) triggered an **index page** response, revealing that
back-matter content (book indexes) is still present in the training data. Front-matter removal
alone is insufficient; a back-matter removal step (detecting and dropping index/bibliography
pages) is the next data-quality improvement needed.

**Key lesson**: The largest remaining source of non-content training data is not front matter
but back matter — index entries, figure captions, and bibliography pages that exist throughout
and at the end of the books.

### Comparison Workflow

#### Claude Prompt

The comparison prompt submitted to Claude Opus 4.6:

    Create a notebook that compares the performance of the "unsloth/gpt-oss-20b-unsloth-bnb-4bit" base model and the model fine tuned on the 4 books from Jordan Peterson. Use whatever quantitative measurements are available that will be able to visually display how the fine tuned model is more Jordan Peterson like than the base model. Be detailed in your explanations and provide visuals. 

The comparison notebook measures domain adaptation across six independent axes:

| Metric | What It Measures |
|--------|-----------------|
| **Perplexity** | How surprised the model is by Peterson's actual prose (lower = better) |
| **TF-IDF Cosine Similarity** | Vocabulary overlap between responses and real Peterson passages |
| **Keyword Density** | Frequency of Peterson's characteristic terms (chaos, order, meaning, hero…) |
| **Type-Token Ratio** | Vocabulary richness — fraction of unique words per response |
| **Response Length** | Average words per response (Peterson is verbose) |
| **Word Cloud** | Visual of which words dominate each model's responses |

Both models are loaded and evaluated sequentially (memory constraint: two 20B models can't fit in 24GB simultaneously). Results are cached to `comparison_cache/` as pickle files so inference only runs once.

### Training Results Summary

| Model | Data | Epochs | Steps | Time | Loss | Peak VRAM | Adapter |
|-------|------|--------|-------|------|------|-----------|---------|
| GPT-OSS 20B V1 | Passage completion (~2,519 passages) | 1 | 641 | 73.3 min | 3.01 | 13.8 GB | ~260 MB |
| Qwen3-14B V1 | Passage completion (~2,519 passages) | 1 | 321 | 23.3 min | 2.44 | 13.4 GB | ~260 MB |
| Qwen3-14B V2 | Synthetic Q&A (5,029 pairs, w/ front matter) | 3 | 1,887 | 144.0 min | 1.5058 | 15.5 GB | 513.9 MB |
| Qwen3-14B V3 | Synthetic Q&A (4,867 pairs, front-matter removed) | 3 | 1,827 | 136.9 min | 1.5258 | 15.3 GB | 513.9 MB |
| Qwen3-32B | Synthetic Q&A (shared cache) | 3 | TBD | TBD | TBD | ~20–22 GB est. | TBD |

## References

- [Unsloth](https://github.com/unslothai/unsloth) - Fast fine-tuning library
- [Unsloth Notebooks](https://github.com/unslothai/notebooks) - Official example notebooks
- [GPT-OSS 20B Fine-tuning Reference](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb) - Reference notebook used as the basis for our implementation
- [Qwen3 Fine-tuning Reference](https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb) - Reference notebook used for Qwen3 implementation
