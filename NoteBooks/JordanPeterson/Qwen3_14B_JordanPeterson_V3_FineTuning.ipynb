{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903f15c1",
   "metadata": {},
   "source": [
    "# Qwen3-14B Jordan Peterson Fine-Tuning ‚Äî Version 3\n",
    "## Cleaner Q&A Data ¬∑ 3 Epochs ¬∑ r=32 LoRA\n",
    "\n",
    "This notebook fine-tunes Qwen3-14B on Jordan B. Peterson's four books using\n",
    "Version 3 of the training pipeline.\n",
    "\n",
    "**Pipeline position:** Run `JordanPeterson_DataPrep.ipynb` first to generate\n",
    "the Q&A cache (`qa_dataset/peterson_qa.jsonl`).  This notebook reads from that\n",
    "cache directly ‚Äî no PDF extraction or question generation happens here.\n",
    "\n",
    "**V3 = fine-tuning only.**  The dataset generation that was embedded in V2\n",
    "(Part 1) has been extracted into the standalone DataPrep notebook.  V3 trains\n",
    "on the same Haiku-generated Q&A pairs but with front-matter removal applied,\n",
    "producing a cleaner dataset (4,867 pairs instead of V2's 5,029)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769f984",
   "metadata": {},
   "source": [
    "---\n",
    "## Version History\n",
    "\n",
    "| Component | V1 | V2 | V3 |\n",
    "|-----------|----|----|-----|\n",
    "| Training task | Passage completion | Synthetic Q&A | Synthetic Q&A |\n",
    "| Dataset | Raw PDF text (with front matter) | Haiku Q&A (with front matter) | Haiku Q&A (clean ‚Äî no front matter) |\n",
    "| Passages | ~2,519 (all pages) | ~2,519 (all pages) | ~2,492 (front-matter removed) |\n",
    "| Q&A pairs | N/A | 5,029 | 4,867 (Haiku, front-matter-free) |\n",
    "| Epochs | 1 | 3 | 3 |\n",
    "| LoRA rank | r=16 | r=32 | r=32 |\n",
    "| LoRA alpha | 16 | 32 | 32 |\n",
    "| Output adapter | `qwen3_14b_‚Ä¶_v1_lora` | `qwen3_14b_peterson_v2_lora` | `qwen3_14b_peterson_v3_lora` |\n",
    "\n",
    "V3 uses the same hyperparameters as V2.  The only change is the dataset: the\n",
    "DataPrep notebook's improved front-matter removal is now applied consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813acf13",
   "metadata": {},
   "source": [
    "---\n",
    "## Why V3 Data Is Better Than V2's\n",
    "\n",
    "The V2 dataset was generated from the same extraction pass used in V1, which\n",
    "included front-matter pages.  The DataPrep notebook introduced a 3-tier\n",
    "heuristic to skip publisher pages before Chapter 1.  V3 trains on the output\n",
    "of that improved extraction.\n",
    "\n",
    "### Maps of Meaning ‚Äî the biggest improvement\n",
    "\n",
    "V2 had **1,826 pairs** including title pages, the ROUTLEDGE copyright block,\n",
    "and the full table of contents from page 0.  V3 starts from page 7 (the\n",
    "actual philosophical content), dropping those pages ‚Äî fewer pairs but all\n",
    "substantive.\n",
    "\n",
    "Without the front-matter fix, the very first training example paired a\n",
    "philosophical question with an answer that began:\n",
    "\n",
    "> \"MAPS OF MEANING MAPS OF MEANING The Architecture of Belief JORDAN B.PETERSON\n",
    "> ROUTLEDGE New York and London Published in 1999 by Routledge‚Ä¶\"\n",
    "\n",
    "The model was learning to associate philosophical questions with publisher\n",
    "metadata.\n",
    "\n",
    "### We Who Wrestle with God\n",
    "\n",
    "V2 had **1,367 pairs** starting from the Penguin copyright page (\"Penguin\n",
    "Random House values and supports copyright‚Ä¶\").  V3 skips those pages and\n",
    "starts from actual content.\n",
    "\n",
    "### Other books\n",
    "\n",
    "12 Rules for Life and Beyond Order saw minimal change (4‚Äì5% reduction) ‚Äî\n",
    "their front matter was shorter and the fix only removed a few boilerplate\n",
    "pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0052342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "PyTorch  : 2.10.0+cu128\n",
      "CUDA     : True  |  GPU: NVIDIA GeForce RTX 4090\n",
      "VRAM     : 25.3 GB\n",
      "\n",
      "Configuration:\n",
      "  Base model  : unsloth/Qwen3-14B-unsloth-bnb-4bit\n",
      "  LoRA rank   : r=32, alpha=32  (ratio=1.0)\n",
      "  Epochs      : 3\n",
      "  Batch (eff) : 2 x 4 = 8\n",
      "  LR          : 0.0002\n",
      "  QA cache    : qa_dataset/peterson_qa.jsonl\n",
      "  Output      : /home/rob/PythonEnvironments/FineTuning/FineTuning/NoteBooks/JordanPeterson/outputs/qwen3_14b_peterson_v3_lora\n"
     ]
    }
   ],
   "source": [
    "import json, re, math, time, gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# ‚îÄ‚îÄ Load shared config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# All paths and the system prompt are centralised in peterson_config.json so\n",
    "# they stay in sync across the DataPrep and fine-tuning notebooks.\n",
    "with open(\"peterson_config.json\") as f:\n",
    "    _config = json.load(f)\n",
    "\n",
    "QA_CACHE      = Path(_config[\"paths\"][\"qa_cache\"])\n",
    "SYSTEM_PROMPT = _config[\"system_prompt\"]\n",
    "\n",
    "# ‚îÄ‚îÄ V3 model + training constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "BASE_MODEL    = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\n",
    "OUTPUT_DIR    = Path(\"./outputs/qwen3_14b_peterson_v3_lora\")   # does NOT touch V2\n",
    "MAX_SEQ_LEN   = 2048\n",
    "LORA_RANK     = 32\n",
    "LORA_ALPHA    = 32\n",
    "BATCH_SIZE    = 2\n",
    "GRAD_ACCUM    = 4\n",
    "NUM_EPOCHS    = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS  = 30\n",
    "WEIGHT_DECAY  = 0.01\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch  : {torch.__version__}\")\n",
    "print(f\"CUDA     : {torch.cuda.is_available()}  |  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM     : {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
    "print()\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Base model  : {BASE_MODEL}\")\n",
    "print(f\"  LoRA rank   : r={LORA_RANK}, alpha={LORA_ALPHA}  (ratio={LORA_ALPHA/LORA_RANK:.1f})\")\n",
    "print(f\"  Epochs      : {NUM_EPOCHS}\")\n",
    "print(f\"  Batch (eff) : {BATCH_SIZE} x {GRAD_ACCUM} = {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print(f\"  LR          : {LEARNING_RATE}\")\n",
    "print(f\"  QA cache    : {QA_CACHE}\")\n",
    "print(f\"  Output      : {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc09f0f",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Load the Q&A Dataset\n",
    "\n",
    "The `JordanPeterson_DataPrep.ipynb` notebook generated this cache by:\n",
    "1. Extracting ~350-word passages from each PDF with front-matter removal\n",
    "2. Calling Claude Haiku to generate 2 questions per passage\n",
    "\n",
    "V2 trained on **5,029 pairs** generated from ~2,519 passages (front matter\n",
    "included).  V3 trains on **4,867 pairs** from ~2,492 passages ‚Äî the 27 pairs\n",
    "of reduction come entirely from dropping publisher metadata pages, not from\n",
    "removing real philosophical content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f26cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 4,867 Q&A pairs\n",
      "Schema: ['question', 'answer']\n",
      "\n",
      "Distribution by book:\n",
      "  Maps of Meaning                      1821  (37.4%)\n",
      "  We Who Wrestle with God              1264  (26.0%)\n",
      "  12 Rules for Life                     926  (19.0%)\n",
      "  Beyond Order                          856  (17.6%)\n",
      "\n",
      "V2 distribution (for comparison):\n",
      "  Maps of Meaning                      V2: 1826  ‚Üí  V3: 1821  (-5)\n",
      "  We Who Wrestle with God              V2: 1367  ‚Üí  V3: 1264  (-103)\n",
      "  12 Rules for Life                    V2:  974  ‚Üí  V3:  926  (-48)\n",
      "  Beyond Order                         V2:  862  ‚Üí  V3:  856  (-6)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "if not QA_CACHE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Q&A cache not found at {QA_CACHE}. \"\n",
    "        \"Run JordanPeterson_DataPrep.ipynb first to generate the dataset.\"\n",
    "    )\n",
    "\n",
    "with open(QA_CACHE) as f:\n",
    "    records = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Build a HuggingFace Dataset from the flat list of dicts.\n",
    "# We only keep \"question\" and \"answer\"; the \"book\" field is used for statistics.\n",
    "raw_dataset = Dataset.from_list([\n",
    "    {\"question\": r[\"question\"], \"answer\": r[\"answer\"]}\n",
    "    for r in records\n",
    "])\n",
    "\n",
    "print(f\"Dataset loaded: {len(raw_dataset):,} Q&A pairs\")\n",
    "print(f\"Schema: {raw_dataset.column_names}\")\n",
    "print()\n",
    "\n",
    "book_dist = Counter(r[\"book\"] for r in records)\n",
    "print(\"Distribution by book:\")\n",
    "for book, count in sorted(book_dist.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {book:<35}  {count:4d}  ({100*count/len(records):.1f}%)\")\n",
    "\n",
    "# V2 reference distribution for comparison\n",
    "print()\n",
    "print(\"V2 distribution (for comparison):\")\n",
    "v2_ref = {\n",
    "    \"Maps of Meaning\": 1826,\n",
    "    \"We Who Wrestle with God\": 1367,\n",
    "    \"12 Rules for Life\": 974,\n",
    "    \"Beyond Order\": 862,\n",
    "}\n",
    "for book, count in sorted(v2_ref.items(), key=lambda x: -x[1]):\n",
    "    v3_count = book_dist.get(book, 0)\n",
    "    delta = v3_count - count\n",
    "    print(f\"  {book:<35}  V2: {count:4d}  ‚Üí  V3: {v3_count:4d}  ({delta:+d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244205d9",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Load the Qwen3-14B Base Model\n",
    "\n",
    "## Why Qwen3-14B Over GPT-OSS 20B\n",
    "\n",
    "Both models were evaluated in V1.  Qwen3-14B is the better choice for three\n",
    "independent reasons:\n",
    "\n",
    "1. **Lower training loss** (2.44 vs 3.01) ‚Äî despite being a smaller model,\n",
    "   Qwen3 absorbed the domain signal more efficiently in one epoch.\n",
    "\n",
    "2. **Training speed** (23 min vs 73 min per epoch) ‚Äî with 3 epochs planned,\n",
    "   GPT-OSS would take ~220 min; Qwen3 takes ~70 min.\n",
    "\n",
    "3. **Architectural recency** ‚Äî Qwen3-14B (2025) is a more capable base model\n",
    "   for instruction-following, which matters for the Q&A format we train on.\n",
    "\n",
    "## Why `max_seq_length=2048`\n",
    "\n",
    "Our training examples are ~350-word passages formatted as ChatML conversations.\n",
    "After tokenisation, each example is typically 650‚Äì800 tokens.  Setting\n",
    "`max_seq_length=2048` gives headroom for the system prompt and formatting\n",
    "overhead without wasting VRAM on unnecessary KV-cache pre-allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f8ca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unsloth/Qwen3-14B-unsloth-bnb-4bit ...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen3 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.516 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c610cb6238403688141368bdf29362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.  VRAM reserved: 11.2 GB\n",
      "Model dtype   : torch.bfloat16\n",
      "Tokenizer     : Qwen2TokenizerFast\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading {BASE_MODEL} ...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    dtype           = None,          # auto: bfloat16 on Ampere+, float16 otherwise\n",
    "    max_seq_length  = MAX_SEQ_LEN,\n",
    "    load_in_4bit    = True,          # 4-bit quantization via bitsandbytes\n",
    "    full_finetuning = False,         # we are adding LoRA, not full fine-tuning\n",
    ")\n",
    "\n",
    "vram_after_load = torch.cuda.memory_reserved() / 1e9\n",
    "print(f\"Model loaded.  VRAM reserved: {vram_after_load:.1f} GB\")\n",
    "print(f\"Model dtype   : {next(model.parameters()).dtype}\")\n",
    "print(f\"Tokenizer     : {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb83819",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Add LoRA Adapters (r=32)\n",
    "\n",
    "## What LoRA Does\n",
    "\n",
    "Low-Rank Adaptation freezes the pre-trained weight matrices `W` and injects a\n",
    "trainable update `ŒîW = (alpha/r) √ó B √ó A` where:\n",
    "- `B` has shape `(d_model √ó r)` ‚Äî initialised to zero\n",
    "- `A` has shape `(r √ó d_model)` ‚Äî initialised with random Gaussian values\n",
    "- `r` is the rank\n",
    "\n",
    "At the start of training `B=0` so `ŒîW=0` and the model behaves identically\n",
    "to the base model.  Training gradually fills `B` with the directions needed\n",
    "to capture Peterson's style.\n",
    "\n",
    "## Why r=32 vs V1's r=16\n",
    "\n",
    "Think of `r` as the number of \"stylistic dimensions\" the adapter can learn.\n",
    "With r=16, the adapter has 16 orthogonal directions to encode things like:\n",
    "*\"use chaos/order vocabulary\"*, *\"frame arguments historically\"*,\n",
    "*\"connect psychology to mythology\"*, etc.\n",
    "\n",
    "With r=32 we have 32 such dimensions ‚Äî enough to capture both the macro-level\n",
    "thematic vocabulary AND the micro-level sentence rhythm that makes Peterson's\n",
    "writing recognisable.  The adapter checkpoint grows from ~260 MB to ~520 MB;\n",
    "VRAM impact is negligible.\n",
    "\n",
    "## Target Modules\n",
    "\n",
    "We apply LoRA to all attention projections (q, k, v, o) and all MLP\n",
    "gate/up/down projections.  This is the full set recommended by Unsloth for\n",
    "stylistic fine-tuning because style is encoded across both attention patterns\n",
    "(what the model attends to) and MLP activations (how it transforms those\n",
    "representations).\n",
    "\n",
    "Setting `alpha = rank` (alpha=32, rank=32) keeps the effective scale factor\n",
    "at `alpha/rank = 1.0` ‚Äî the LoRA update is applied at the same relative\n",
    "strength as the base weights, which is the established baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81751ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.2.1 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters    :   8,691,809,280\n",
      "Trainable (LoRA)    :     128,450,560  (1.4778% of total)\n",
      "  V1 had r=16: ~64,225,280 trainable params\n",
      "  V3 has r=32: ~128,450,560 trainable params (2x capacity)\n",
      "VRAM after LoRA     : 11.7 GB\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                   = LORA_RANK,    # 32 ‚Äî doubled from V1's r=16\n",
    "    lora_alpha          = LORA_ALPHA,   # 32 ‚Äî alpha/rank = 1.0 (same scale as V1)\n",
    "    lora_dropout        = 0,            # 0 is optimal for Unsloth according to docs\n",
    "    target_modules      = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # all attention projections\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",        # all MLP projections\n",
    "    ],\n",
    "    use_gradient_checkpointing = \"unsloth\",   # saves ~30% VRAM vs PyTorch default\n",
    "    random_state        = 42,\n",
    "    use_rslora          = False,   # standard LoRA (not rank-stabilised variant)\n",
    "    loftq_config        = None,    # no quantization-aware init needed with 4-bit base\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ Count trainable parameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "pct_trainable    = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"Total parameters    : {total_params:>15,}\")\n",
    "print(f\"Trainable (LoRA)    : {trainable_params:>15,}  ({pct_trainable:.4f}% of total)\")\n",
    "print(f\"  V1 had r=16: ~{trainable_params//2:,} trainable params\")\n",
    "print(f\"  V3 has r=32: ~{trainable_params:,} trainable params (2x capacity)\")\n",
    "print(f\"VRAM after LoRA     : {torch.cuda.memory_reserved()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23bc5b4",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Format Dataset into Qwen3 ChatML Format\n",
    "\n",
    "Each Q&A pair is converted into a three-turn conversation using the Qwen3\n",
    "ChatML template.  The format change from V1 to V2/V3:\n",
    "\n",
    "| Role | V1 content | **V2/V3 content** |\n",
    "|------|-----------|----------------|\n",
    "| `system` | Generic assistant prompt | **Peterson persona prompt** |\n",
    "| `user` | A book passage chunk | **A synthetically generated question** |\n",
    "| `assistant` | Continuation of the passage | **The original passage as an answer** |\n",
    "\n",
    "The system prompt changes from generic to persona-specific so the model\n",
    "learns: *\"when given the Peterson persona instruction, respond in his voice\"*.\n",
    "\n",
    "The user message changes from a passage fragment to a real question so the\n",
    "model learns: *\"when asked a question in this domain, produce a substantive\n",
    "Peterson-style answer\"*.\n",
    "\n",
    "## About `enable_thinking=False`\n",
    "\n",
    "We format training examples with `enable_thinking=False`.  This tells Qwen3's\n",
    "chat template not to include chain-of-thought reasoning tokens in the formatted\n",
    "string.  Even with this setting, the template adds an empty\n",
    "`<think>\\n\\n</think>` block before the assistant response ‚Äî this is expected\n",
    "and will be masked out by `train_on_responses_only` in the next step, so it\n",
    "does not contribute to the loss.\n",
    "\n",
    "## System Prompt Source\n",
    "\n",
    "`SYSTEM_PROMPT` is loaded from `peterson_config.json` (not hardcoded) so\n",
    "the DataPrep notebook and all fine-tuning notebooks use identical persona\n",
    "instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5b306c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e8e0ad05cd4e82a8688f4b3ab7ce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted dataset: 4,867 examples\n",
      "Columns: ['text']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SAMPLE FORMATTED TRAINING EXAMPLE\n",
      "----------------------------------------------------------------------\n",
      "<|im_start|>system\n",
      "You are an AI assistant that has been trained on the complete works of Jordan B. Peterson, a Canadian clinical psychologist, professor, and author. You speak with deep knowledge of psychology, philosophy, mythology, religion, and personal responsibility. Your responses reflect Peterson's writing style, intellectual depth, and interdisciplinary approach to understanding human nature and meaning.<|im_end|>\n",
      "<|im_start|>user\n",
      "How does the brain's neurological structure relate to our psychological capacity to generate meaning from chaos?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "FIGURES 1 The Domain and Constituent Elements of the Known 15 2 The Metamythological Cycle of the Way 17 3 Normal Life 28 4 Revolutionary Adaptation 31 5 The Ambivalent Nature of Novelty 44 6 \n",
      "...\n",
      "----------------------------------------------------------------------\n",
      "Full sample length: 425 words / 2739 chars\n"
     ]
    }
   ],
   "source": [
    "def format_example(batch):\n",
    "    '''\n",
    "    Convert a batch of Q&A records into formatted Qwen3 ChatML strings.\n",
    "\n",
    "    Each formatted string looks like:\n",
    "        <|im_start|>system\n",
    "        You are an AI assistant... (Peterson persona)\n",
    "        <|im_end|>\n",
    "        <|im_start|>user\n",
    "        <the generated question>\n",
    "        <|im_end|>\n",
    "        <|im_start|>assistant\n",
    "        <think>\n",
    "\n",
    "        </think>\n",
    "        <the Peterson passage>\n",
    "        <|im_end|>\n",
    "\n",
    "    The <think></think> block appears because Qwen3's template always includes\n",
    "    a thinking placeholder even when enable_thinking=False.  It is masked out\n",
    "    during training by train_on_responses_only and does not affect the loss.\n",
    "    '''\n",
    "    formatted_texts = []\n",
    "    for question, answer in zip(batch[\"question\"], batch[\"answer\"]):\n",
    "        conversation = [\n",
    "            {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",      \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize              = False,\n",
    "            add_generation_prompt = False,\n",
    "            enable_thinking       = False,\n",
    "        )\n",
    "        formatted_texts.append(text)\n",
    "    return {\"text\": formatted_texts}\n",
    "\n",
    "\n",
    "dataset = raw_dataset.map(\n",
    "    format_example,\n",
    "    batched=True,\n",
    "    batch_size=256,\n",
    "    remove_columns=raw_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"Formatted dataset: {len(dataset):,} examples\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print()\n",
    "print(\"-\" * 70)\n",
    "print(\"SAMPLE FORMATTED TRAINING EXAMPLE\")\n",
    "print(\"-\" * 70)\n",
    "sample = dataset[0][\"text\"]\n",
    "print(sample[:800])\n",
    "print(\"...\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Full sample length: {len(sample.split())} words / {len(sample)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d1f813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Qwen3 ChatML response boundary tokens:\n",
      "  instruction_part : '<|im_start|>user\\n'\n",
      "  response_part    : '<|im_start|>assistant\\n'\n",
      "\n",
      "Boundary tokens set ‚Äî train_on_responses_only will be applied below.\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect the response boundary tokens by inspecting the formatted data.\n",
    "# This guards against future Qwen3 template changes that might shift the token\n",
    "# boundaries, and mirrors the robust approach used in V2.\n",
    "_sample_text = dataset[0][\"text\"]\n",
    "\n",
    "if \"<|im_start|>assistant\\n\" in _sample_text:\n",
    "    instruction_part = \"<|im_start|>user\\n\"\n",
    "    response_part    = \"<|im_start|>assistant\\n\"\n",
    "    print(\"Detected Qwen3 ChatML response boundary tokens:\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"Could not detect Qwen3 ChatML tokens in formatted dataset. \"\n",
    "        \"Check the output of format_example() above.\"\n",
    "    )\n",
    "\n",
    "print(f\"  instruction_part : {repr(instruction_part)}\")\n",
    "print(f\"  response_part    : {repr(response_part)}\")\n",
    "print()\n",
    "print(\"Boundary tokens set ‚Äî train_on_responses_only will be applied below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8bc69a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Configure and Run Training\n",
    "\n",
    "## SFTConfig Parameters Explained\n",
    "\n",
    "- **`num_train_epochs=3`**: The most important change from V1.  Three full\n",
    "  passes over the dataset let the model move from memorisation into genuine\n",
    "  stylistic generalisation.  The empirical sweet spot for LoRA stylistic\n",
    "  fine-tuning: 1 epoch = memorisation, 3 epochs = generalisation, 5+ = risk\n",
    "  of overfitting.\n",
    "\n",
    "- **`warmup_steps=30`**: Gradually ramps the learning rate from 0 ‚Üí 2e-4\n",
    "  over the first 30 gradient steps (~3% of total training).  Prevents the\n",
    "  randomly-initialised LoRA matrices from making destructively large weight\n",
    "  updates at the start.\n",
    "\n",
    "- **`lr_scheduler_type=\"cosine\"`**: After warmup the learning rate decays\n",
    "  smoothly following a cosine curve, reaching ~0 at the end of epoch 3.\n",
    "  Cosine decay is generally more stable than linear decay for LoRA.\n",
    "\n",
    "- **`weight_decay=0.01`**: Mild L2 regularisation on the LoRA weights.\n",
    "  Discourages the adapter from memorising individual passages and encourages\n",
    "  generalisation.\n",
    "\n",
    "- **`optim=\"adamw_8bit\"`**: Unsloth's 8-bit AdamW keeps optimizer states in\n",
    "  8-bit rather than 32-bit, saving ~3 GB of VRAM with no measurable quality\n",
    "  cost for LoRA training.\n",
    "\n",
    "- **`packing=False`**: Sequence packing combines multiple short examples into\n",
    "  one long sequence for efficiency.  We disable it because our examples are\n",
    "  long (~400 words ‚Üí ~700 tokens), and packing them could leak context between\n",
    "  unrelated Q&A pairs.\n",
    "\n",
    "## V3 Expected Step Count\n",
    "\n",
    "With 4,867 pairs (vs V2's 5,029), the expected gradient updates are:\n",
    "\n",
    "```\n",
    "ceil(4867 / 2) √ó 3 // 4 = 2434 √ó 3 // 4 = 7302 // 4 = 1,825\n",
    "```\n",
    "\n",
    "V2 ran **1,887 steps** from 5,029 pairs ‚Äî V3 should run approximately\n",
    "**1,825 steps**, a minor reduction from cleaner data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf49145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711186fba1934d29a3558fde578efdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/4867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4659c84876f2436e929b0fe3d6e252cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f70a2de34f408088361add10aa1d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/4867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response masking check on first example:\n",
      "  Total tokens   : 744\n",
      "  Trained tokens : 645  (assistant response only)\n",
      "  Masked tokens  : 99   (system + user = 13% of input)\n",
      "\n",
      "Estimated gradient updates: 1,825\n",
      "  (4,867 examples / 2 batch x 4 accum x 3 epochs)\n"
     ]
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model            = model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset    = dataset,\n",
    "    args             = SFTConfig(\n",
    "        dataset_text_field          = \"text\",\n",
    "        max_length                  = MAX_SEQ_LEN,\n",
    "        dataset_num_proc            = 2,\n",
    "\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        gradient_accumulation_steps = GRAD_ACCUM,\n",
    "        num_train_epochs            = NUM_EPOCHS,\n",
    "\n",
    "        learning_rate               = LEARNING_RATE,\n",
    "        warmup_steps                = WARMUP_STEPS,\n",
    "        lr_scheduler_type           = \"cosine\",\n",
    "        weight_decay                = WEIGHT_DECAY,\n",
    "\n",
    "        optim                       = \"adamw_8bit\",\n",
    "        fp16                        = not torch.cuda.is_bf16_supported(),\n",
    "        bf16                        = torch.cuda.is_bf16_supported(),\n",
    "\n",
    "        logging_steps               = 25,\n",
    "        save_strategy               = \"epoch\",\n",
    "        output_dir                  = str(OUTPUT_DIR),\n",
    "        report_to                   = \"none\",\n",
    "\n",
    "        seed                        = 42,\n",
    "        packing                     = False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "sft_trainer = train_on_responses_only(\n",
    "    sft_trainer,\n",
    "    instruction_part = instruction_part,\n",
    "    response_part    = response_part,\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ Masking verification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# If masking is working, labels will be -100 for all system/user tokens and\n",
    "# real token IDs only for the assistant response tokens.\n",
    "_sample_input = sft_trainer.train_dataset[0]\n",
    "_n_total      = len(_sample_input[\"input_ids\"])\n",
    "_n_trained    = sum(1 for lbl in _sample_input[\"labels\"] if lbl != -100)\n",
    "_n_masked     = _n_total - _n_trained\n",
    "print(\"Response masking check on first example:\")\n",
    "print(f\"  Total tokens   : {_n_total}\")\n",
    "print(f\"  Trained tokens : {_n_trained}  (assistant response only)\")\n",
    "print(f\"  Masked tokens  : {_n_masked}   (system + user = {100*_n_masked/_n_total:.0f}% of input)\")\n",
    "print()\n",
    "\n",
    "_total_steps = math.ceil(len(dataset) / BATCH_SIZE) * NUM_EPOCHS // GRAD_ACCUM\n",
    "print(f\"Estimated gradient updates: {_total_steps:,}\")\n",
    "print(f\"  ({len(dataset):,} examples / {BATCH_SIZE} batch x {GRAD_ACCUM} accum x {NUM_EPOCHS} epochs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1703dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,867 | Num Epochs = 3 | Total steps = 1,827\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 128,450,560 of 14,896,757,760 (0.86% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM before training: 11.7 GB\n",
      "Starting training ‚Äî 3 epochs, ~1,825 gradient updates...\n",
      "\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1827' max='1827' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1827/1827 2:16:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.719300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.719100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.586700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.560700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.505700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>1.498300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1.398600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.331700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>1.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>1.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>1.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.804500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.795100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.756600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.777800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.748100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.735600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "  Steps          : 1,827\n",
      "  Training loss  : 1.5258\n",
      "  Elapsed        : 136.9 min\n",
      "  Peak VRAM      : 15.3 GB\n",
      "\n",
      "V2 reference:\n",
      "  Steps: 1,887  |  Loss: 1.5058  |  Time: 144.0 min  |  VRAM: 15.5 GB\n",
      "\n",
      "V1 reference (1 epoch, Q&A format):\n",
      "  Steps: 321    |  Loss: 2.4400  |  Time:  23.3 min  |  VRAM: 13.4 GB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "vram_before = torch.cuda.memory_reserved() / 1e9\n",
    "print(f\"VRAM before training: {vram_before:.1f} GB\")\n",
    "print(f\"Starting training ‚Äî {NUM_EPOCHS} epochs, ~{_total_steps:,} gradient updates...\")\n",
    "print()\n",
    "\n",
    "t0 = time.time()\n",
    "train_result = sft_trainer.train()\n",
    "elapsed_min  = (time.time() - t0) / 60\n",
    "\n",
    "vram_peak = torch.cuda.max_memory_reserved() / 1e9\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Steps          : {train_result.global_step:,}\")\n",
    "print(f\"  Training loss  : {train_result.training_loss:.4f}\")\n",
    "print(f\"  Elapsed        : {elapsed_min:.1f} min\")\n",
    "print(f\"  Peak VRAM      : {vram_peak:.1f} GB\")\n",
    "print()\n",
    "print(\"V2 reference:\")\n",
    "print(\"  Steps: 1,887  |  Loss: 1.5058  |  Time: 144.0 min  |  VRAM: 15.5 GB\")\n",
    "print()\n",
    "print(\"V1 reference (1 epoch, Q&A format):\")\n",
    "print(\"  Steps: 321    |  Loss: 2.4400  |  Time:  23.3 min  |  VRAM: 13.4 GB\")\n",
    "\n",
    "# 136m 52.3s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef51a6",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Save the LoRA Adapter\n",
    "\n",
    "We save only the LoRA adapter weights ‚Äî not the full 14B model.  This keeps\n",
    "the checkpoint small (~520 MB for r=32 vs ~28 GB for a full model copy).\n",
    "At inference time Unsloth loads the base model from HuggingFace Hub and\n",
    "merges the adapter on-the-fly.\n",
    "\n",
    "**V3 output path**: `outputs/qwen3_14b_peterson_v3_lora/`\n",
    "\n",
    "The V2 adapter at `outputs/qwen3_14b_peterson_v2_lora/` is **not touched**.\n",
    "Both adapters coexist and can be compared independently in the\n",
    "`AllModels_JordanPeterson_Comparison.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9941712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving V3 LoRA adapter to outputs/qwen3_14b_peterson_v3_lora ...\n",
      "\n",
      "Adapter files:\n",
      "  adapter_model.safetensors  (513.9 MB)\n",
      "\n",
      "Total adapter size: 513.9 MB\n",
      "  (V2 r=32 was 513.9 MB; V3 r=32 should be similar)\n",
      "\n",
      "V2 adapter (untouched) : outputs/qwen3_14b_peterson_v2_lora/\n",
      "V3 adapter (just saved): outputs/qwen3_14b_peterson_v3_lora\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving V3 LoRA adapter to {OUTPUT_DIR} ...\")\n",
    "\n",
    "model.save_pretrained(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "\n",
    "# List saved files with sizes\n",
    "adapter_files = (\n",
    "    list(OUTPUT_DIR.glob(\"*.safetensors\"))\n",
    "    + list(OUTPUT_DIR.glob(\"*.bin\"))\n",
    ")\n",
    "total_mb = sum(f.stat().st_size for f in adapter_files) / 1e6\n",
    "\n",
    "print()\n",
    "print(\"Adapter files:\")\n",
    "for f in sorted(adapter_files):\n",
    "    print(f\"  {f.name}  ({f.stat().st_size/1e6:.1f} MB)\")\n",
    "\n",
    "print()\n",
    "print(f\"Total adapter size: {total_mb:.1f} MB\")\n",
    "print(f\"  (V2 r=32 was 513.9 MB; V3 r=32 should be similar)\")\n",
    "print()\n",
    "print(f\"V2 adapter (untouched) : outputs/qwen3_14b_peterson_v2_lora/\")\n",
    "print(f\"V3 adapter (just saved): {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a6595",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Inference Test\n",
    "\n",
    "The same 5 evaluation prompts are used here as in V1 and V2 so results are\n",
    "directly comparable.  Copy-paste the outputs into a comparison doc alongside\n",
    "V2 outputs to spot-check whether the front-matter fix produces visibly better\n",
    "responses (especially for Maps of Meaning questions).\n",
    "\n",
    "We run greedy decoding (`do_sample=False`) for deterministic, reproducible\n",
    "outputs ‚Äî the same setting used in all comparison notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6a102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing V3 fine-tuned model (greedy decoding)...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Q1: What is the relationship between order and chaos in human experience?\n",
      "----------------------------------------------------------------------\n",
      "of the world parents (and their ‚Äúchildren‚Äù) from a different perspective. The great mother‚Äînature, in her benevolent guise‚Äîprovides us with the necessities and opportunities of life: she is the source of rich harvest and the giver of freely available water. She eternally nurtures and protects. The terrible mother‚Äînature, in revolutionary guise‚Äîis the sudden appearance of the stranger; the force that destroys crops and livestock through drought, flood or pestilence; the giver of disease and death. She is the eternal source of threat to the individual state and culture, and can manifest herself in unexpected ways. It is the combination of these two eternally extant ‚Äúpersonality aspects‚Äù that constitute the most primordial deity of womanhood‚Äîthe goddess who is simultaneously creation and destruction, birth and abortion, fertility and sterility, beauty and horror. The Great Father, for his part, is protection for the innocent and shelter for the vulnerable. He is the embodiment of ancestral wisdom, the protector of tradition and the shield against the terrors of the unknown. But he is also authoritarian, rigid and repressive. He is the force that denies opportunity, freedom and possibility. He is the reason that children outgrow their homes, and must leave to pursue their destinies. He is the tyranny of outdated ideas, and the endless war between entrenched cultures. The Great Father is the eternal king, the eternal oppressor‚Äîand the eternal savior. Finally, the Terrible Son is\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Q2: Why is personal responsibility the foundation of a meaningful life?\n",
      "----------------------------------------------------------------------\n",
      "the world, 143‚Äì74 and Genesis story, 160‚Äì68 in Harry Potter series, 259‚Äì60 individual transformation through, 261‚Äì62 as living process, 262 love as ultimate goal of, 268‚Äì69 and Neo-Pagans‚Äô reverence for nature, 171‚Äì72 new information generated by, 263‚Äì64 and order/disorder dynamic, 173‚Äì74 as path forward, 264‚Äì65 and reality itself, 265‚Äì66 as regenerative, 174 voluntarily undertaken, 261 voluntary participation in, 144, 261 and Whanganui River case, 363 consciousness, 22, 23, 24, 25, 26 consumerism, 173 consensus, 101 conservatism, 183, 184 constant elements in life, xxviii contingent elements in life, xxvii contracts, 112 cooperation, 60, 61 Corinthians, 1 corruption, overcoming, 130 corpses, 349‚Äì50 countenance, 102 courage, 70, 211, 212 creativity, 60, 61, 62\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Q3: How do ancient myths and stories reveal truths about human nature?\n",
      "----------------------------------------------------------------------\n",
      "of the world parents. The ‚Äúdying god‚Äù is always found in a ‚Äúbitter valley,‚Äù remote from the rest of humanity (and from the ‚Äúkingdom of youth‚Äù). He is languishing in a torpor or is completely paralyzed; his body is often ravaged by disease. Sometimes he is devoured by a serpent, which either resides in the sky‚Äîwhere it is scorching him with its fire‚Äîor grows up around him, in the form of a tree, and then breaks into pieces, one of which makes him impotent. His death is caused directly by some kind of sexual assault: sometimes by a scorpion, spider, snake, or similar creature, who tears open his loins, privy parts, or womb; more frequently by a woman, who dances before him, bewitches him, and then rapes him. When Osiris was killed by Seth, for example, it Maps of Meaning 159 Figure 23: The Exploratory Hero as Son of the Heavenly Mother was with his wife‚Äôs help that he was able to impregnate Isis. Other variants tell of Atum- Re being overthrown by a goddess named Tefnut, of Ra-Horus being overcome by a foreign queen, and of Horus being attacked by a serpent named Apophis‚Ä¶. The dying god is succeeded by a new ruler, but this usurper lacks wisdom and experience. Frequently he is marked by physical defectiveness, and his subordinates serve\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Q4: What does it mean to pursue what is meaningful rather than what is expedient?\n",
      "----------------------------------------------------------------------\n",
      "the most profound sense, the Word becomes flesh, and eternally mediates between the divine and the mortal; and that in doing so, reconciles the two. That is the central message of Christianity (although not its only message). It is also something else, however: the description of the pattern of being that protects people from falling prey to their own worst temptations‚Äîand, simultaneously, the best strategy for uniting themselves with those who are other than they are, for mutual benefit. The story of the Christian savior is thus both the tale of the man who embodies the spirit of his culture and the meta-story describing how such spirits come to be organized, in the first place‚Äîhow they come to fight one another, and then unite into a hierarchy; how that hierarchy comes to be represented, abstractly or imagistically; and how that representation becomes integrated into the practice and dogma of the church. Christ is therefore the personality whose actions and thoughts represent the highest possible integration of all previous cultural striving, as well as the personality who embodies the Meta-Image of God‚Äîthe same God whose likeness was stamped on every coin, so that it could be used in trade. In this manner, Christ is the personality who gives full expression to the implicit moral order, encoded in society itself, and which serves as the precondition even for the existence of that society. He is, by definition, the embodiment of the ideal balance between individual and social, past and present, self and other. He\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Q5: How should a person confront suffering rather than flee from it?\n",
      "----------------------------------------------------------------------\n",
      "the world is very deep, very mysterious‚Äîand we know almost nothing about it. It‚Äôs not clear at all how we could justifiably treat something as complex as the world (or even as complex as ourselves) in the same manner as a solved problem, or a mere tool. We don‚Äôt understand what makes people tick‚Äîwhat makes them happy, satisfied, productive, grateful, loving, artistic, cooperative, disciplined, generous, kind, courageous, playful, wise, attentive, alert, calm, patient, healthy, awake‚Äîor how to best use, improve, modify or transform them. We can‚Äôt easily apply the deterministic theories of classical mechanics to other people, because they‚Äôre simply too unpredictable. They‚Äôre emergent phenomena themselves, like us; complex, dynamic, open systems, whose behaviour cannot be specified by equations derived from initial conditions. People aren‚Äôt billiard balls. Furthermore, even if we knew exactly what was out there ‚Äúin front‚Äù of us, so to speak‚Äîas we do when our sights are set on a well-understood object‚Äîit would still be questionable to reduce Being itself to the status of mere material. The value of a diamond, for example, depends on its context of use. That context includes the desires and beliefs of the observer, who may also be subject to modification, in response to the diamond‚Äîor to something else equally unexpected and remarkable. The diamond is only relevant to the seeker, and might well have remained forever unidentified, had it not been for the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    \"Why is personal responsibility the foundation of a meaningful life?\",\n",
    "    \"How do ancient myths and stories reveal truths about human nature?\",\n",
    "    \"What does it mean to pursue what is meaningful rather than what is expedient?\",\n",
    "    \"How should a person confront suffering rather than flee from it?\",\n",
    "]\n",
    "\n",
    "\n",
    "def ask_v3(question: str, max_new_tokens: int = 300) -> str:\n",
    "    '''\n",
    "    Generate a response from the V3 fine-tuned model.\n",
    "\n",
    "    Uses greedy decoding (do_sample=False) for reproducibility, consistent\n",
    "    with how V1 and V2 outputs are evaluated in the comparison notebooks.\n",
    "    enable_thinking=False keeps reasoning mode disabled, matching training.\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens     = max_new_tokens,\n",
    "            do_sample          = False,\n",
    "            temperature        = 1.0,\n",
    "            repetition_penalty = 1.1,\n",
    "        )\n",
    "\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response   = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    # Strip any residual thinking blocks\n",
    "    response   = re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL).strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"Testing V3 fine-tuned model (greedy decoding)...\")\n",
    "print()\n",
    "for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Q{i+1}: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "    answer = ask_v3(prompt)\n",
    "    print(answer if answer.strip() else \"(empty response)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2fdf8",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusions\n",
    "\n",
    "## What V3 Changed vs V2\n",
    "\n",
    "V3 uses identical hyperparameters to V2 (r=32, 3 epochs, 2e-4 LR).  The\n",
    "only change is the training data: the DataPrep notebook's front-matter removal\n",
    "heuristic is now applied, removing publisher metadata pages from all four books.\n",
    "\n",
    "The net effect: **162 fewer Q&A pairs** (5,029 ‚Üí 4,867), all subtracted from\n",
    "boilerplate passages.  No philosophical content was removed.\n",
    "\n",
    "## Expected Improvement\n",
    "\n",
    "The primary expected improvement is in **Maps of Meaning responses**.  V2 had\n",
    "~57 passages (1,826 - ~1,769 actual content passages) drawn from publisher\n",
    "pages, title pages, and the table of contents.  V3 eliminates those.  If the\n",
    "model still produces responses that begin with book metadata, check the DataPrep\n",
    "notebook's front-matter detection for that book.\n",
    "\n",
    "For the other three books, response quality should be indistinguishable from V2\n",
    "‚Äî the data change was minimal.\n",
    "\n",
    "## Adding V3 to the AllModels Comparison Notebook\n",
    "\n",
    "To compare V3 against V1, V2, and the base models:\n",
    "\n",
    "1. Open `AllModels_JordanPeterson_Comparison.ipynb`\n",
    "2. Add `\"qwen3_v3\"` to `MODEL_KEYS`\n",
    "3. Add `\"qwen3_v3\": \"./outputs/qwen3_14b_peterson_v3_lora/\"` to `MODEL_PATHS`\n",
    "4. Add a colour entry in `MODEL_COLORS`\n",
    "5. Delete `comparison_cache_all_models/qwen3_v3_results.pkl` if it exists\n",
    "6. Re-run the comparison notebook\n",
    "\n",
    "## Next Levers If Further Improvement Is Needed\n",
    "\n",
    "| Option | Expected gain | Effort |\n",
    "|--------|--------------|--------|\n",
    "| 5 epochs (instead of 3) | Marginal style improvement; risk of overfitting | Low |\n",
    "| 3 questions per passage | More data diversity, ~7,400 pairs | Medium |\n",
    "| `r=64, alpha=64` | More adapter capacity; +200 MB checkpoint | Low |\n",
    "| Human-edited answers | Highest quality ceiling | High |\n",
    "| Qwen3-32B (if it fits) | Stronger base model | High |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4ae0b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 21 12:10:17 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.126.09             Driver Version: 580.126.09     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:04:00.0 Off |                  Off |\n",
      "| 39%   50C    P8             26W /  450W |   12648MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 2070 ...    Off |   00000000:2B:00.0  On |                  N/A |\n",
      "| 32%   55C    P8             14W /  215W |     453MiB /   8192MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2651      G   /usr/bin/gnome-shell                      6MiB |\n",
      "|    0   N/A  N/A          271813      C   ...Tuning/.finetuning/bin/python      12616MiB |\n",
      "|    1   N/A  N/A            2651      G   /usr/bin/gnome-shell                    277MiB |\n",
      "|    1   N/A  N/A          422092      G   ...rack-uuid=3190708988185955192         93MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
