{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b29a9c",
   "metadata": {},
   "source": [
    "# Qwen3-14B All Versions Comparison: Jordan B. Peterson Fine-Tuning\n",
    "## Base → V1 → V2 → V3: Tracking Training Task and Data-Quality Effects\n",
    "\n",
    "This notebook evaluates all four Qwen3-14B checkpoints side by side on the same\n",
    "Peterson-domain prompts and metrics, answering three specific research questions:\n",
    "\n",
    "1. **Task change (V1→V2)** — Does switching from passage-completion to synthetic Q&A\n",
    "   training improve answering ability, TF-IDF similarity, and keyword density?\n",
    "2. **Data quality (V2→V3)** — Does removing front-matter pages from the training set\n",
    "   produce measurably different metrics, given nearly identical training loss?\n",
    "3. **Fine-tuning vs Base** — How do all three fine-tuned versions compare to the\n",
    "   unmodified base model on Peterson-domain vocabulary and perplexity metrics?\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "| Metric | What it measures | Direction |\n",
    "|--------|-----------------|-----------|\n",
    "| **Perplexity** | How surprised the model is by real Peterson text | ↓ better |\n",
    "| **TF-IDF cosine similarity** | Vocabulary overlap with Peterson's writing | ↑ better |\n",
    "| **Keyword density** | Rate of use of Peterson's ~60 signature terms | ↑ better |\n",
    "| **Type-Token Ratio (TTR)** | Vocabulary richness (unique / total words) | ↑ better |\n",
    "| **Response length** | Average words per response | informational |\n",
    "\n",
    "### GPU Memory Strategy\n",
    "\n",
    "All four Qwen3-14B models (~13–16 GB VRAM each) are evaluated **sequentially**:\n",
    "\n",
    "```\n",
    "Load → Infer → Save pkl → Unload → repeat for next model\n",
    "```\n",
    "\n",
    "Once all four pkl files exist in `comparison_cache_qwen3_versions/`, no model loading\n",
    "is required — metrics and charts load from cache in seconds. Delete any pkl to force\n",
    "re-evaluation of that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b43c6",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Registry\n",
    "\n",
    "| Key | Path | Training Task | Epochs | LoRA | Training Stats |\n",
    "|-----|------|---------------|--------|------|----------------|\n",
    "| `base` | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | None (base) | — | — | — |\n",
    "| `v1` | `./outputs/qwen3_14b_jordan_peterson_lora` | Passage completion | 1 | r=16 | 321 steps, 23.3 min, loss 2.44 |\n",
    "| `v2` | `./outputs/qwen3_14b_peterson_v2_lora` | Synthetic Q&A (5,029 pairs, w/ front matter) | 3 | r=32 | 1,887 steps, 144.0 min, loss 1.5058 |\n",
    "| `v3` | `./outputs/qwen3_14b_peterson_v3_lora` | Synthetic Q&A (4,867 pairs, clean) | 3 | r=32 | 1,827 steps, 136.9 min, loss 1.5258 |\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **V1 training task**: V1 was trained on passage-completion (user = passage fragment,\n",
    "  assistant = continuation). At inference we apply the Peterson persona system prompt\n",
    "  for fair comparison, even though V1 never saw that prompt during training. Expect\n",
    "  lower Q&A quality — V1 learned to continue passages, not answer questions.\n",
    "- **V2 vs V3 comparability**: Identical architecture (r=32, 3 epochs, same LR). The\n",
    "  only difference is the training data: V2 includes front-matter pages; V3 uses the\n",
    "  DataPrep notebook's front-matter heuristic to remove them.\n",
    "- **V3 data reduction**: Net −162 pairs (5,029 → 4,867). Largest reduction was in\n",
    "  *We Who Wrestle with God* (−103 pairs), not *Maps of Meaning* as predicted. The\n",
    "  V3 training loss is +0.02 vs V2 — negligible, within noise.\n",
    "- **V3 index contamination**: Evaluation showed Q2 produced raw index text\n",
    "  (\"143–74 and Genesis story, 160–68…\"), confirming that back-matter content\n",
    "  is still present. Front-matter removal alone is insufficient.\n",
    "- **System prompts**: `base` receives `\"You are a helpful assistant.\"`; `v1`, `v2`,\n",
    "  `v3` all receive the Peterson persona prompt from `peterson_config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ab1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc, math, pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt',     quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ── System prompts ─────────────────────────────────────────────────────────\n",
    "with open(\"peterson_config.json\") as _f:\n",
    "    _config = _f.read()\n",
    "import json as _json\n",
    "_cfg = _json.loads(_config)\n",
    "TUNED_SYSTEM_PROMPT = _cfg[\"system_prompt\"]\n",
    "BASE_SYSTEM_PROMPT  = \"You are a helpful assistant.\"\n",
    "\n",
    "# ── Model registry ─────────────────────────────────────────────────────────\n",
    "MODEL_KEYS   = [\"base\", \"v1\", \"v2\", \"v3\"]\n",
    "MODEL_PATHS  = {\n",
    "    \"base\": \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"v1\":   \"./outputs/qwen3_14b_jordan_peterson_lora\",\n",
    "    \"v2\":   \"./outputs/qwen3_14b_peterson_v2_lora\",\n",
    "    \"v3\":   \"./outputs/qwen3_14b_peterson_v3_lora\",\n",
    "}\n",
    "MODEL_DISPLAY = {\n",
    "    \"base\": \"Qwen3-14B  |  Base\",\n",
    "    \"v1\":   \"Qwen3-14B  |  V1 (1-ep completion)\",\n",
    "    \"v2\":   \"Qwen3-14B  |  V2 (3-ep Q&A)\",\n",
    "    \"v3\":   \"Qwen3-14B  |  V3 (3-ep clean)\",\n",
    "}\n",
    "MODEL_SHORT = {\n",
    "    \"base\": \"Base\",\n",
    "    \"v1\":   \"V1\\n(1-ep completion)\",\n",
    "    \"v2\":   \"V2\\n(3-ep Q&A)\",\n",
    "    \"v3\":   \"V3\\n(3-ep clean)\",\n",
    "}\n",
    "MODEL_COLORS = {\n",
    "    \"base\": \"#4C72B0\",   # blue\n",
    "    \"v1\":   \"#DD8452\",   # orange\n",
    "    \"v2\":   \"#55A868\",   # green\n",
    "    \"v3\":   \"#C44E52\",   # red\n",
    "}\n",
    "MODEL_SYSTEM = {\n",
    "    \"base\": BASE_SYSTEM_PROMPT,\n",
    "    \"v1\":   TUNED_SYSTEM_PROMPT,\n",
    "    \"v2\":   TUNED_SYSTEM_PROMPT,\n",
    "    \"v3\":   TUNED_SYSTEM_PROMPT,\n",
    "}\n",
    "\n",
    "# ── Directories ────────────────────────────────────────────────────────────\n",
    "CACHE_DIR   = Path(\"./comparison_cache_qwen3_versions\")\n",
    "FIGURES_DIR = Path(\"./comparison_figures_qwen3_versions\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ── Plot style ─────────────────────────────────────────────────────────────\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 120,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#f8f8f8',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.4,\n",
    "    'font.size': 11,\n",
    "})\n",
    "\n",
    "# ── Reference passages ─────────────────────────────────────────────────────\n",
    "# Eight verbatim excerpts from Peterson's books, held out from training.\n",
    "# Used to compute perplexity and TF-IDF similarity.\n",
    "PETERSON_PASSAGES = [\n",
    "    # From Maps of Meaning — the world as a forum for action\n",
    "    \"The world can be validly construed as a forum for action, or as a place of things. \"\n",
    "    \"The former manner of interpretation — more primordial, and less clearly understood — \"\n",
    "    \"finds its expression in the arts or humanities, in ritual, drama, literature, and myth. \"\n",
    "    \"The world as forum for action is a place of value, a place where all things have meaning.\",\n",
    "\n",
    "    # From 12 Rules for Life — embodied courage\n",
    "    \"To stand up straight with your shoulders back is to accept the terrible responsibility \"\n",
    "    \"of life, with eyes wide open. It means deciding to voluntarily transform the chaos of \"\n",
    "    \"potential into the realities of habitable order. It means adopting the burden of \"\n",
    "    \"self-conscious vulnerability, and accepting the end of the unconscious paradise of \"\n",
    "    \"childhood, where finitude and mortality are only dimly comprehended.\",\n",
    "\n",
    "    # From Beyond Order — chaos as unexplored territory\n",
    "    \"Order is the place where the things you are currently doing are working out well \"\n",
    "    \"for you. Chaos is the domain of ignorance itself. It's unexplored territory. Chaos \"\n",
    "    \"is what extends, endlessly and without limit, beyond the boundaries of all states, \"\n",
    "    \"all ideas, and all disciplines. It's the foreigner, the stranger, the member of \"\n",
    "    \"another gang, the rustle in the bushes in the night-time.\",\n",
    "\n",
    "    # From We Who Wrestle with God — logos and truth\n",
    "    \"The divine spark in man is the logos — the word, the reason, the creative principle \"\n",
    "    \"that gives order to the chaos of experience. To act in accordance with the logos is \"\n",
    "    \"to speak the truth, to pursue what is meaningful rather than what is expedient, and \"\n",
    "    \"to take on the burden of Being itself with courage and humility.\",\n",
    "\n",
    "    # From 12 Rules — compare yourself to who you were\n",
    "    \"Compare yourself to who you were yesterday, not to who someone else is today. \"\n",
    "    \"You have a nature. You can play the game of life and improve. You can set a \"\n",
    "    \"standard, even a minimal standard, and try to live up to it. You can improve \"\n",
    "    \"incrementally, moving forward step by step. You can judge your life against \"\n",
    "    \"what you know to be good, against what you should be.\",\n",
    "\n",
    "    # From Maps of Meaning — myth and the hero\n",
    "    \"The great myths and rituals of the past have been formulated in the language of \"\n",
    "    \"the imagination. They say: act out the role of the hero; do not be the villain; \"\n",
    "    \"do not be the tyrant. They say: update your maps of meaning when new information \"\n",
    "    \"warrants it; admit your errors and change. They say: encounter the stranger and \"\n",
    "    \"extract from that encounter what is valuable. Treat the stranger with respect.\",\n",
    "\n",
    "    # From Beyond Order — meaning as balance\n",
    "    \"Meaning is the ultimate balance between, on the one hand, the chaos of transformation \"\n",
    "    \"and possibility and, on the other, the discipline of pristine order, whose purpose is \"\n",
    "    \"to produce out of the attendant chaos a new order that will be even more productive \"\n",
    "    \"and worthwhile than the old. Pursue what is meaningful, not what is expedient.\",\n",
    "\n",
    "    # From We Who Wrestle with God — suffering and transcendence\n",
    "    \"Suffering is not a mistake or an accident. It is the very ground of Being itself. \"\n",
    "    \"To wrestle with God, as Jacob did, is to confront that suffering honestly, to take \"\n",
    "    \"responsibility for it, and to find within it the possibility of transcendence. The \"\n",
    "    \"hero does not flee from the dragon; he faces it and transforms the encounter.\",\n",
    "]\n",
    "\n",
    "# ── Evaluation prompts ─────────────────────────────────────────────────────\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    \"Why is personal responsibility the foundation of a meaningful life?\",\n",
    "    \"How do ancient myths and stories reveal truths about human nature?\",\n",
    "    \"What does it mean to pursue what is meaningful rather than what is expedient?\",\n",
    "    \"How should a person confront suffering rather than flee from it?\",\n",
    "    \"What is the significance of the hero archetype in understanding the human psyche?\",\n",
    "    \"Why is telling the truth essential to a properly functioning life?\",\n",
    "    \"What is the role of the divine or the sacred in organizing human society?\",\n",
    "    \"How does the Jungian concept of the shadow relate to individual development?\",\n",
    "    \"What does it mean to stand up straight with your shoulders back?\",\n",
    "]\n",
    "\n",
    "# ── Peterson keyword dictionary ────────────────────────────────────────────\n",
    "PETERSON_KEYWORDS = list(set([\n",
    "    \"chaos\", \"order\", \"logos\", \"being\", \"meaning\", \"meaningful\", \"meaningless\",\n",
    "    \"transcendence\", \"transcendent\", \"archetype\", \"archetypal\",\n",
    "    \"shadow\", \"anima\", \"animus\", \"unconscious\", \"consciousness\", \"psyche\",\n",
    "    \"individuation\", \"projection\",\n",
    "    \"responsibility\", \"suffering\", \"redemption\", \"courage\", \"virtue\",\n",
    "    \"nihilism\", \"nihilistic\", \"expedient\", \"expedience\", \"tyranny\", \"tyrannical\",\n",
    "    \"sovereignty\", \"heroic\", \"malevolent\",\n",
    "    \"myth\", \"mythological\", \"hero\", \"dragon\", \"narrative\", \"story\",\n",
    "    \"ritual\", \"sacrifice\", \"resurrection\", \"transformation\",\n",
    "    \"divine\", \"sacred\", \"god\", \"biblical\", \"genesis\", \"logos\", \"spirit\",\n",
    "    \"wrestle\", \"jacob\", \"adam\", \"eve\", \"serpent\",\n",
    "    \"confront\", \"hierarchy\", \"dominance\", \"voluntarily\", \"catastrophe\",\n",
    "    \"pathological\", \"resentment\", \"ideological\", \"totalitarian\",\n",
    "]))\n",
    "\n",
    "# ── Results container ─────────────────────────────────────────────────────\n",
    "# Populated by the four inference cells; consumed by the metrics cell.\n",
    "results = {}\n",
    "\n",
    "print(f\"PyTorch  : {torch.__version__}\")\n",
    "print(f\"CUDA     : {torch.cuda.is_available()}  |  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM     : {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB total\")\n",
    "print(f\"Cache    : {CACHE_DIR.resolve()}\")\n",
    "print(f\"Figures  : {FIGURES_DIR.resolve()}\")\n",
    "print()\n",
    "print(\"Cache status:\")\n",
    "for _k in MODEL_KEYS:\n",
    "    _pkl = CACHE_DIR / f\"{_k}_results.pkl\"\n",
    "    _status = \"EXISTS (will skip inference)\" if _pkl.exists() else \"missing (will run inference)\"\n",
    "    print(f\"  {MODEL_DISPLAY[_k]:<42}  {_status}\")\n",
    "print()\n",
    "print(f\"Reference passages  : {len(PETERSON_PASSAGES)}\")\n",
    "print(f\"Evaluation prompts  : {len(EVAL_PROMPTS)}\")\n",
    "print(f\"Keyword dictionary  : {len(PETERSON_KEYWORDS)} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb044e6d",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Helper Functions\n",
    "\n",
    "All four models share the same evaluation pipeline — only the model weights differ.\n",
    "Five helper functions are defined below:\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `generate_response()` | Qwen3 ChatML inference wrapper (strips `<think>` blocks) |\n",
    "| `compute_perplexity()` | Token-level cross-entropy on held-out Peterson passages |\n",
    "| `compute_text_stats()` | Word count, TTR, keyword density per response |\n",
    "| `compute_tfidf_similarity()` | TF-IDF cosine vs reference passages |\n",
    "| `_avg()` | Simple list average helper |\n",
    "\n",
    "### Why the same evaluation for V1?\n",
    "\n",
    "V1 was trained on a **passage-completion** task (assistant = continuation of a\n",
    "passage fragment) rather than Q&A. At inference we apply the same Peterson persona\n",
    "prompt as V2/V3 — even though V1 never saw that prompt during training — to keep\n",
    "the evaluation conditions identical. The Q&A prompts are foreign to V1's training\n",
    "distribution, so expect lower-quality, more passage-like responses.\n",
    "\n",
    "### Critical pattern — `compute_text_stats()`\n",
    "\n",
    "This function **must** append exactly one value per text in the input list, even\n",
    "for empty strings. If any text is skipped, the output lists will be shorter than\n",
    "`len(texts)`, causing index-misalignment crashes in the per-prompt bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt: str,\n",
    "                      system_prompt: str,\n",
    "                      max_new_tokens: int = 300) -> str:\n",
    "    '''\n",
    "    Generate a single response from a Qwen3 (ChatML-format) model.\n",
    "\n",
    "    Qwen3's apply_chat_template() requires a two-step process:\n",
    "      1. Call with tokenize=False and enable_thinking=False to get a plain string.\n",
    "      2. Pass that string to the tokenizer separately to get input_ids.\n",
    "\n",
    "    Even with thinking disabled, the template sometimes adds empty\n",
    "    <think>\\n\\n</think> artifacts, so we strip any such blocks with re.sub.\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts: list,\n",
    "                       max_length: int = 256) -> list:\n",
    "    '''\n",
    "    Compute token-level perplexity for each text in the list.\n",
    "\n",
    "    Perplexity = exp( average negative log-likelihood per token ).\n",
    "    Lower value = model assigns higher probability to the text = more \"in-domain\".\n",
    "\n",
    "    NOTE: We intentionally do NOT pass labels= to the model. Unsloth patches\n",
    "    the forward pass to use a fused CE loss that allocates a ~1.5 GB gradient\n",
    "    buffer for lm_head (vocab_size x hidden_dim) even under torch.no_grad(),\n",
    "    causing OOM on 24 GB cards when the model is already using ~11 GB.\n",
    "    Instead we obtain the logits and compute CE loss manually on CPU.\n",
    "    '''\n",
    "    import torch.nn.functional as F\n",
    "    model.eval()\n",
    "    perplexities = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            enc = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "            ).to(\"cuda\")\n",
    "            out    = model(**enc)                           # no labels → no fused CE\n",
    "            logits = out.logits.cpu().float()               # move to CPU to save VRAM\n",
    "            ids    = enc[\"input_ids\"].cpu()\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = ids[..., 1:].contiguous()\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "            )\n",
    "            perplexities.append(math.exp(loss.item()))\n",
    "            del out, logits, shift_logits, shift_labels, ids, enc\n",
    "    return perplexities\n",
    "\n",
    "\n",
    "def compute_text_stats(texts: list) -> dict:\n",
    "    '''\n",
    "    Compute per-text statistics over a list of model responses.\n",
    "\n",
    "    IMPORTANT: every text in the list must contribute exactly one entry to\n",
    "    each output list — even empty strings — or per-prompt bar charts will\n",
    "    crash with shape mismatches.\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    kw_set     = set(k.lower() for k in PETERSON_KEYWORDS)\n",
    "    word_counts, sentence_counts, ttr_values = [], [], []\n",
    "    keyword_density = []\n",
    "    keyword_counts  = Counter()\n",
    "    all_words       = []\n",
    "\n",
    "    for text in texts:\n",
    "        if text.strip():\n",
    "            words = word_tokenize(text.lower())\n",
    "            sents = sent_tokenize(text)\n",
    "        else:\n",
    "            words, sents = [], []   # empty response → all zeros; do NOT skip\n",
    "\n",
    "        words_alpha = [w for w in words if w.isalpha()]\n",
    "        word_counts.append(len(words_alpha))\n",
    "        sentence_counts.append(len(sents))\n",
    "\n",
    "        ttr = len(set(words_alpha)) / max(len(words_alpha), 1)\n",
    "        ttr_values.append(ttr)\n",
    "\n",
    "        kw_hits = [w for w in words_alpha if w in kw_set]\n",
    "        keyword_density.append(len(kw_hits) / max(len(words_alpha), 1))\n",
    "        keyword_counts.update(kw_hits)\n",
    "\n",
    "        all_words.extend(w for w in words_alpha if w not in stop_words and len(w) > 2)\n",
    "\n",
    "    return {\n",
    "        \"word_counts\":     word_counts,\n",
    "        \"sentence_counts\": sentence_counts,\n",
    "        \"ttr_values\":      ttr_values,\n",
    "        \"keyword_density\": keyword_density,\n",
    "        \"keyword_counts\":  keyword_counts,\n",
    "        \"all_words\":       all_words,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_tfidf_similarity(responses: list, references: list) -> list:\n",
    "    '''\n",
    "    Measure how similar each model response is to Peterson's actual writing\n",
    "    using TF-IDF cosine similarity. Returns the maximum similarity across\n",
    "    all reference passages for each response.\n",
    "    '''\n",
    "    if not responses or not any(r.strip() for r in responses):\n",
    "        return [0.0] * len(responses)\n",
    "    all_texts  = references + responses\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    tfidf      = vectorizer.fit_transform(all_texts)\n",
    "    ref_vecs   = tfidf[:len(references)]\n",
    "    resp_vecs  = tfidf[len(references):]\n",
    "    similarities = []\n",
    "    for i in range(resp_vecs.shape[0]):\n",
    "        sims = cosine_similarity(resp_vecs[i], ref_vecs)[0]\n",
    "        similarities.append(float(sims.max()))\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def _avg(lst): return sum(lst) / len(lst) if lst else 0.0\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0e985",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Qwen3-14B — Base Model\n",
    "\n",
    "The unmodified Qwen3-14B base model with no Peterson fine-tuning. It receives the\n",
    "generic `\"You are a helpful assistant.\"` system prompt — using the Peterson persona\n",
    "prompt for an unfine-tuned model would encourage superficial mimicry rather than\n",
    "showing the model's natural out-of-the-box performance on these topics.\n",
    "\n",
    "This phase establishes the **baseline**: any improvement in V1/V2/V3 can be attributed\n",
    "directly to the respective training interventions. Perplexity on held-out Peterson\n",
    "passages measures how \"surprised\" the model is by his characteristic prose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "_key = \"base\"\n",
    "_pkl = CACHE_DIR / f\"{_key}_results.pkl\"\n",
    "\n",
    "if _pkl.exists():\n",
    "    print(f\"Cache found — skipping {MODEL_DISPLAY[_key]} inference.\")\n",
    "    print(f\"  {_pkl}\")\n",
    "else:\n",
    "    print(f\"Loading {MODEL_DISPLAY[_key]} model ...\")\n",
    "    _model, _tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = MODEL_PATHS[_key],\n",
    "        dtype           = None,\n",
    "        max_seq_length  = 2048,\n",
    "        load_in_4bit    = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)\n",
    "    print(f\"  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "    print(\"\\nComputing perplexity on reference passages ...\")\n",
    "    _ppls = compute_perplexity(_model, _tok, PETERSON_PASSAGES)\n",
    "    print(f\"  Per-passage: {[round(p,1) for p in _ppls]}\")\n",
    "    print(f\"  Average    : {_avg(_ppls):.2f}\")\n",
    "\n",
    "    print(\"\\nGenerating responses ...\")\n",
    "    _resps = []\n",
    "    for _i, _prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{_i+1:02d}/{len(EVAL_PROMPTS)}] {_prompt[:70]}\")\n",
    "        _r = generate_response(_model, _tok, _prompt, MODEL_SYSTEM[_key])\n",
    "        _resps.append(_r)\n",
    "        print(f\"         -> {_r[:100]}\\n\")\n",
    "\n",
    "    with open(_pkl, \"wb\") as _f:\n",
    "        pickle.dump({\"responses\": _resps, \"perplexities\": _ppls}, _f)\n",
    "    print(f\"\\nSaved -> {_pkl}\")\n",
    "\n",
    "    del _model, _tok, _resps, _ppls\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Model unloaded.  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "with open(_pkl, \"rb\") as _f:\n",
    "    results[_key] = pickle.load(_f)\n",
    "print(f\"\\n{MODEL_DISPLAY[_key]} ready.\")\n",
    "print(f\"  Responses    : {len(results[_key]['responses'])}\")\n",
    "print(f\"  Avg PPL      : {_avg(results[_key]['perplexities']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe8d61",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: Qwen3-14B — V1 Fine-Tuned (Passage Completion)\n",
    "\n",
    "**Training task**: The V1 adapter was trained with user = passage fragment, assistant =\n",
    "passage continuation. This is a fundamentally different task from Q&A: the model learned\n",
    "to predict what comes next in Peterson's prose, not to answer a question about his ideas.\n",
    "\n",
    "**System prompt at inference**: We apply the Peterson persona prompt (same as V2/V3) to\n",
    "keep the evaluation fair. V1 never saw this prompt during training, so it operates somewhat\n",
    "outside its training distribution at inference time.\n",
    "\n",
    "**Expected behaviour**: V1 responses may look more like passage continuations than structured\n",
    "answers — longer sentences, possibly starting mid-thought, potentially regurgitating verbatim\n",
    "passages. The Q&A prompts are foreign to its training objective.\n",
    "\n",
    "**LoRA**: r=16, alpha=16 (half the capacity of V2/V3's r=32). Training was 1 epoch\n",
    "(321 steps, 23.3 min, loss 2.44 — a much lower loss than V2/V3 *before* considering\n",
    "that V2/V3 have 3× more gradient updates on a harder task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d314c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key = \"v1\"\n",
    "_pkl = CACHE_DIR / f\"{_key}_results.pkl\"\n",
    "\n",
    "if _pkl.exists():\n",
    "    print(f\"Cache found — skipping {MODEL_DISPLAY[_key]} inference.\")\n",
    "    print(f\"  {_pkl}\")\n",
    "else:\n",
    "    print(f\"Loading {MODEL_DISPLAY[_key]} model ...\")\n",
    "    _model, _tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = MODEL_PATHS[_key],\n",
    "        dtype           = None,\n",
    "        max_seq_length  = 2048,\n",
    "        load_in_4bit    = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)\n",
    "    print(f\"  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "    print(\"\\nComputing perplexity on reference passages ...\")\n",
    "    _ppls = compute_perplexity(_model, _tok, PETERSON_PASSAGES)\n",
    "    print(f\"  Per-passage: {[round(p,1) for p in _ppls]}\")\n",
    "    print(f\"  Average    : {_avg(_ppls):.2f}\")\n",
    "\n",
    "    print(\"\\nGenerating responses ...\")\n",
    "    _resps = []\n",
    "    for _i, _prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{_i+1:02d}/{len(EVAL_PROMPTS)}] {_prompt[:70]}\")\n",
    "        _r = generate_response(_model, _tok, _prompt, MODEL_SYSTEM[_key])\n",
    "        _resps.append(_r)\n",
    "        print(f\"         -> {_r[:100]}\\n\")\n",
    "\n",
    "    with open(_pkl, \"wb\") as _f:\n",
    "        pickle.dump({\"responses\": _resps, \"perplexities\": _ppls}, _f)\n",
    "    print(f\"\\nSaved -> {_pkl}\")\n",
    "\n",
    "    del _model, _tok, _resps, _ppls\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Model unloaded.  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "with open(_pkl, \"rb\") as _f:\n",
    "    results[_key] = pickle.load(_f)\n",
    "print(f\"\\n{MODEL_DISPLAY[_key]} ready.\")\n",
    "print(f\"  Responses    : {len(results[_key]['responses'])}\")\n",
    "print(f\"  Avg PPL      : {_avg(results[_key]['perplexities']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3783f",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Qwen3-14B — V2 Fine-Tuned (Synthetic Q&A, 5,029 pairs)\n",
    "\n",
    "**Training task**: V2 uses synthetic Q&A pairs generated by Claude Haiku — the training\n",
    "task now *matches* the inference task. Each example is (system: Peterson persona, user:\n",
    "question, assistant: verbatim passage that answers it). This is the root fix for the\n",
    "passage-regurgitation problem in V1.\n",
    "\n",
    "**Dataset**: 5,029 Q&A pairs generated from ~2,500 passage chunks across all 4 books.\n",
    "Front-matter pages were **included** — some training examples may contain publisher/\n",
    "copyright text, ISBN numbers, or other non-content material.\n",
    "\n",
    "**LoRA**: r=32, alpha=32 — double the capacity of V1. 3 epochs (1,887 steps, 144.0 min,\n",
    "loss 1.5058, peak VRAM 15.5 GB).\n",
    "\n",
    "**Expected improvement over V1**: Better TF-IDF similarity and keyword density because\n",
    "the model now produces structured answers instead of passage continuations. However,\n",
    "front-matter contamination may produce occasional noise responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a22d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key = \"v2\"\n",
    "_pkl = CACHE_DIR / f\"{_key}_results.pkl\"\n",
    "\n",
    "if _pkl.exists():\n",
    "    print(f\"Cache found — skipping {MODEL_DISPLAY[_key]} inference.\")\n",
    "    print(f\"  {_pkl}\")\n",
    "else:\n",
    "    print(f\"Loading {MODEL_DISPLAY[_key]} model ...\")\n",
    "    _model, _tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = MODEL_PATHS[_key],\n",
    "        dtype           = None,\n",
    "        max_seq_length  = 2048,\n",
    "        load_in_4bit    = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)\n",
    "    print(f\"  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "    print(\"\\nComputing perplexity on reference passages ...\")\n",
    "    _ppls = compute_perplexity(_model, _tok, PETERSON_PASSAGES)\n",
    "    print(f\"  Per-passage: {[round(p,1) for p in _ppls]}\")\n",
    "    print(f\"  Average    : {_avg(_ppls):.2f}\")\n",
    "\n",
    "    print(\"\\nGenerating responses ...\")\n",
    "    _resps = []\n",
    "    for _i, _prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{_i+1:02d}/{len(EVAL_PROMPTS)}] {_prompt[:70]}\")\n",
    "        _r = generate_response(_model, _tok, _prompt, MODEL_SYSTEM[_key])\n",
    "        _resps.append(_r)\n",
    "        print(f\"         -> {_r[:100]}\\n\")\n",
    "\n",
    "    with open(_pkl, \"wb\") as _f:\n",
    "        pickle.dump({\"responses\": _resps, \"perplexities\": _ppls}, _f)\n",
    "    print(f\"\\nSaved -> {_pkl}\")\n",
    "\n",
    "    del _model, _tok, _resps, _ppls\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Model unloaded.  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "with open(_pkl, \"rb\") as _f:\n",
    "    results[_key] = pickle.load(_f)\n",
    "print(f\"\\n{MODEL_DISPLAY[_key]} ready.\")\n",
    "print(f\"  Responses    : {len(results[_key]['responses'])}\")\n",
    "print(f\"  Avg PPL      : {_avg(results[_key]['perplexities']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d257d",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: Qwen3-14B — V3 Fine-Tuned (Synthetic Q&A, 4,867 pairs, clean)\n",
    "\n",
    "**Training task**: Identical to V2 — synthetic Q&A pairs, same architecture, same\n",
    "hyperparameters (r=32, alpha=32, 3 epochs, 2e-4 LR). The *only* difference is the\n",
    "training data.\n",
    "\n",
    "**Dataset**: 4,867 Q&A pairs — 162 fewer than V2 — generated from passages that passed\n",
    "the DataPrep front-matter heuristic. The biggest reduction was in *We Who Wrestle with\n",
    "God* (−103 pairs), indicating that book had the most front-matter contamination.\n",
    "\n",
    "**Training stats**: 1,827 steps, 136.9 min, loss 1.5258, peak VRAM 15.3 GB.\n",
    "Loss is +0.02 vs V2 — negligible, within noise. Fewer training pairs means slightly\n",
    "less gradient signal but the difference is immaterial.\n",
    "\n",
    "**V3 vs V2**: This is the cleanest A/B test in the series — identical everything except\n",
    "data cleanliness. If front-matter removal helps, we expect V3 to show slightly better\n",
    "metrics. If the difference is negligible, it confirms that data purity at this scale\n",
    "has minimal impact compared to the task formulation change (V1→V2).\n",
    "\n",
    "**Known remaining issue**: V3 still contains back-matter content (index pages, figure\n",
    "lists). The front-matter fix alone is insufficient; a back-matter removal step is the\n",
    "next highest-leverage data improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key = \"v3\"\n",
    "_pkl = CACHE_DIR / f\"{_key}_results.pkl\"\n",
    "\n",
    "if _pkl.exists():\n",
    "    print(f\"Cache found — skipping {MODEL_DISPLAY[_key]} inference.\")\n",
    "    print(f\"  {_pkl}\")\n",
    "else:\n",
    "    print(f\"Loading {MODEL_DISPLAY[_key]} model ...\")\n",
    "    _model, _tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = MODEL_PATHS[_key],\n",
    "        dtype           = None,\n",
    "        max_seq_length  = 2048,\n",
    "        load_in_4bit    = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)\n",
    "    print(f\"  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "    print(\"\\nComputing perplexity on reference passages ...\")\n",
    "    _ppls = compute_perplexity(_model, _tok, PETERSON_PASSAGES)\n",
    "    print(f\"  Per-passage: {[round(p,1) for p in _ppls]}\")\n",
    "    print(f\"  Average    : {_avg(_ppls):.2f}\")\n",
    "\n",
    "    print(\"\\nGenerating responses ...\")\n",
    "    _resps = []\n",
    "    for _i, _prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{_i+1:02d}/{len(EVAL_PROMPTS)}] {_prompt[:70]}\")\n",
    "        _r = generate_response(_model, _tok, _prompt, MODEL_SYSTEM[_key])\n",
    "        _resps.append(_r)\n",
    "        print(f\"         -> {_r[:100]}\\n\")\n",
    "\n",
    "    with open(_pkl, \"wb\") as _f:\n",
    "        pickle.dump({\"responses\": _resps, \"perplexities\": _ppls}, _f)\n",
    "    print(f\"\\nSaved -> {_pkl}\")\n",
    "\n",
    "    del _model, _tok, _resps, _ppls\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Model unloaded.  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "with open(_pkl, \"rb\") as _f:\n",
    "    results[_key] = pickle.load(_f)\n",
    "print(f\"\\n{MODEL_DISPLAY[_key]} ready.\")\n",
    "print(f\"  Responses    : {len(results[_key]['responses'])}\")\n",
    "print(f\"  Avg PPL      : {_avg(results[_key]['perplexities']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d09b9f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Metrics Aggregation\n",
    "\n",
    "All four pkl files are now loaded. We compute TF-IDF similarity and text statistics\n",
    "(CPU-only — no GPU required) and print a formatted summary table comparing all\n",
    "metrics across all four versions.\n",
    "\n",
    "**Delta columns** use `▲ ✓` / `▼ ✗` to flag whether each change moved in the\n",
    "expected direction. For perplexity (lower is better), a decrease is `✓`; for all\n",
    "other metrics (higher is better), an increase is `✓`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extract responses and perplexities from results dict ──────────────────\n",
    "all_responses    = {k: results[k][\"responses\"]    for k in MODEL_KEYS}\n",
    "all_perplexities = {k: results[k][\"perplexities\"] for k in MODEL_KEYS}\n",
    "\n",
    "# ── TF-IDF cosine similarity ───────────────────────────────────────────────\n",
    "print(\"Computing TF-IDF similarities ...\")\n",
    "all_similarities = {\n",
    "    k: compute_tfidf_similarity(all_responses[k], PETERSON_PASSAGES)\n",
    "    for k in MODEL_KEYS\n",
    "}\n",
    "\n",
    "# ── Text statistics ────────────────────────────────────────────────────────\n",
    "print(\"Computing text statistics ...\")\n",
    "all_stats = {k: compute_text_stats(all_responses[k]) for k in MODEL_KEYS}\n",
    "\n",
    "# ── Averages ───────────────────────────────────────────────────────────────\n",
    "avg_ppl = {k: _avg(all_perplexities[k])              for k in MODEL_KEYS}\n",
    "avg_sim = {k: _avg(all_similarities[k])               for k in MODEL_KEYS}\n",
    "avg_kd  = {k: _avg(all_stats[k][\"keyword_density\"])   for k in MODEL_KEYS}\n",
    "avg_ttr = {k: _avg(all_stats[k][\"ttr_values\"])        for k in MODEL_KEYS}\n",
    "avg_len = {k: _avg(all_stats[k][\"word_counts\"])       for k in MODEL_KEYS}\n",
    "\n",
    "\n",
    "def pct_change(bv, tv, higher_better=True):\n",
    "    if abs(bv) < 1e-9:\n",
    "        return \"N/A\"\n",
    "    pct   = 100 * (tv - bv) / bv\n",
    "    is_up = pct > 0\n",
    "    ok    = (is_up == higher_better)\n",
    "    return f\"{pct:+.1f}% {'▲' if is_up else '▼'} {'✓' if ok else '✗'}\"\n",
    "\n",
    "\n",
    "# ── Summary table ──────────────────────────────────────────────────────────\n",
    "print()\n",
    "print(f\"{'Metric':<30} {'Base':>12} {'V1':>14} {'V2':>14} {'V3':>14}\")\n",
    "print(\"─\" * 88)\n",
    "print(f\"{'Avg Perplexity (↓)':<30} {avg_ppl['base']:>12.2f} {avg_ppl['v1']:>14.2f} {avg_ppl['v2']:>14.2f} {avg_ppl['v3']:>14.2f}\")\n",
    "print(f\"{'Avg TF-IDF Sim (↑)':<30} {avg_sim['base']:>12.4f} {avg_sim['v1']:>14.4f} {avg_sim['v2']:>14.4f} {avg_sim['v3']:>14.4f}\")\n",
    "print(f\"{'Avg Keyword Density (↑)':<30} {100*avg_kd['base']:>11.2f}% {100*avg_kd['v1']:>13.2f}% {100*avg_kd['v2']:>13.2f}% {100*avg_kd['v3']:>13.2f}%\")\n",
    "print(f\"{'Avg TTR (↑)':<30} {avg_ttr['base']:>12.4f} {avg_ttr['v1']:>14.4f} {avg_ttr['v2']:>14.4f} {avg_ttr['v3']:>14.4f}\")\n",
    "print(f\"{'Avg Response Length (words)':<30} {avg_len['base']:>11.1f}  {avg_len['v1']:>13.1f}  {avg_len['v2']:>13.1f}  {avg_len['v3']:>13.1f}\")\n",
    "print(\"─\" * 88)\n",
    "print()\n",
    "# Delta vs base\n",
    "print(f\"{'Delta vs Base:':<30} {'':>12} {'V1 Δ':>14} {'V2 Δ':>14} {'V3 Δ':>14}\")\n",
    "print(\"─\" * 88)\n",
    "print(f\"{'Perplexity (↓ is ✓)':<30} {'—':>12} {pct_change(avg_ppl['base'], avg_ppl['v1'], False):>14} {pct_change(avg_ppl['base'], avg_ppl['v2'], False):>14} {pct_change(avg_ppl['base'], avg_ppl['v3'], False):>14}\")\n",
    "print(f\"{'TF-IDF Sim (↑ is ✓)':<30} {'—':>12} {pct_change(avg_sim['base'], avg_sim['v1']):>14} {pct_change(avg_sim['base'], avg_sim['v2']):>14} {pct_change(avg_sim['base'], avg_sim['v3']):>14}\")\n",
    "print(f\"{'Keyword Density (↑ is ✓)':<30} {'—':>12} {pct_change(avg_kd['base'], avg_kd['v1']):>14} {pct_change(avg_kd['base'], avg_kd['v2']):>14} {pct_change(avg_kd['base'], avg_kd['v3']):>14}\")\n",
    "print(f\"{'TTR (↑ is ✓)':<30} {'—':>12} {pct_change(avg_ttr['base'], avg_ttr['v1']):>14} {pct_change(avg_ttr['base'], avg_ttr['v2']):>14} {pct_change(avg_ttr['base'], avg_ttr['v3']):>14}\")\n",
    "print(\"─\" * 88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba168542",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 1: Perplexity on Peterson Reference Passages\n",
    "\n",
    "**Left panel** — Per-passage perplexity for all four model versions. Each cluster of\n",
    "four bars corresponds to one held-out reference passage. Fine-tuned models should sit\n",
    "lower in their respective clusters, indicating they assign higher probability to Peterson's\n",
    "prose — i.e., they have \"learned\" his writing.\n",
    "\n",
    "**Right panel** — Average perplexity across all eight passages. This is the most direct\n",
    "measure of domain adaptation. The percentage annotation shows change vs the base model.\n",
    "\n",
    "Note: perplexity is architecture-agnostic — all four models share the same Qwen3-14B\n",
    "base and the same tokenizer, so the numbers are directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cf31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = np.arange(len(PETERSON_PASSAGES))\n",
    "width = 0.18\n",
    "n     = len(MODEL_KEYS)\n",
    "colors = [MODEL_COLORS[k] for k in MODEL_KEYS]\n",
    "labels = [MODEL_SHORT[k]  for k in MODEL_KEYS]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6),\n",
    "                                gridspec_kw={'width_ratios': [3, 1]})\n",
    "fig.suptitle(\"Perplexity on Held-Out Peterson Passages\", fontsize=15, fontweight='bold')\n",
    "\n",
    "for i, (key, color, label) in enumerate(zip(MODEL_KEYS, colors, labels)):\n",
    "    offset = (i - (n - 1) / 2) * width\n",
    "    ax1.bar(x + offset, all_perplexities[key], width,\n",
    "            label=label, color=color, alpha=0.85, edgecolor='white')\n",
    "ax1.set_xlabel(\"Reference Passage\")\n",
    "ax1.set_ylabel(\"Perplexity  (lower = more domain-adapted)\")\n",
    "ax1.set_title(\"Per-Passage Perplexity\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"P{i+1}\" for i in range(len(PETERSON_PASSAGES))], fontsize=10)\n",
    "ax1.legend(fontsize=9, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.4)\n",
    "\n",
    "avgs = [avg_ppl[k] for k in MODEL_KEYS]\n",
    "bars = ax2.bar(range(n), avgs, color=colors, alpha=0.85, edgecolor='white', width=0.6)\n",
    "base_ppl = avg_ppl[\"base\"]\n",
    "for bar, v, k in zip(bars, avgs, MODEL_KEYS):\n",
    "    ann = f\"{v:.1f}\"\n",
    "    if k != \"base\":\n",
    "        ann += f\"\\n({100*(v-base_ppl)/base_ppl:+.1f}%)\"\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, v + 0.2,\n",
    "             ann, ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "ax2.set_xticks(range(n))\n",
    "ax2.set_xticklabels(labels, fontsize=9)\n",
    "ax2.set_ylabel(\"Average Perplexity\")\n",
    "ax2.set_title(\"Average Perplexity\\n(% change vs Base)\")\n",
    "ax2.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"01_perplexity.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 01_perplexity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d9e0b",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 2: TF-IDF Cosine Similarity to Peterson's Writing\n",
    "\n",
    "For each model response to each evaluation prompt, we compute the cosine similarity\n",
    "between its TF-IDF vector and each of the eight reference passages, then take the\n",
    "**maximum** — so a response only needs to resemble one passage to score well.\n",
    "\n",
    "**Higher = the model's word choices are more similar to how Peterson actually writes.**\n",
    "\n",
    "We expect V2/V3 to improve substantially over V1 on this metric because Q&A training\n",
    "produces structured responses with Peterson vocabulary, whereas V1's passage continuations\n",
    "may have different vocabulary distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df998eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = np.arange(len(EVAL_PROMPTS))\n",
    "width = 0.18\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6),\n",
    "                                gridspec_kw={'width_ratios': [3, 1]})\n",
    "fig.suptitle(\"TF-IDF Cosine Similarity to Peterson's Writing\",\n",
    "             fontsize=15, fontweight='bold')\n",
    "\n",
    "for i, (key, color, label) in enumerate(zip(MODEL_KEYS, colors, labels)):\n",
    "    offset = (i - (n - 1) / 2) * width\n",
    "    ax1.bar(x + offset, all_similarities[key], width,\n",
    "            label=label, color=color, alpha=0.85, edgecolor='white')\n",
    "ax1.set_xlabel(\"Evaluation Prompt\")\n",
    "ax1.set_ylabel(\"TF-IDF Cosine Similarity  (higher = more Peterson-like)\")\n",
    "ax1.set_title(\"Per-Prompt TF-IDF Similarity\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))], fontsize=10)\n",
    "ax1.legend(fontsize=9, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.4)\n",
    "\n",
    "avgs = [avg_sim[k] for k in MODEL_KEYS]\n",
    "bars = ax2.bar(range(n), avgs, color=colors, alpha=0.85, edgecolor='white', width=0.6)\n",
    "for bar, v in zip(bars, avgs):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, v + 0.001,\n",
    "             f\"{v:.4f}\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "ax2.set_xticks(range(n))\n",
    "ax2.set_xticklabels(labels, fontsize=9)\n",
    "ax2.set_ylabel(\"Average TF-IDF Similarity\")\n",
    "ax2.set_title(\"Average TF-IDF Similarity\")\n",
    "ax2.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"02_tfidf_similarity.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 02_tfidf_similarity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6247b",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 3: Peterson Keyword Density\n",
    "\n",
    "Keyword density = (Peterson-signature words in response) ÷ (total words).\n",
    "\n",
    "This is a more *targeted* measure than TF-IDF: we are specifically asking \"does the\n",
    "model spontaneously use terms like chaos, logos, archetype, hierarchy, sovereignty,\n",
    "etc.?\" High density suggests the fine-tuning has wired those concepts into the model's\n",
    "first-choice vocabulary.\n",
    "\n",
    "For Q&A-trained models (V2/V3), we expect higher density because the model has learned\n",
    "to answer questions *using* Peterson's conceptual vocabulary. V1 (passage completion)\n",
    "may score lower if it primarily outputs narrative prose that relies on different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = np.arange(len(EVAL_PROMPTS))\n",
    "width = 0.18\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6),\n",
    "                                gridspec_kw={'width_ratios': [3, 1]})\n",
    "fig.suptitle(\"Peterson Keyword Density per Prompt\", fontsize=15, fontweight='bold')\n",
    "\n",
    "for i, (key, color, label) in enumerate(zip(MODEL_KEYS, colors, labels)):\n",
    "    offset = (i - (n - 1) / 2) * width\n",
    "    ax1.bar(x + offset,\n",
    "            [100 * v for v in all_stats[key][\"keyword_density\"]],\n",
    "            width, label=label, color=color, alpha=0.85, edgecolor='white')\n",
    "ax1.set_xlabel(\"Evaluation Prompt\")\n",
    "ax1.set_ylabel(\"Keyword Density  (%)\")\n",
    "ax1.set_title(\"Per-Prompt Keyword Density\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))], fontsize=10)\n",
    "ax1.legend(fontsize=9, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.4)\n",
    "\n",
    "avgs = [100 * avg_kd[k] for k in MODEL_KEYS]\n",
    "bars = ax2.bar(range(n), avgs, color=colors, alpha=0.85, edgecolor='white', width=0.6)\n",
    "for bar, v in zip(bars, avgs):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, v + 0.05,\n",
    "             f\"{v:.2f}%\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "ax2.set_xticks(range(n))\n",
    "ax2.set_xticklabels(labels, fontsize=9)\n",
    "ax2.set_ylabel(\"Average Keyword Density  (%)\")\n",
    "ax2.set_title(\"Average Keyword Density\")\n",
    "ax2.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"03_keyword_density.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 03_keyword_density.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9197d",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 4: Response Characteristics\n",
    "\n",
    "Three-panel chart showing per-model averages for:\n",
    "\n",
    "- **Word count** — how verbose is each version?\n",
    "- **Sentence count** — structural complexity of responses\n",
    "- **Type-Token Ratio (TTR)** — vocabulary richness (unique/total words).\n",
    "  Higher TTR = more varied word choice. Peterson is known for elaborate, varied\n",
    "  prose, so higher TTR suggests stylistic alignment.\n",
    "\n",
    "Note that V1's passage-completion training may produce longer responses (full passage\n",
    "continuations) while V2/V3 may produce more concise, structured Q&A answers. This\n",
    "difference in response style is a direct consequence of the training task change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(\"Response Characteristics — All 4 Qwen3-14B Versions\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "stat_keys    = [\"word_counts\",     \"sentence_counts\",  \"ttr_values\"]\n",
    "stat_titles  = [\"Avg Word Count\",  \"Avg Sentence Count\",\"Avg Type-Token Ratio\"]\n",
    "stat_ylabels = [\"Words\",           \"Sentences\",         \"TTR  (unique/total words)\"]\n",
    "\n",
    "for ax, stat_key, title, ylabel in zip(axes, stat_keys, stat_titles, stat_ylabels):\n",
    "    avgs = [_avg(all_stats[k][stat_key]) for k in MODEL_KEYS]\n",
    "    bars = ax.bar(range(n), avgs, color=colors, alpha=0.85,\n",
    "                  edgecolor='white', width=0.6)\n",
    "    for bar, v in zip(bars, avgs):\n",
    "        fmt = f\"{v:.3f}\" if \"ttr\" in stat_key else f\"{v:.1f}\"\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, v * 1.02,\n",
    "                fmt, ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_xticklabels(labels, fontsize=8)\n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(axis='y', alpha=0.4)\n",
    "\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=MODEL_COLORS[k], label=MODEL_DISPLAY[k])\n",
    "    for k in MODEL_KEYS\n",
    "]\n",
    "fig.legend(handles=legend_patches, loc='lower center', ncol=4,\n",
    "           fontsize=9, bbox_to_anchor=(0.5, -0.08))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"04_response_characteristics.png\",\n",
    "            bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 04_response_characteristics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb1b57",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 5: Word Clouds\n",
    "\n",
    "Word clouds show the most frequent *content* words (stop words removed) in each\n",
    "model's aggregate responses. Peterson's vocabulary centres on a distinctive cluster\n",
    "— order, chaos, meaning, responsibility, hero, archetype — so we expect to see those\n",
    "words dominate the fine-tuned clouds.\n",
    "\n",
    "A visual comparison of all four clouds reveals:\n",
    "- Whether the base model spontaneously uses Peterson-specific vocabulary\n",
    "- Whether V1's passage-completion training produced different word distributions than\n",
    "  V2/V3's Q&A training\n",
    "- Whether V2 and V3 clouds look nearly identical (expected given similar training loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle(\"Word Clouds — Content Word Frequency per Model Version\",\n",
    "             fontsize=15, fontweight='bold')\n",
    "\n",
    "for ax, key in zip(axes.flat, MODEL_KEYS):\n",
    "    words = all_stats[key][\"all_words\"]\n",
    "    if words:\n",
    "        wc = WordCloud(\n",
    "            width=600, height=400,\n",
    "            background_color='white',\n",
    "            colormap='viridis',\n",
    "            max_words=80,\n",
    "            collocations=False,\n",
    "            stopwords=STOPWORDS,\n",
    "        ).generate(' '.join(words))\n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"(no content words)\", ha='center', va='center',\n",
    "                transform=ax.transAxes, fontsize=14, color='grey')\n",
    "    ax.set_title(MODEL_DISPLAY[key], fontsize=12, fontweight='bold',\n",
    "                 color=MODEL_COLORS[key])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"05_wordclouds.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 05_wordclouds.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d1749",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 6: Peterson Keyword Heatmap (per prompt × per keyword)\n",
    "\n",
    "Each cell shows the raw count of a specific Peterson keyword in the response to a\n",
    "specific prompt. The top-20 most-used keywords (pooled across all four models) are\n",
    "shown on the x-axis; the 10 prompts on the y-axis.\n",
    "\n",
    "Comparing the four heatmaps side by side reveals:\n",
    "- Which keywords the fine-tuned models spontaneously use more\n",
    "- Whether V1's passage-completion training concentrates on different keywords than V2/V3\n",
    "- Whether V2 and V3 have similar keyword distributions (expected)\n",
    "- Any anomalous cells where a model produces the wrong type of content (e.g., V3's\n",
    "  Q2 index contamination would show as a cell with zero Peterson keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_set = set(k.lower() for k in PETERSON_KEYWORDS)\n",
    "\n",
    "\n",
    "def per_prompt_kw_matrix(responses, keywords):\n",
    "    '''Return (n_prompts x n_keywords) integer matrix of keyword hit counts.'''\n",
    "    mat = []\n",
    "    for resp in responses:\n",
    "        words = word_tokenize(resp.lower()) if resp.strip() else []\n",
    "        words_alpha = [w for w in words if w.isalpha()]\n",
    "        mat.append([words_alpha.count(kw) for kw in keywords])\n",
    "    return np.array(mat)\n",
    "\n",
    "\n",
    "all_kw_counter = Counter()\n",
    "for key in MODEL_KEYS:\n",
    "    for resp in all_responses[key]:\n",
    "        for w in word_tokenize(resp.lower()):\n",
    "            if w in kw_set:\n",
    "                all_kw_counter[w] += 1\n",
    "top20 = [kw for kw, _ in all_kw_counter.most_common(20)]\n",
    "\n",
    "if not top20:\n",
    "    print(\"No Peterson keywords found — skipping heatmap.\")\n",
    "else:\n",
    "    matrices = {k: per_prompt_kw_matrix(all_responses[k], top20) for k in MODEL_KEYS}\n",
    "    vmax = max(m.max() for m in matrices.values() if m.size > 0) or 1\n",
    "\n",
    "    p_labels = [f\"Q{i+1}: {EVAL_PROMPTS[i][:28]}...\" for i in range(len(EVAL_PROMPTS))]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14), sharey=True)\n",
    "    fig.suptitle(\"Peterson Keyword Usage Heatmap — All 4 Model Versions\",\n",
    "                 fontsize=15, fontweight='bold')\n",
    "\n",
    "    for ax, key in zip(axes.flat, MODEL_KEYS):\n",
    "        mat = matrices[key]\n",
    "        im = ax.imshow(mat, aspect='auto', cmap='YlOrRd', vmin=0, vmax=vmax)\n",
    "        ax.set_xticks(range(len(top20)))\n",
    "        ax.set_xticklabels(top20, rotation=45, ha='right', fontsize=8)\n",
    "        ax.set_yticks(range(len(p_labels)))\n",
    "        ax.set_yticklabels(p_labels, fontsize=8)\n",
    "        ax.set_title(MODEL_DISPLAY[key], fontsize=11, fontweight='bold',\n",
    "                     color=MODEL_COLORS[key])\n",
    "        ax.set_xlabel(\"Peterson Keyword\")\n",
    "        for i in range(mat.shape[0]):\n",
    "            for j in range(mat.shape[1]):\n",
    "                if mat[i, j] > 0:\n",
    "                    ax.text(j, i, str(mat[i, j]), ha='center', va='center',\n",
    "                            fontsize=7,\n",
    "                            color='white' if mat[i, j] > vmax * 0.6 else 'black')\n",
    "        plt.colorbar(im, ax=ax, label=\"Keyword count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"06_keyword_heatmap.png\", bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved: 06_keyword_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a334a",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 7: Radar / Spider Chart — Overall \"Peterson-Likeness\"\n",
    "\n",
    "The radar chart summarises all five metrics simultaneously for all four model versions\n",
    "on a single normalised scale [0.1, 1.0]. All metrics are oriented so that\n",
    "**larger = more Peterson-like**:\n",
    "\n",
    "| Spoke | Raw metric | Orientation |\n",
    "|-------|-----------|-------------|\n",
    "| Perplexity (inverted) | Lower PPL → higher score | inverted |\n",
    "| TF-IDF Similarity | Higher cos-sim → higher score | normal |\n",
    "| Keyword Density | Higher % → higher score | normal |\n",
    "| Vocabulary Richness (TTR) | Higher TTR → higher score | normal |\n",
    "| Response Length | Longer responses → higher score | normal |\n",
    "\n",
    "Normalisation uses min–max across all four models simultaneously, so the chart shows\n",
    "*relative* differences rather than absolute magnitudes. A larger polygon area means\n",
    "the model scores more consistently across all five axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d650c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm4(values, higher_better=True):\n",
    "    '''Normalise four values to [0.1, 1.0] with optional inversion.'''\n",
    "    lo, hi = min(values), max(values)\n",
    "    if abs(hi - lo) < 1e-9:\n",
    "        return [0.5] * len(values)\n",
    "    normed = [(v - lo) / (hi - lo) for v in values]\n",
    "    if not higher_better:\n",
    "        normed = [1.0 - v for v in normed]\n",
    "    return [0.1 + 0.9 * v for v in normed]\n",
    "\n",
    "\n",
    "radar_metrics = [\n",
    "    (\"Perplexity\\n(inverted)\",\n",
    "     norm4([avg_ppl[k] for k in MODEL_KEYS], higher_better=False)),\n",
    "    (\"TF-IDF\\nSimilarity\",\n",
    "     norm4([avg_sim[k] for k in MODEL_KEYS], higher_better=True)),\n",
    "    (\"Keyword\\nDensity\",\n",
    "     norm4([avg_kd[k]  for k in MODEL_KEYS], higher_better=True)),\n",
    "    (\"Vocabulary\\nRichness (TTR)\",\n",
    "     norm4([avg_ttr[k] for k in MODEL_KEYS], higher_better=True)),\n",
    "    (\"Response\\nLength\",\n",
    "     norm4([avg_len[k] for k in MODEL_KEYS], higher_better=True)),\n",
    "]\n",
    "\n",
    "spoke_labels = [m[0] for m in radar_metrics] + [radar_metrics[0][0]]\n",
    "angles       = np.linspace(0, 2 * np.pi, len(spoke_labels), endpoint=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9), subplot_kw=dict(polar=True))\n",
    "fig.suptitle(\n",
    "    \"Radar Summary: Peterson-Likeness Across All Metrics\\n(Qwen3-14B: Base vs V1 vs V2 vs V3)\",\n",
    "    fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "for i, key in enumerate(MODEL_KEYS):\n",
    "    values = [m[1][i] for m in radar_metrics] + [radar_metrics[0][1][i]]\n",
    "    ax.plot(angles, values, color=MODEL_COLORS[key], lw=2.5,\n",
    "            label=MODEL_DISPLAY[key])\n",
    "    ax.fill(angles, values, color=MODEL_COLORS[key], alpha=0.08)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(spoke_labels[:-1], size=11)\n",
    "ax.set_yticks([0.25, 0.5, 0.75, 1.0])\n",
    "ax.set_yticklabels(['0.25', '0.5', '0.75', '1.0'], size=7, color='grey')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.45, 1.15), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"07_radar_summary.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 07_radar_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e97f07",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Version Progression Analysis\n",
    "\n",
    "This section is unique to this notebook. Instead of treating the four models as\n",
    "independent groups (like the bar charts above), we examine them as a **progression**:\n",
    "Base → V1 → V2 → V3 — each step representing a deliberate training intervention.\n",
    "\n",
    "**Line chart**: V1 → V2 → V3 on the x-axis, one line per metric (normalised so all\n",
    "metrics fit on the same scale). The base model is shown as a horizontal dashed reference\n",
    "line for each metric.\n",
    "\n",
    "**Delta table**: metric × version transition (Base→V1, V1→V2, V2→V3) with directional\n",
    "arrows and expected-direction checks. This makes it easy to answer the three research\n",
    "questions:\n",
    "1. **V1→V2** (Base→V1 for TF-IDF/density, V1→V2 for task change): did Q&A training help?\n",
    "2. **V2→V3**: did cleaner data produce measurably different results?\n",
    "3. **Base vs all fine-tuned**: how much did fine-tuning help overall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Normalise all metrics to [0, 1] for the line chart ────────────────────\n",
    "def _norm01(values, higher_better=True):\n",
    "    lo, hi = min(values), max(values)\n",
    "    if abs(hi - lo) < 1e-9:\n",
    "        return [0.5] * len(values)\n",
    "    normed = [(v - lo) / (hi - lo) for v in values]\n",
    "    if not higher_better:\n",
    "        normed = [1.0 - v for v in normed]\n",
    "    return normed\n",
    "\n",
    "\n",
    "tuned_keys  = [\"v1\", \"v2\", \"v3\"]\n",
    "x_pos       = [1, 2, 3]\n",
    "x_labels    = [\"V1\\n(1-ep completion)\", \"V2\\n(3-ep Q&A)\", \"V3\\n(3-ep clean)\"]\n",
    "all_keys    = [\"base\"] + tuned_keys\n",
    "\n",
    "metric_specs = [\n",
    "    # (label, avg_dict, higher_better, subplot_col)\n",
    "    (\"Perplexity\\n(↓ better)\", avg_ppl, False),\n",
    "    (\"TF-IDF Similarity\\n(↑ better)\", avg_sim, True),\n",
    "    (\"Keyword Density\\n(↑ better)\", avg_kd, True),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(\"Version Progression: Base Reference + V1 → V2 → V3\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax, (metric_label, metric_dict, higher_better) in zip(axes, metric_specs):\n",
    "    all_vals  = [metric_dict[k] for k in all_keys]\n",
    "    norm_vals = _norm01(all_vals, higher_better)\n",
    "\n",
    "    base_norm   = norm_vals[0]\n",
    "    tuned_norms = norm_vals[1:]  # v1, v2, v3\n",
    "\n",
    "    # Base model as horizontal dashed reference line\n",
    "    ax.axhline(y=base_norm, color=MODEL_COLORS[\"base\"], linestyle='--', lw=2,\n",
    "               label=f\"Base (ref)\")\n",
    "\n",
    "    # Line through V1 → V2 → V3\n",
    "    ax.plot(x_pos, tuned_norms, 'o-', color='#7B2D8B', lw=2.5, ms=8,\n",
    "            label=\"Fine-tuned\")\n",
    "\n",
    "    raw_vals = [metric_dict[k] for k in tuned_keys]\n",
    "    for x, y_norm, y_raw, k in zip(x_pos, tuned_norms, raw_vals, tuned_keys):\n",
    "        ax.annotate(f\"{y_raw:.3f}\", (x, y_norm),\n",
    "                    textcoords=\"offset points\", xytext=(6, 4),\n",
    "                    fontsize=8, color=MODEL_COLORS[k])\n",
    "    # Also annotate base\n",
    "    ax.annotate(f\"{metric_dict['base']:.3f}\", (0.5, base_norm),\n",
    "                textcoords=\"offset points\", xytext=(4, 4),\n",
    "                fontsize=8, color=MODEL_COLORS[\"base\"])\n",
    "\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(x_labels, fontsize=9)\n",
    "    ax.set_xlim(0.5, 3.5)\n",
    "    ax.set_ylim(-0.1, 1.2)\n",
    "    ax.set_title(metric_label, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(\"Normalised score (0=worst, 1=best)\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"08_version_progression.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 08_version_progression.png\")\n",
    "\n",
    "# ── Delta table ────────────────────────────────────────────────────────────\n",
    "def fmt_delta(old_v, new_v, higher_better=True):\n",
    "    if abs(old_v) < 1e-9:\n",
    "        return \"N/A\"\n",
    "    pct   = 100 * (new_v - old_v) / old_v\n",
    "    is_up = pct > 0\n",
    "    ok    = (is_up == higher_better)\n",
    "    return f\"{pct:+.1f}% {'▲' if is_up else '▼'} {'✓' if ok else '✗'}\"\n",
    "\n",
    "\n",
    "transitions = [\n",
    "    (\"Base → V1\", \"base\", \"v1\"),\n",
    "    (\"V1  → V2\",  \"v1\",   \"v2\"),\n",
    "    (\"V2  → V3\",  \"v2\",   \"v3\"),\n",
    "    (\"Base → V3\", \"base\", \"v3\"),\n",
    "]\n",
    "\n",
    "print(\"\\nVersion Transition Delta Table\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Transition':<14} {'Perplexity (↓✓)':>18} {'TF-IDF (↑✓)':>16} {'Kwd Density (↑✓)':>18} {'TTR (↑✓)':>14}\")\n",
    "print(\"-\" * 90)\n",
    "for label, from_k, to_k in transitions:\n",
    "    ppl_d = fmt_delta(avg_ppl[from_k], avg_ppl[to_k], higher_better=False)\n",
    "    sim_d = fmt_delta(avg_sim[from_k], avg_sim[to_k], higher_better=True)\n",
    "    kd_d  = fmt_delta(avg_kd[from_k],  avg_kd[to_k],  higher_better=True)\n",
    "    ttr_d = fmt_delta(avg_ttr[from_k], avg_ttr[to_k], higher_better=True)\n",
    "    print(f\"{label:<14} {ppl_d:>18} {sim_d:>16} {kd_d:>18} {ttr_d:>14}\")\n",
    "print(\"=\" * 90)\n",
    "print(\"✓ = change in expected direction  |  ✗ = unexpected  |  ▲/▼ = direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a07c6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Side-by-Side Response Comparison\n",
    "\n",
    "Quantitative metrics tell only part of the story. Below we print all four model\n",
    "responses to three selected prompts to allow direct qualitative comparison.\n",
    "\n",
    "**Prompts chosen to maximise observable differences:**\n",
    "\n",
    "1. *\"What is the relationship between order and chaos in human experience?\"* — Core\n",
    "   Peterson theme; all models should have strong signal on this one.\n",
    "2. *\"Why is personal responsibility the foundation of a meaningful life?\"* — The prompt\n",
    "   where V3 was observed to produce raw index text in prior evaluation; reveals\n",
    "   back-matter contamination vs substantive response.\n",
    "3. *\"How should a person confront suffering rather than flee from it?\"* — Suffering and\n",
    "   transcendence is a key theme from *We Who Wrestle with God* — the book with the\n",
    "   most front-matter contamination difference between V2 and V3.\n",
    "\n",
    "**What to look for:**\n",
    "- Does V1 produce passage-like continuations vs structured Q&A answers?\n",
    "- Do V2 and V3 produce qualitatively similar responses (suggesting data cleanliness\n",
    "  has minimal impact on output style)?\n",
    "- Does any model produce index/figure-list text (indicating back-matter contamination)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea744df",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARISON_PROMPT_INDICES = [0, 1, 4]  # Q1, Q2, Q5\n",
    "\n",
    "for idx in COMPARISON_PROMPT_INDICES:\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"PROMPT Q{idx+1}: {EVAL_PROMPTS[idx]}\")\n",
    "    print(\"=\" * 100)\n",
    "    for key in MODEL_KEYS:\n",
    "        resp = all_responses[key][idx]\n",
    "        print(f\"\\n[{MODEL_DISPLAY[key]}]\")\n",
    "        print(resp if resp.strip() else \"(empty response)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36bce3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Conclusions\n",
    "\n",
    "### Research Question 1 — Did Q&A training (V1→V2) improve answering ability?\n",
    "\n",
    "The V1→V2 transition changed both the training task (completion → Q&A) *and*\n",
    "the LoRA rank (r=16 → r=32) and epoch count (1 → 3). Any improvement in TF-IDF\n",
    "similarity and keyword density in V2 vs V1 should be attributed to all three changes\n",
    "together. The most likely dominant factor is the task change: V1 learned to continue\n",
    "passages; V2 learned to answer questions using Peterson's vocabulary.\n",
    "\n",
    "If TF-IDF and keyword density improved V1→V2, the answer to Q1 is **yes**. If V1's\n",
    "passage-completion model happened to score similarly on these metrics, it would suggest\n",
    "that domain vocabulary is being learned regardless of task framing — an interesting null\n",
    "result in its own right.\n",
    "\n",
    "### Research Question 2 — Did clean data (V2→V3) produce measurably different results?\n",
    "\n",
    "V2 and V3 are the cleanest A/B comparison: identical architecture, identical\n",
    "hyperparameters, training loss within 0.02 of each other (negligible). Any metric\n",
    "difference is attributable to the 162-pair data reduction and front-matter cleanup.\n",
    "\n",
    "If V2 and V3 scores are nearly identical, the conclusion is that **front-matter removal\n",
    "at this dataset scale has minimal impact on inference quality** — and the next\n",
    "highest-leverage data improvement is back-matter removal (index pages, figure lists).\n",
    "\n",
    "### Research Question 3 — How do fine-tuned versions compare to the base model?\n",
    "\n",
    "All fine-tuned models share the same Qwen3-14B base architecture and tokenizer,\n",
    "making perplexity directly comparable. Any version showing lower perplexity on\n",
    "held-out Peterson passages has learned something about his writing style.\n",
    "\n",
    "### V3 Index Contamination\n",
    "\n",
    "The known V3 Q2 anomaly (raw index text output) is visible in the response comparison\n",
    "above. This confirms that the training data still contains index/back-matter pages.\n",
    "The front-matter heuristic in `JordanPeterson_DataPrep.ipynb` removes content *before*\n",
    "Chapter 1 but does not handle content *after* the final chapter.\n",
    "\n",
    "**Next step**: Add a back-matter removal heuristic (stop extracting at the last chapter\n",
    "header or first \"Index\" section header) and regenerate the Q&A cache as V4 training data.\n",
    "\n",
    "### Adding any of these models to AllModels comparison\n",
    "\n",
    "To include any Qwen3 version in `AllModels_JordanPeterson_Comparison.ipynb`:\n",
    "1. Add the key to `MODEL_KEYS` in the AllModels config cell\n",
    "2. Add its path to `MODEL_PATHS`\n",
    "3. Add display name, colour, and system prompt\n",
    "4. The pkl format is identical — copy from `comparison_cache_qwen3_versions/` to\n",
    "   `comparison_cache_all_models/` to skip re-inference\n",
    "5. Adjust figure layout if needed (currently 2×2 → may need 2×3 or 3×2 for 6 models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
