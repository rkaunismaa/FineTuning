{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT-OSS 20B on Jordan Peterson's Books Using Unsloth\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook fine-tunes the **`unsloth/gpt-oss-20b-unsloth-bnb-4bit`** model using text extracted from Jordan Peterson's books. We use **Unsloth** to make training 2x faster and use significantly less VRAM than standard approaches.\n",
    "\n",
    "### What is Fine-Tuning?\n",
    "\n",
    "Fine-tuning is the process of taking a pre-trained language model (one that already understands language) and training it further on a specific dataset so it learns the style, knowledge, and patterns from that data. Think of it like this: the base model went to \"general school\" and learned language broadly. Fine-tuning is like sending it to a \"specialized course\" on Jordan Peterson's writings.\n",
    "\n",
    "### What is Unsloth?\n",
    "\n",
    "**[Unsloth](https://github.com/unslothai/unsloth)** is an open-source library that makes fine-tuning large language models (LLMs) significantly faster and more memory-efficient. It achieves 2x faster training with up to 80% less VRAM usage compared to standard methods. This is critical because LLMs are enormous and typically require expensive hardware to train.\n",
    "\n",
    "### What is LoRA (Low-Rank Adaptation)?\n",
    "\n",
    "Instead of updating ALL 20 billion parameters in the model (which would require enormous amounts of memory and compute), LoRA adds small \"adapter\" layers that contain only a fraction of the parameters. We train only these adapters while keeping the original model frozen. This means we train roughly 0.02% of the total parameters while still getting meaningful improvements in the model's behavior.\n",
    "\n",
    "### What is 4-bit Quantization?\n",
    "\n",
    "Normally, each model parameter is stored as a 16-bit or 32-bit floating-point number. 4-bit quantization compresses each parameter down to just 4 bits, reducing the model's memory footprint by 4-8x. The `bnb-4bit` in the model name means it uses **bitsandbytes** library for this quantization. This allows a 20B parameter model to fit in ~12GB of VRAM instead of the ~40GB it would normally require.\n",
    "\n",
    "### Our Data Source\n",
    "\n",
    "We will extract text from four Jordan Peterson books (PDFs):\n",
    "1. **Maps of Meaning: The Architecture of Belief** (1999)\n",
    "2. **12 Rules for Life: An Antidote to Chaos** (2018)\n",
    "3. **Beyond Order: 12 More Rules For Life**\n",
    "4. **We Who Wrestle with God: Perceptions of the Divine**\n",
    "\n",
    "The extracted text will be formatted into a conversational dataset suitable for supervised fine-tuning (SFT).\n",
    "\n",
    "### Hardware\n",
    "\n",
    "This notebook is designed to run on an **NVIDIA RTX 4090** (24GB VRAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Verify Environment and Imports\n",
    "\n",
    "Before we start, let's verify that our Python environment has everything we need. We're using the `.finetuning` virtual environment which already has Unsloth and its dependencies installed.\n",
    "\n",
    "We also need **PyMuPDF** (`fitz`) to extract text from PDF files. PyMuPDF is one of the fastest and most reliable PDF text extraction libraries for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify key packages are available\n",
    "import importlib\n",
    "\n",
    "required_packages = {\n",
    "    'unsloth': 'Unsloth - fast fine-tuning library',\n",
    "    'torch': 'PyTorch - deep learning framework',\n",
    "    'transformers': 'HuggingFace Transformers - model loading and tokenization',\n",
    "    'peft': 'PEFT - parameter-efficient fine-tuning (LoRA)',\n",
    "    'trl': 'TRL - transformer reinforcement learning / SFT trainer',\n",
    "    'datasets': 'HuggingFace Datasets - dataset handling',\n",
    "    'fitz': 'PyMuPDF - PDF text extraction',\n",
    "}\n",
    "\n",
    "print(\"Checking required packages:\\n\")\n",
    "for pkg, description in required_packages.items():\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        version = getattr(m, '__version__', 'installed')\n",
    "        print(f\"  {pkg}: {version} -- {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"  {pkg}: NOT FOUND -- {description}\")\n",
    "        print(f\"    -> Install with: pip install {pkg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and specs\n",
    "# This is critical - fine-tuning requires a CUDA-capable GPU\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Fine-tuning will not work without a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Step 2: Extract Text from Jordan Peterson's Books\n\nWe need to convert the PDF books into plain text that the model can learn from. Here's what happens in this step:\n\n1. **Read each PDF** using PyMuPDF (`fitz`), which parses the PDF format and extracts raw text from each page.\n2. **Clean the text** by removing excessive whitespace, headers/footers, and other artifacts that commonly appear in PDF extraction.\n3. **Chunk the text** into passages of manageable length. LLMs have a maximum sequence length (we'll use 2048 tokens), so we need to break the books into pieces that fit within this limit.\n\n### Why Chunk the Text?\n\nThe model can only process a limited number of tokens at a time (our `max_seq_length` of 2048 tokens). A whole book would be hundreds of thousands of tokens. By splitting into chunks, each training example fits within the model's context window. We use overlapping chunks so that ideas that span chunk boundaries aren't lost.\n\n**Important note on token budget:** The GPT-OSS chat template adds a substantial system preamble (~150 tokens) on top of our system prompt and user message. We need to account for this overhead when sizing our chunks, so we keep chunks to ~350 words (~470 tokens) to stay safely within the 2048 limit."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the Jordan Peterson books\n",
    "BOOKS_DIR = Path(\"../../Books/JordanPeterson\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract all text from a PDF file using PyMuPDF.\n",
    "    \n",
    "    PyMuPDF reads each page of the PDF and extracts the text content.\n",
    "    This works well for text-based PDFs (not scanned images).\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        The full text content of the PDF as a single string\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_parts = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "        if text.strip():  # Only include pages that have text\n",
    "            text_parts.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean extracted PDF text by removing common artifacts.\n",
    "    \n",
    "    PDF extraction often introduces artifacts like:\n",
    "    - Excessive newlines from page layouts\n",
    "    - Page numbers and headers/footers\n",
    "    - Multiple consecutive spaces\n",
    "    - Non-printable characters\n",
    "    \n",
    "    This function normalizes the text while preserving paragraph structure.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text extracted from PDF\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text with normalized whitespace\n",
    "    \"\"\"\n",
    "    # Remove non-printable characters (except newlines and tabs)\n",
    "    text = re.sub(r'[^\\x20-\\x7E\\n\\t]', ' ', text)\n",
    "    \n",
    "    # Replace tabs with spaces\n",
    "    text = text.replace('\\t', ' ')\n",
    "    \n",
    "    # Collapse multiple spaces into one\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Collapse 3+ newlines into 2 (preserving paragraph breaks)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Remove lines that are just page numbers (common PDF artifact)\n",
    "    text = re.sub(r'^\\s*\\d{1,4}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Strip leading/trailing whitespace from each line\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    text = '\\n'.join(lines)\n",
    "    \n",
    "    # Collapse multiple newlines again after line stripping\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Extract text from all PDFs\n",
    "print(\"Extracting text from Jordan Peterson's books...\\n\")\n",
    "print(f\"Looking in: {BOOKS_DIR.resolve()}\\n\")\n",
    "\n",
    "books = {}\n",
    "pdf_files = sorted(BOOKS_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "if not pdf_files:\n",
    "    raise FileNotFoundError(f\"No PDF files found in {BOOKS_DIR.resolve()}\")\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    raw_text = extract_text_from_pdf(str(pdf_path))\n",
    "    cleaned = clean_text(raw_text)\n",
    "    books[pdf_path.stem] = cleaned\n",
    "    \n",
    "    # Show stats for this book\n",
    "    word_count = len(cleaned.split())\n",
    "    char_count = len(cleaned)\n",
    "    print(f\"  -> {word_count:,} words, {char_count:,} characters\\n\")\n",
    "\n",
    "# Total stats\n",
    "total_words = sum(len(text.split()) for text in books.values())\n",
    "total_chars = sum(len(text) for text in books.values())\n",
    "print(f\"\\nTotal across all books: {total_words:,} words, {total_chars:,} characters\")\n",
    "print(f\"Number of books processed: {len(books)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Step 3: Create Training Dataset from Book Text\n\nNow we need to convert the raw book text into a format the model can learn from. For **Supervised Fine-Tuning (SFT)**, we need the data in a conversational format — specifically, a list of messages with roles like `\"system\"`, `\"user\"`, and `\"assistant\"`.\n\n### Our Approach: Passage-Based Q&A Format\n\nWe will structure each training example as:\n- **System message**: Sets the context — tells the model it is an expert on Jordan Peterson's ideas\n- **User message**: Asks the model to discuss, explain, or continue a passage from the book\n- **Assistant message**: Contains the actual book text passage\n\nThis format teaches the model to produce text that sounds like Jordan Peterson when asked about topics from his books.\n\n### Why This Format?\n\nThe GPT-OSS model uses OpenAI's [Harmony](https://github.com/openai/harmony) chat format. To fine-tune it effectively, our training data must match this conversational structure. The `tokenizer.apply_chat_template()` function handles all the special tokens and formatting automatically.\n\n### Chunking Strategy\n\nWe split the text into chunks of approximately 350 words (~470 tokens) with 50-word overlaps between consecutive chunks. This ensures:\n- Each chunk fits within our 2048-token sequence length (with room to spare for the chat template overhead)\n- Ideas that span chunk boundaries are partially represented in adjacent chunks\n- We leave room for the system/user prompt tokens and the GPT-OSS system preamble (~150 tokens)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def chunk_text(text: str, chunk_size: int = 350, overlap: int = 50) -> list[str]:\n    \"\"\"\n    Split text into overlapping chunks by word count.\n    \n    We chunk by words rather than characters because the model processes\n    tokens (which roughly correspond to words). A chunk of ~350 words\n    translates to roughly 470 tokens, leaving plenty of room for the \n    system/user prompt and chat template overhead within our 2048 token limit.\n    \n    The overlap ensures continuity between chunks — a sentence that gets\n    split at a chunk boundary will appear (at least partially) in both\n    the current chunk and the next one.\n    \n    Args:\n        text: The full text to chunk\n        chunk_size: Number of words per chunk\n        overlap: Number of overlapping words between consecutive chunks\n        \n    Returns:\n        List of text chunks\n    \"\"\"\n    words = text.split()\n    chunks = []\n    start = 0\n    \n    while start < len(words):\n        end = start + chunk_size\n        chunk = ' '.join(words[start:end])\n        \n        # Only include chunks that have meaningful content (at least 50 words)\n        if len(chunk.split()) >= 50:\n            chunks.append(chunk)\n        \n        # Move the window forward, but overlap with the previous chunk\n        start = end - overlap\n    \n    return chunks\n\n\n# Define the prompts that will be paired with each chunk.\n# We rotate through these to add variety to the training data.\n# Each prompt frames the book passage differently, teaching the model\n# to respond to various types of requests about Peterson's ideas.\nUSER_PROMPTS = [\n    \"Please share your thoughts on the following topic from your writings.\",\n    \"Can you elaborate on this idea from your work?\",\n    \"Explain this concept in detail.\",\n    \"What are your views on this subject?\",\n    \"Continue discussing this topic.\",\n    \"Tell me more about this idea.\",\n    \"Share your perspective on this.\",\n    \"Discuss the following in depth.\",\n]\n\n# The system prompt that will be used for all training examples.\n# This establishes the persona the model should adopt after fine-tuning.\nSYSTEM_PROMPT = (\n    \"You are an AI assistant that has been trained on the complete works of Jordan B. Peterson, \"\n    \"a Canadian clinical psychologist, professor, and author. You speak with deep knowledge of \"\n    \"psychology, philosophy, mythology, religion, and personal responsibility. Your responses \"\n    \"reflect Peterson's writing style, intellectual depth, and interdisciplinary approach to \"\n    \"understanding human nature and meaning.\"\n)\n\n\ndef create_training_examples(books: dict[str, str]) -> list[dict]:\n    \"\"\"\n    Convert book text into conversational training examples.\n    \n    Each example is a conversation with:\n    - A system message setting the Peterson-expert persona\n    - A user message with a rotating prompt\n    - An assistant message containing the book passage\n    \n    Args:\n        books: Dictionary mapping book names to their full text\n        \n    Returns:\n        List of conversation dictionaries in the format expected by\n        the tokenizer's chat template\n    \"\"\"\n    examples = []\n    prompt_idx = 0\n    \n    for book_name, text in books.items():\n        chunks = chunk_text(text)\n        print(f\"  {book_name}: {len(chunks)} chunks\")\n        \n        for chunk in chunks:\n            # Create a conversational training example\n            messages = [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": USER_PROMPTS[prompt_idx % len(USER_PROMPTS)]},\n                {\"role\": \"assistant\", \"content\": chunk},\n            ]\n            examples.append({\"messages\": messages})\n            prompt_idx += 1\n    \n    return examples\n\n\nprint(\"Creating training examples from book chunks...\\n\")\ntraining_data = create_training_examples(books)\nprint(f\"\\nTotal training examples: {len(training_data)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at one example to understand the format\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE TRAINING CONVERSATION (first example):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example = training_data[0]\n",
    "for msg in example[\"messages\"]:\n",
    "    role = msg[\"role\"].upper()\n",
    "    content = msg[\"content\"]\n",
    "    # Truncate long content for display\n",
    "    if len(content) > 300:\n",
    "        content = content[:300] + \"...[truncated]\"\n",
    "    print(f\"\\n[{role}]:\\n{content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Convert to HuggingFace Dataset\n",
    "\n",
    "The `SFTTrainer` from the `trl` library expects data in the HuggingFace `Dataset` format. We convert our list of conversation dictionaries into this format.\n",
    "\n",
    "We also apply the **chat template** using the tokenizer. The GPT-OSS model uses a specific format with special tokens like `<|start|>`, `<|message|>`, `<|end|>`, and `<|channel|>`. The `tokenizer.apply_chat_template()` function handles all of this formatting automatically — we just need to provide conversations in the standard `messages` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert our list of dictionaries to a HuggingFace Dataset\n",
    "# The Dataset class provides efficient data handling, shuffling,\n",
    "# and integration with the SFTTrainer.\n",
    "dataset = Dataset.from_list(training_data)\n",
    "\n",
    "# Shuffle the dataset so examples from different books are interleaved.\n",
    "# This prevents the model from \"forgetting\" earlier books as it trains\n",
    "# on later ones (a phenomenon called \"catastrophic forgetting\").\n",
    "dataset = dataset.shuffle(seed=3407)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} examples\")\n",
    "print(f\"\\nDataset features: {dataset.features}\")\n",
    "print(f\"\\nFirst example keys: {list(dataset[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Step 5: Load the Model with Unsloth\n\nNow we load the GPT-OSS 20B model using Unsloth's `FastLanguageModel`. Here's what each parameter does:\n\n- **`model_name`**: The HuggingFace model ID. We use `unsloth/gpt-oss-20b-unsloth-bnb-4bit` which is pre-quantized to 4-bit, meaning it's already compressed for efficient loading.\n- **`max_seq_length`**: The maximum number of tokens the model processes at once. We use 2048, which gives ample room for our ~350-word chunks plus the system/user prompts and the chat template overhead that GPT-OSS adds.\n- **`dtype`**: Set to `None` for automatic detection. Unsloth will pick the best precision for your GPU (bfloat16 if supported, otherwise float32).\n- **`load_in_4bit`**: Enables 4-bit quantization via bitsandbytes. This compresses the model to fit in GPU memory.\n- **`full_finetuning`**: Set to `False` because we're using LoRA (parameter-efficient), not full fine-tuning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\nimport torch\n\n# Configuration\nmax_seq_length = 2048  # Maximum token length per training example\ndtype = None           # Auto-detect best dtype for the GPU\n\n# Load the pre-quantized 4-bit model\n# This downloads the model from HuggingFace (cached after first download)\n# and loads it into GPU memory in 4-bit quantized format.\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n    dtype = dtype,\n    max_seq_length = max_seq_length,\n    load_in_4bit = True,         # Use 4-bit quantization to reduce memory\n    full_finetuning = False,     # We use LoRA, not full fine-tuning\n    # token = \"YOUR_HF_TOKEN\",  # Uncomment if model requires authentication\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Add LoRA Adapters\n",
    "\n",
    "Now we add **LoRA (Low-Rank Adaptation)** adapters to the model. This is the key to parameter-efficient fine-tuning.\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "In a standard neural network layer, the weight matrix might be 4096x4096 = ~16 million parameters. LoRA decomposes the update to this matrix into two smaller matrices: one that is 4096x8 and another that is 8x4096. This means instead of updating 16M parameters, we only update 65K parameters (8 is the \"rank\" — our `r` parameter). The original weights stay frozen.\n",
    "\n",
    "### Parameter Explanation\n",
    "\n",
    "- **`r = 16`**: The rank of the LoRA adapters. Higher values = more parameters to train = more capacity to learn, but also more memory. 16 is a good balance for learning writing style and knowledge. We use 16 instead of the reference notebook's 8 because we want the model to capture more nuance from Peterson's writing style.\n",
    "- **`target_modules`**: Which layers in the model to add LoRA adapters to. We target all the attention layers (q/k/v/o projections) and feed-forward layers (gate/up/down projections). This gives the adapters access to both \"what the model pays attention to\" and \"how it transforms information.\"\n",
    "- **`lora_alpha = 32`**: A scaling factor for LoRA. The effective learning rate for LoRA is proportional to `lora_alpha / r`. With alpha=32 and r=16, we get a scaling of 2x, which provides a good learning signal.\n",
    "- **`lora_dropout = 0`**: No dropout on LoRA layers. Unsloth optimizes for dropout=0.\n",
    "- **`use_gradient_checkpointing = \"unsloth\"`**: Unsloth's custom gradient checkpointing uses 30% less VRAM than standard gradient checkpointing, allowing larger batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # LoRA rank: controls adapter capacity (8, 16, 32, 64, 128)\n",
    "    target_modules = [\n",
    "        \"q_proj\",    # Query projection in attention\n",
    "        \"k_proj\",    # Key projection in attention\n",
    "        \"v_proj\",    # Value projection in attention\n",
    "        \"o_proj\",    # Output projection in attention\n",
    "        \"gate_proj\", # Gate projection in feed-forward\n",
    "        \"up_proj\",   # Up projection in feed-forward\n",
    "        \"down_proj\", # Down projection in feed-forward\n",
    "    ],\n",
    "    lora_alpha = 32,    # Scaling factor (effective lr ~ alpha/r)\n",
    "    lora_dropout = 0,   # No dropout (optimized by Unsloth)\n",
    "    bias = \"none\",      # Don't train bias terms (optimized)\n",
    "    use_gradient_checkpointing = \"unsloth\",  # 30% less VRAM!\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # Rank-stabilized LoRA (not needed here)\n",
    "    loftq_config = None, # LoftQ initialization (not needed here)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Format Dataset with Chat Template\n",
    "\n",
    "The model needs the training data formatted with its specific chat template. The GPT-OSS format uses special tokens to delineate different parts of the conversation. The `tokenizer.apply_chat_template()` function converts our simple `messages` format into the model's native format.\n",
    "\n",
    "For example, a message like `{\"role\": \"user\", \"content\": \"Hello\"}` gets converted to something like:\n",
    "```\n",
    "<|start|>user<|message|>Hello<|end|>\n",
    "```\n",
    "\n",
    "We also use `standardize_sharegpt()` from Unsloth to ensure our data conforms to the expected format before applying the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Apply the model's chat template to each conversation.\n",
    "    \n",
    "    This function is called by the dataset.map() method. It takes a batch\n",
    "    of examples (each with a 'messages' field) and converts them into\n",
    "    the model's native text format using the tokenizer's chat template.\n",
    "    \n",
    "    The resulting 'text' field is what the SFTTrainer will use for training.\n",
    "    \n",
    "    Args:\n",
    "        examples: A batch of examples from the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'text' field containing formatted conversations\n",
    "    \"\"\"\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, \n",
    "            tokenize=False,            # Return text, not token IDs\n",
    "            add_generation_prompt=False  # Don't add prompt at the end (we have the full conversation)\n",
    "        ) \n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "# Standardize the dataset format to match what Unsloth expects\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "# Apply the chat template to create the 'text' column\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Dataset formatted. Columns: {dataset.column_names}\")\n",
    "print(f\"Number of examples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what a formatted example looks like.\n",
    "# This shows the exact text the model will be trained on,\n",
    "# including all the special tokens.\n",
    "print(\"=\" * 80)\n",
    "print(\"FORMATTED TRAINING EXAMPLE (first 1000 chars):\")\n",
    "print(\"=\" * 80)\n",
    "print(dataset[0]['text'][:1000])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Diagnostic: Check token counts to ensure examples fit within max_seq_length.\n# If too many examples exceed the limit, they'll be dropped during training,\n# resulting in an empty dataset. This cell helps catch that problem early.\n\ntoken_counts = []\nfor i in range(min(50, len(dataset))):  # Sample first 50 examples\n    tokens = tokenizer.encode(dataset[i]['text'])\n    token_counts.append(len(tokens))\n\nimport statistics\nprint(f\"Token count statistics (sampled from {len(token_counts)} examples):\")\nprint(f\"  Min:    {min(token_counts)}\")\nprint(f\"  Max:    {max(token_counts)}\")\nprint(f\"  Mean:   {statistics.mean(token_counts):.0f}\")\nprint(f\"  Median: {statistics.median(token_counts):.0f}\")\nprint(f\"  Max seq length: {max_seq_length}\")\nprint(f\"\")\nover_limit = sum(1 for tc in token_counts if tc > max_seq_length)\nprint(f\"  Examples over limit: {over_limit}/{len(token_counts)}\")\nif over_limit > 0:\n    print(f\"  WARNING: {over_limit} examples exceed max_seq_length and may be truncated or dropped!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Configure the Trainer\n",
    "\n",
    "We use `SFTTrainer` (Supervised Fine-Tuning Trainer) from the `trl` library. This trainer is specifically designed for fine-tuning language models on conversational data.\n",
    "\n",
    "### Training Configuration Explained\n",
    "\n",
    "- **`per_device_train_batch_size = 1`**: Process 1 example at a time per GPU. With a 20B model, even in 4-bit, we can only fit 1 example in memory at a time.\n",
    "- **`gradient_accumulation_steps = 4`**: Accumulate gradients over 4 batches before updating weights. This simulates a batch size of 4 without needing 4x the memory. Larger effective batch sizes lead to more stable training.\n",
    "- **`warmup_steps = 10`**: Gradually increase the learning rate from 0 to the target over the first 10 steps. This prevents the model from making too-large updates at the start when it hasn't \"seen\" much data yet.\n",
    "- **`num_train_epochs = 1`**: Go through the entire dataset once. For book-based fine-tuning, 1-3 epochs is typical. More epochs risk \"overfitting\" (memorizing the text rather than learning the style).\n",
    "- **`learning_rate = 2e-4`**: How much to adjust weights per update. 2e-4 is a standard rate for LoRA fine-tuning — small enough to avoid destroying the model's existing knowledge, large enough to learn new patterns.\n",
    "- **`optim = \"adamw_8bit\"`**: The optimizer algorithm, using 8-bit quantization to save memory. AdamW is the standard optimizer for transformer training.\n",
    "- **`weight_decay = 0.01`**: A regularization technique that slightly penalizes large weights, helping prevent overfitting.\n",
    "- **`lr_scheduler_type = \"cosine\"`**: The learning rate follows a cosine curve, starting high and gradually decreasing. This is generally better than linear decay for fine-tuning.\n",
    "- **`output_dir`**: Where to save training checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Define the output directory for checkpoints and the final model\n",
    "OUTPUT_DIR = \"./outputs/gpt_oss_20b_jordan_peterson\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,   # 1 example per GPU at a time\n",
    "        gradient_accumulation_steps = 4,    # Effective batch size = 1 * 4 = 4\n",
    "        warmup_steps = 10,                  # Gradual learning rate warmup\n",
    "        num_train_epochs = 1,               # 1 full pass through the data\n",
    "        # max_steps = 30,                   # Uncomment to limit steps for testing\n",
    "        learning_rate = 2e-4,               # Standard LoRA learning rate\n",
    "        logging_steps = 1,                  # Log metrics every step\n",
    "        optim = \"adamw_8bit\",               # Memory-efficient optimizer\n",
    "        weight_decay = 0.01,                # Regularization\n",
    "        lr_scheduler_type = \"cosine\",       # Cosine learning rate decay\n",
    "        seed = 3407,                        # Reproducibility\n",
    "        output_dir = OUTPUT_DIR,            # Checkpoint directory\n",
    "        report_to = \"none\",                 # Disable WandB/TensorBoard logging\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),  # Use fp16 if bf16 not available\n",
    "        bf16 = torch.cuda.is_bf16_supported(),      # Use bf16 if available (RTX 4090 supports it)\n",
    "        save_strategy = \"steps\",            # Save checkpoints by step count\n",
    "        save_steps = 100,                   # Save every 100 steps\n",
    "        save_total_limit = 3,               # Keep only the 3 most recent checkpoints\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Step 9: Apply Response-Only Training\n\nThis is an important optimization: we configure the trainer to **only compute loss on the assistant's responses**, not on the system/user prompts.\n\n### Why Train Only on Responses?\n\nDuring training, the model processes the entire conversation and tries to predict each next token. Without this optimization, the model would try to learn to predict the system prompt and user messages too — which is wasteful because those are always provided as input during inference. We only want the model to learn how to generate good responses.\n\nBy masking the loss on the instruction parts (system + user), we:\n1. **Improve training efficiency**: The model focuses its learning capacity on what matters\n2. **Reduce loss**: The loss metric more accurately reflects how well the model generates responses\n3. **Improve output quality**: All learning signal goes toward improving responses\n\nThe `instruction_part` and `response_part` parameters tell the trainer which special tokens mark the boundary between \"input\" (don't train on) and \"output\" (train on).\n\n**Important:** The GPT-OSS chat template formats simple assistant messages with `<|start|>assistant<|message|>` (without the `<|channel|>final` part that appears in multi-channel conversations like the reference notebook's dataset). We must match the exact tokens our formatted data actually contains."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth.chat_templates import train_on_responses_only\n\n# Auto-detect the correct response part token from our formatted data.\n# The GPT-OSS template uses different formats depending on whether the\n# conversation has channel info (analysis/final) or not.\n# Our simple assistant messages use: <|start|>assistant<|message|>\n# The reference notebook's multi-channel data uses: <|start|>assistant<|channel|>final<|message|>\nsample_text = dataset[0]['text']\nif \"<|start|>assistant<|channel|>final<|message|>\" in sample_text:\n    response_part = \"<|start|>assistant<|channel|>final<|message|>\"\nelif \"<|start|>assistant<|message|>\" in sample_text:\n    response_part = \"<|start|>assistant<|message|>\"\nelse:\n    raise ValueError(f\"Could not find assistant response marker in formatted text!\")\n\nprint(f\"Detected response_part: {response_part}\")\n\ngpt_oss_kwargs = dict(\n    instruction_part = \"<|start|>user<|message|>\",\n    response_part = response_part,\n)\n\ntrainer = train_on_responses_only(\n    trainer,\n    **gpt_oss_kwargs,\n)\n\nprint(\"Response-only training configured.\")\nprint(\"The model will only learn from assistant responses, not from prompts.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify that masking is working correctly.\n# The first print shows the full tokenized input.\n# The second print shows only the parts the model will train on\n# (everything else is replaced with padding tokens, shown as spaces).\n\ndataset_size = len(trainer.train_dataset)\nprint(f\"Training dataset size after tokenization: {dataset_size}\")\n\nif dataset_size == 0:\n    print(\"\\nERROR: Training dataset is empty after tokenization!\")\n    print(\"This usually means all examples exceeded max_seq_length and were dropped.\")\n    print(\"Try reducing chunk_size or increasing max_seq_length.\")\nelse:\n    sample_idx = 0\n    print(f\"\\n{'=' * 80}\")\n    print(\"FULL TOKENIZED INPUT (what the model sees):\")\n    print(\"=\" * 80)\n    full_text = tokenizer.decode(trainer.train_dataset[sample_idx][\"input_ids\"])\n    print(full_text[:500] + \"...\\n\")\n\n    print(\"=\" * 80)\n    print(\"MASKED LABELS (what the model trains on - spaces are masked out):\")\n    print(\"=\" * 80)\n    labels = trainer.train_dataset[sample_idx][\"labels\"]\n    masked_text = tokenizer.decode(\n        [tokenizer.pad_token_id if x == -100 else x for x in labels]\n    ).replace(tokenizer.pad_token, \" \")\n    print(masked_text[:500] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Check Memory Before Training\n",
    "\n",
    "Let's see how much GPU memory the model is using before training starts. This helps us understand how much headroom we have and whether we might encounter out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Total GPU Memory: {max_memory} GB\")\n",
    "print(f\"Memory reserved before training: {start_gpu_memory} GB\")\n",
    "print(f\"Available for training: {max_memory - start_gpu_memory:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Train the Model!\n",
    "\n",
    "This is the main training step. The trainer will:\n",
    "\n",
    "1. Iterate through the dataset in batches\n",
    "2. For each batch, feed the text through the model\n",
    "3. Compute the loss (how wrong the model's predictions were) on the assistant responses only\n",
    "4. Compute gradients (which direction to adjust weights)\n",
    "5. Accumulate gradients over 4 steps\n",
    "6. Update the LoRA adapter weights\n",
    "7. Log the loss and learning rate\n",
    "\n",
    "**What to watch for during training:**\n",
    "- **Loss should decrease** over time, indicating the model is learning\n",
    "- **Loss shouldn't drop to near 0**, which would indicate overfitting (memorization)\n",
    "- A healthy final loss is typically between 0.5 and 2.0 for this type of fine-tuning\n",
    "\n",
    "Training time depends on the dataset size and number of epochs. With the RTX 4090, expect roughly 1-2 seconds per training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "# This will take a while depending on your dataset size.\n",
    "# You'll see a progress bar with loss values.\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training statistics and memory usage\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "training_percentage = round(used_memory_for_training / max_memory * 100, 3)\n",
    "\n",
    "runtime_seconds = trainer_stats.metrics['train_runtime']\n",
    "runtime_minutes = round(runtime_seconds / 60, 2)\n",
    "train_loss = trainer_stats.metrics.get('train_loss', 'N/A')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTime: {runtime_seconds:.1f} seconds ({runtime_minutes} minutes)\")\n",
    "print(f\"Final training loss: {train_loss}\")\n",
    "print(f\"\\nMemory Usage:\")\n",
    "print(f\"  Peak reserved memory: {used_memory} GB\")\n",
    "print(f\"  Memory used for training: {used_memory_for_training} GB\")\n",
    "print(f\"  Peak memory % of total: {used_percentage}%\")\n",
    "print(f\"  Training memory % of total: {training_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Test the Fine-Tuned Model (Inference)\n",
    "\n",
    "Let's test our fine-tuned model! We'll ask it questions related to the topics in Jordan Peterson's books to see if it has learned his writing style and ideas.\n",
    "\n",
    "The GPT-OSS model supports a `reasoning_effort` parameter that controls how much the model \"thinks\" before responding:\n",
    "- **low**: Fast, less detailed responses\n",
    "- **medium**: Balanced\n",
    "- **high**: Most detailed, more reasoning tokens used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Put model in inference mode (disables dropout, optimizes for generation)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def ask_model(question: str, reasoning_effort: str = \"medium\", max_tokens: int = 512):\n",
    "    \"\"\"\n",
    "    Ask the fine-tuned model a question and stream the response.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "        reasoning_effort: 'low', 'medium', or 'high'\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        reasoning_effort=reasoning_effort,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reasoning Effort: {reasoning_effort}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # TextStreamer prints tokens as they're generated (like ChatGPT's typing effect)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    _ = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=max_tokens, \n",
    "        streamer=streamer,\n",
    "        temperature=0.7,       # Controls randomness (0=deterministic, 1=creative)\n",
    "        top_p=0.9,             # Nucleus sampling (consider top 90% probability tokens)\n",
    "        repetition_penalty=1.1, # Slightly penalize repetition\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: A topic central to \"12 Rules for Life\"\n",
    "ask_model(\"What is the importance of taking responsibility in one's life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: A topic from \"Maps of Meaning\"\n",
    "ask_model(\"How do myths and stories help us understand the nature of reality?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: A topic from \"Beyond Order\"\n",
    "ask_model(\"What does it mean to pursue what is meaningful rather than what is expedient?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: A topic from \"We Who Wrestle with God\"\n",
    "ask_model(\n",
    "    \"What is the relationship between suffering and meaning in human existence?\",\n",
    "    reasoning_effort=\"high\",\n",
    "    max_tokens=768,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 13: Save the Fine-Tuned Model\n",
    "\n",
    "We can save our fine-tuned model in different ways:\n",
    "\n",
    "### LoRA Adapters Only (Recommended for Storage)\n",
    "Saves just the trained LoRA adapter weights (~20-50MB). To use the model later, you load the base model and then apply these adapters on top. This is the most storage-efficient option.\n",
    "\n",
    "### Merged Model (For Deployment)\n",
    "Merges the LoRA adapters back into the base model weights and saves the full model. This is larger but simpler to deploy since you only need one set of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as LoRA adapters (small, ~20-50MB)\n",
    "LORA_OUTPUT_DIR = \"./outputs/gpt_oss_20b_jordan_peterson_lora\"\n",
    "\n",
    "model.save_pretrained(LORA_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
    "\n",
    "print(f\"LoRA adapters saved to: {LORA_OUTPUT_DIR}\")\n",
    "\n",
    "# Show the saved files and their sizes\n",
    "import os\n",
    "total_size = 0\n",
    "for f in sorted(os.listdir(LORA_OUTPUT_DIR)):\n",
    "    fpath = os.path.join(LORA_OUTPUT_DIR, f)\n",
    "    if os.path.isfile(fpath):\n",
    "        size = os.path.getsize(fpath)\n",
    "        total_size += size\n",
    "        print(f\"  {f}: {size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"\\nTotal size: {total_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save as merged 16-bit model (larger, ~40GB, but simpler to deploy)\n",
    "# Uncomment the lines below if you want to save the full merged model.\n",
    "# WARNING: This requires significant disk space!\n",
    "\n",
    "# MERGED_OUTPUT_DIR = \"./outputs/gpt_oss_20b_jordan_peterson_merged_16bit\"\n",
    "# model.save_pretrained_merged(MERGED_OUTPUT_DIR, tokenizer, save_method=\"merged_16bit\")\n",
    "# print(f\"Merged 16-bit model saved to: {MERGED_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 14: How to Load the Fine-Tuned Model Later\n",
    "\n",
    "Once saved, you can load the fine-tuned model in a new session without re-training. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# To load the fine-tuned model in a new session, run this cell.\n# Set the condition to True when you want to load a previously saved model.\n\nLOAD_SAVED_MODEL = False  # Change to True to load from saved LoRA adapters\n\nif LOAD_SAVED_MODEL:\n    from unsloth import FastLanguageModel\n    import torch\n    \n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"./outputs/gpt_oss_20b_jordan_peterson_lora\",\n        max_seq_length = 2048,\n        dtype = None,\n        load_in_4bit = True,\n    )\n    FastLanguageModel.for_inference(model)\n    print(\"Fine-tuned model loaded successfully!\")\nelse:\n    print(\"Using the model from the current training session.\")\n    print(\"Set LOAD_SAVED_MODEL = True to load from saved adapters instead.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\nIn this notebook, we:\n\n1. **Extracted text** from 4 Jordan Peterson books using PyMuPDF\n2. **Created a training dataset** by chunking the text into ~350-word passages and formatting them as conversations\n3. **Loaded the GPT-OSS 20B model** in 4-bit quantization using Unsloth for 2x faster training\n4. **Added LoRA adapters** to train only ~0.02% of the model's parameters\n5. **Applied response-only training** so the model only learns from the assistant responses\n6. **Trained the model** using SFTTrainer with optimized settings\n7. **Tested the model** with questions related to Peterson's ideas\n8. **Saved the fine-tuned model** as LoRA adapters for future use\n\n### Key Takeaways\n\n- **LoRA** makes it possible to fine-tune a 20B parameter model on a single consumer GPU\n- **4-bit quantization** compresses the model to fit in ~12GB of VRAM\n- **Unsloth** provides 2x speedup and significant VRAM savings over standard training\n- **Response-only training** improves efficiency by focusing learning on what matters\n- The fine-tuned model learns Jordan Peterson's writing style and can discuss his ideas\n\n### Next Steps\n\n- **Increase epochs**: Try `num_train_epochs = 2` or `3` for deeper learning (watch for overfitting)\n- **Adjust LoRA rank**: Try `r = 32` or `r = 64` for more capacity\n- **Increase chunk size**: Try larger chunks if token counts have headroom within `max_seq_length`\n- **Try GRPO**: Use reinforcement learning to further align the model's outputs\n- **Push to HuggingFace Hub**: Share your fine-tuned model with the community"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}