{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9098a6",
   "metadata": {},
   "source": [
    "# Jordan Peterson — Data Preparation Notebook\n",
    "\n",
    "This notebook centralises all data-preparation steps for the Jordan Peterson\n",
    "fine-tuning pipeline.  It is designed to run **once** before any fine-tuning\n",
    "notebook and produces the Q&A dataset that all downstream notebooks consume.\n",
    "\n",
    "## Pipeline Position\n",
    "\n",
    "```\n",
    "[This notebook]                  [Fine-tuning notebooks]\n",
    "  PDF extraction                   Qwen3_14B_V2 / Qwen3_32B\n",
    "      ↓                                    ↓\n",
    "  Front-matter removal        ←  qa_dataset/peterson_qa.jsonl\n",
    "      ↓\n",
    "  Q&A generation\n",
    "  (local model OR Anthropic API)\n",
    "      ↓\n",
    "  qa_dataset/peterson_qa.jsonl\n",
    "```\n",
    "\n",
    "## Why a Separate Notebook?\n",
    "\n",
    "Previously the extraction + generation code was **duplicated** inside both the\n",
    "V2 and 32B fine-tuning notebooks.  Two problems drove this refactor:\n",
    "\n",
    "1. **Front-matter pollution**: extracted passages included title pages,\n",
    "   copyright notices, table of contents, and forewords — publisher boilerplate,\n",
    "   not Peterson's writing.\n",
    "2. **API dependency**: question generation required a paid Anthropic API key.\n",
    "   This notebook adds a **free local alternative** using a small HuggingFace\n",
    "   model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5fad66",
   "metadata": {},
   "source": [
    "## Configuration File (`peterson_config.json`)\n",
    "\n",
    "All tunable parameters live in `peterson_config.json` in this directory.\n",
    "Edit that file to change paths, chunk sizes, or the generation backend —\n",
    "no notebook code changes required.\n",
    "\n",
    "### Backend Comparison\n",
    "\n",
    "| Backend | Model | VRAM | Speed | Cost |\n",
    "|---------|-------|------|-------|------|\n",
    "| `local` (default) | Qwen3-4B 4-bit | ~2.5 GB | ~45 min | Free |\n",
    "| `local` | Qwen3-1.7B 4-bit | ~1.2 GB | ~25 min | Free |\n",
    "| `local` | Qwen3-8B 4-bit | ~5 GB | ~80 min | Free |\n",
    "| `local` | Phi-4-mini-instruct | ~2.5 GB | ~45 min | Free |\n",
    "| `anthropic` | claude-haiku-4-5-20251001 | 0 GB | ~20 min | ~$1–3 |\n",
    "\n",
    "To switch to the Anthropic backend, set `\"backend\": \"anthropic\"` in\n",
    "`peterson_config.json` and ensure `ANTHROPIC_API_KEY` is set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79777a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import fitz    # PyMuPDF\n",
    "import torch\n",
    "\n",
    "# ── Load config ───────────────────────────────────────────────────────────────\n",
    "_config_path = Path(__file__).parent / \"peterson_config.json\" if \"__file__\" in dir() else Path(\"peterson_config.json\")\n",
    "with open(_config_path) as _f:\n",
    "    config = json.load(_f)\n",
    "\n",
    "# Paths\n",
    "BOOKS_DIR = Path(config[\"paths\"][\"books_dir\"])\n",
    "QA_CACHE  = Path(config[\"paths\"][\"qa_cache\"])\n",
    "QA_DIR    = QA_CACHE.parent\n",
    "QA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Extraction params\n",
    "CHUNK_WORDS     = config[\"extraction\"][\"chunk_words\"]\n",
    "OVERLAP_WORDS   = config[\"extraction\"][\"overlap_words\"]\n",
    "MIN_CHUNK_WORDS = config[\"extraction\"][\"min_chunk_words\"]\n",
    "\n",
    "# Generation params\n",
    "QUESTIONS_PER_PASSAGE = config[\"generation\"][\"questions_per_passage\"]\n",
    "MAX_NEW_TOKENS        = config[\"generation\"][\"max_new_tokens\"]\n",
    "BACKEND               = config[\"generation\"][\"backend\"]\n",
    "LOCAL_MODEL_NAME      = config[\"generation\"][\"local_model\"]\n",
    "ANTHROPIC_MODEL       = config[\"generation\"][\"anthropic_model\"]\n",
    "SYSTEM_PROMPT         = config[\"system_prompt\"]\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Books dir          : {BOOKS_DIR.resolve()}\")\n",
    "print(f\"  Q&A cache          : {QA_CACHE.resolve()}\")\n",
    "print(f\"  Chunk words        : {CHUNK_WORDS}  (overlap: {OVERLAP_WORDS}, min: {MIN_CHUNK_WORDS})\")\n",
    "print(f\"  Questions/passage  : {QUESTIONS_PER_PASSAGE}\")\n",
    "print(f\"  Backend            : {BACKEND}\")\n",
    "if BACKEND == \"local\":\n",
    "    print(f\"  Local model        : {LOCAL_MODEL_NAME}\")\n",
    "else:\n",
    "    print(f\"  Anthropic model    : {ANTHROPIC_MODEL}\")\n",
    "print(f\"  CUDA available     : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU                : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM               : {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d27f44",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1 — PDF Extraction with Front-Matter Removal\n",
    "\n",
    "## The Front-Matter Problem\n",
    "\n",
    "When PyMuPDF extracts a book PDF page-by-page, the first several pages contain\n",
    "publisher boilerplate that does NOT reflect Peterson's writing:\n",
    "\n",
    "- **Title page**: book title, author name, publisher logo\n",
    "- **Copyright page**: ISBN, rights notices, Library of Congress data\n",
    "- **Table of contents**: chapter names and page numbers (not prose)\n",
    "- **Foreword**: written by someone else (e.g. Norman Doidge in *12 Rules*)\n",
    "- **Preface / Overture**: sometimes counts as front matter\n",
    "\n",
    "Including these in training data teaches the model to regurgitate publishing\n",
    "metadata when asked Peterson-style questions.\n",
    "\n",
    "## Detection Strategy (3 Tiers)\n",
    "\n",
    "**Tier 1 — Chapter marker page** (works for 12 Rules, Beyond Order)\n",
    "\n",
    "Search for a page that:\n",
    "- Contains a chapter-1 marker (`Chapter 1`, `Rule 1`, `Rule I`, `Overture`, ...)\n",
    "- Has ≥ 150 words (real content, not just a chapter heading on otherwise blank page)\n",
    "- Does NOT also contain a chapter-2 marker (filters out TOC entries)\n",
    "\n",
    "**Tier 2 — Post-copyright content** (needed for We Who Wrestle with God)\n",
    "\n",
    "Some books don't have explicit chapter-1 markers early enough. Fall back to:\n",
    "- Find the last page containing copyright/publisher indicators (ISBN, etc.)\n",
    "- Return the first subsequent page with ≥ 200 words\n",
    "\n",
    "**Tier 3 — No-op** (fallback for Maps of Meaning, which starts immediately)\n",
    "\n",
    "If neither tier succeeds, return page 0 — extract from the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw: str) -> str:\n",
    "    \"\"\"Remove PDF artefacts: control chars, excess whitespace, ligatures.\"\"\"\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', raw)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.replace('\\ufb01', 'fi').replace('\\ufb02', 'fl')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# ── Front-matter detection patterns ──────────────────────────────────────────\n",
    "_CHAPTER1_RE = re.compile(\n",
    "    r'\\b(Chapter\\s+1|CHAPTER\\s+1|Rule\\s+1|RULE\\s+1'\n",
    "    r'|Rule\\s+I\\b|RULE\\s+I\\b'\n",
    "    r'|Overture|OVERTURE'\n",
    "    r'|Cain\\s+and\\s+Abel|CAIN\\s+AND\\s+ABEL)\\b',\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "_CHAPTER2_RE = re.compile(\n",
    "    r'\\b(Chapter\\s+2|CHAPTER\\s+2|Rule\\s+2|RULE\\s+2'\n",
    "    r'|Rule\\s+II\\b|RULE\\s+II\\b'\n",
    "    r'|Noah\\b)\\b',\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "_FRONTMATTER_RE = re.compile(\n",
    "    r'ISBN|All rights reserved|Library of Congress'\n",
    "    r'|copyright.*\\d{4}|\\d{4}.*copyright'\n",
    "    r'|Published by|penguinrandomhouse|routledge\\.com',\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "def _find_chapter1_page(pages: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Return the index of the first real-content page (skipping front matter).\n",
    "\n",
    "    Uses a 3-tier heuristic — see the markdown cell above for rationale.\n",
    "    \"\"\"\n",
    "    # Tier 1: page with chapter-1 marker, ≥150 words, no chapter-2 marker\n",
    "    for i, page in enumerate(pages):\n",
    "        if (_CHAPTER1_RE.search(page)\n",
    "                and len(page.split()) >= 150\n",
    "                and not _CHAPTER2_RE.search(page)):\n",
    "            return i\n",
    "\n",
    "    # Tier 2: find last copyright/publisher page, return first ≥200-word page after it\n",
    "    fm_pages = [i for i, p in enumerate(pages) if _FRONTMATTER_RE.search(p)]\n",
    "    if fm_pages:\n",
    "        last_fm = max(fm_pages)\n",
    "        for i in range(last_fm + 1, len(pages)):\n",
    "            if len(pages[i].split()) >= 200:\n",
    "                return i\n",
    "\n",
    "    # Tier 3: no-op — start from beginning\n",
    "    return 0\n",
    "\n",
    "\n",
    "def extract_chunks(\n",
    "    pdf_path: Path,\n",
    "    chunk_words: int = CHUNK_WORDS,\n",
    "    overlap_words: int = OVERLAP_WORDS,\n",
    "    min_chunk_words: int = MIN_CHUNK_WORDS,\n",
    ") -> tuple[list[str], int]:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, skip front matter, and split into overlapping chunks.\n",
    "\n",
    "    Returns (chunks, pages_skipped).\n",
    "    \"\"\"\n",
    "    doc   = fitz.open(str(pdf_path))\n",
    "    pages = [clean_text(page.get_text()) for page in doc]\n",
    "    doc.close()\n",
    "\n",
    "    start_page = _find_chapter1_page(pages)\n",
    "    content_pages = pages[start_page:]\n",
    "    full_text = ' '.join(content_pages)\n",
    "\n",
    "    words = full_text.split()\n",
    "    step  = chunk_words - overlap_words\n",
    "    chunks = []\n",
    "    for start in range(0, len(words), step):\n",
    "        chunk = ' '.join(words[start: start + chunk_words])\n",
    "        if len(chunk.split()) >= min_chunk_words:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks, start_page\n",
    "\n",
    "\n",
    "print(\"Extraction functions defined.\")\n",
    "print(f\"  Patterns: _CHAPTER1_RE, _CHAPTER2_RE, _FRONTMATTER_RE\")\n",
    "print(f\"  Functions: clean_text(), _find_chapter1_page(), extract_chunks()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27be556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run extraction over all 4 books ──────────────────────────────────────────\n",
    "pdf_files = sorted(BOOKS_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files in {BOOKS_DIR.resolve()}\")\n",
    "for p in pdf_files:\n",
    "    print(f\"  {p.name}\")\n",
    "\n",
    "all_chunks = []    # flat list of passage strings\n",
    "chunk_meta = []    # parallel list of {\"book\": label} dicts\n",
    "\n",
    "def _label_pdf(fname: str) -> str:\n",
    "    fname = fname.lower()\n",
    "    if \"maps\" in fname:\n",
    "        return \"Maps of Meaning\"\n",
    "    elif \"12 rules\" in fname or \"antidote\" in fname:\n",
    "        return \"12 Rules for Life\"\n",
    "    elif \"beyond\" in fname:\n",
    "        return \"Beyond Order\"\n",
    "    else:\n",
    "        return \"We Who Wrestle with God\"\n",
    "\n",
    "print()\n",
    "for pdf in pdf_files:\n",
    "    label = _label_pdf(pdf.name)\n",
    "    chunks, skipped = extract_chunks(pdf)\n",
    "    all_chunks.extend(chunks)\n",
    "    chunk_meta.extend([{\"book\": label}] * len(chunks))\n",
    "    total_words = sum(len(c.split()) for c in chunks)\n",
    "    print(f\"  {label:<35}  skipped {skipped:2d} pages  |  \"\n",
    "          f\"{len(chunks):4d} chunks  (~{total_words:,} words)\")\n",
    "\n",
    "print(f\"\\nTotal passages: {len(all_chunks):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Spot-check: first ~120 chars of first chunk per book ─────────────────────\n",
    "# Should show actual prose, NOT copyright/ISBN/title-page text.\n",
    "print(\"First chunk preview per book (verify no boilerplate):\")\n",
    "print(\"=\" * 72)\n",
    "seen = set()\n",
    "for chunk, meta in zip(all_chunks, chunk_meta):\n",
    "    book = meta[\"book\"]\n",
    "    if book not in seen:\n",
    "        seen.add(book)\n",
    "        preview = chunk[:120].replace(\"\\n\", \" \")\n",
    "        print(f\"\\n{book}:\")\n",
    "        print(f\"  {preview}...\")\n",
    "    if len(seen) == 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b6675",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2 — Q&A Generation\n",
    "\n",
    "## Backend Options\n",
    "\n",
    "This notebook supports two question-generation backends, controlled by\n",
    "`\"backend\"` in `peterson_config.json`:\n",
    "\n",
    "| Setting | Description |\n",
    "|---------|-------------|\n",
    "| `\"local\"` (default) | Loads a small 4-bit model via Unsloth — **free, no API key** |\n",
    "| `\"anthropic\"` | Calls Claude Haiku API — faster (~20 min), costs ~$1–3 |\n",
    "\n",
    "### Why the Local Model Works\n",
    "\n",
    "Question generation is not a demanding task. We only need plausible, on-topic\n",
    "questions that the passage directly answers. The *training signal* is entirely\n",
    "in the **answers** (Peterson's verbatim text), not the questions.\n",
    "\n",
    "A Qwen3-4B model at 4-bit quantization (~2.5 GB VRAM) is more than capable of\n",
    "this and runs on the same GPU as the fine-tuning pipeline.\n",
    "\n",
    "### Speed Estimates (Qwen3-4B, RTX 4090)\n",
    "\n",
    "| Model | VRAM | Time for ~2,519 passages × 2 Q |\n",
    "|-------|------|-------------------------------|\n",
    "| Qwen3-1.7B 4-bit | ~1.2 GB | ~25 min |\n",
    "| **Qwen3-4B 4-bit** | **~2.5 GB** | **~45 min** |\n",
    "| Qwen3-8B 4-bit | ~5 GB | ~80 min |\n",
    "| Phi-4-mini-instruct | ~2.5 GB | ~45 min |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Backend selection ─────────────────────────────────────────────────────────\n",
    "print(f\"Generation backend: {BACKEND!r}\")\n",
    "\n",
    "if BACKEND == \"local\":\n",
    "    print(f\"Local model       : {LOCAL_MODEL_NAME}\")\n",
    "    print(f\"Max new tokens    : {MAX_NEW_TOKENS}\")\n",
    "    print(\"\\nSkipping Anthropic setup — using local model.\")\n",
    "elif BACKEND == \"anthropic\":\n",
    "    print(f\"Anthropic model   : {ANTHROPIC_MODEL}\")\n",
    "    print(\"\\nSkipping local model setup — using Anthropic API.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown backend {BACKEND!r}. Must be 'local' or 'anthropic'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb64b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Local model setup (skipped if backend=anthropic) ─────────────────────────\n",
    "gen_model = None\n",
    "gen_tokenizer = None\n",
    "\n",
    "if BACKEND == \"local\":\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    print(f\"Loading {LOCAL_MODEL_NAME} for inference ...\")\n",
    "    gen_model, gen_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name     = LOCAL_MODEL_NAME,\n",
    "        max_seq_length = 1024,\n",
    "        load_in_4bit   = True,\n",
    "        dtype          = None,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(gen_model)\n",
    "\n",
    "    vram_used = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"Model loaded. VRAM reserved: {vram_used:.1f} GB\")\n",
    "    print(\"gen_model and gen_tokenizer are ready.\")\n",
    "else:\n",
    "    print(\"Local model setup skipped (backend=anthropic).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Anthropic setup (skipped if backend=local) ────────────────────────────────\n",
    "anthropic_client = None\n",
    "\n",
    "if BACKEND == \"anthropic\":\n",
    "    import subprocess\n",
    "    _result = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"anthropic\", \"-q\"],\n",
    "        capture_output=True, text=True,\n",
    "    )\n",
    "    if _result.returncode != 0:\n",
    "        print(\"pip install warning:\", _result.stderr[:300])\n",
    "\n",
    "    import anthropic as _anthropic_module\n",
    "\n",
    "    # Load API key from env or ~/.env\n",
    "    _api_key = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")\n",
    "    if not _api_key:\n",
    "        _env_file = Path.home() / \".env\"\n",
    "        if _env_file.exists():\n",
    "            for _line in _env_file.read_text().splitlines():\n",
    "                if _line.startswith(\"ANTHROPIC_API_KEY=\"):\n",
    "                    _api_key = _line.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
    "                    break\n",
    "\n",
    "    if not _api_key:\n",
    "        raise EnvironmentError(\n",
    "            \"ANTHROPIC_API_KEY not found.\\n\"\n",
    "            \"Set it with:  export ANTHROPIC_API_KEY='sk-ant-...'\\n\"\n",
    "            \"Or add it to ~/.env as:  ANTHROPIC_API_KEY=sk-ant-...\"\n",
    "        )\n",
    "\n",
    "    anthropic_client = _anthropic_module.Anthropic(api_key=_api_key)\n",
    "    print(f\"Anthropic SDK ready (v{_anthropic_module.__version__}).\")\n",
    "    print(f\"Using model: {ANTHROPIC_MODEL}\")\n",
    "else:\n",
    "    print(\"Anthropic setup skipped (backend=local).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21164932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generation functions ──────────────────────────────────────────────────────\n",
    "\n",
    "def _make_prompt(passage: str, book: str) -> str:\n",
    "    \"\"\"Shared prompt template used by both backends.\"\"\"\n",
    "    return (\n",
    "        f\"You are building a training dataset for a Jordan B. Peterson AI model.\\n\\n\"\n",
    "        f\"The passage below is from Peterson's book '{book}'. Generate exactly \"\n",
    "        f\"{QUESTIONS_PER_PASSAGE} questions that:\\n\"\n",
    "        f\"1. This passage directly and substantively answers\\n\"\n",
    "        f\"2. Someone interested in Peterson's ideas might genuinely ask\\n\"\n",
    "        f\"3. Cover different angles of the passage (e.g. one concrete, one philosophical)\\n\\n\"\n",
    "        f\"Peterson's topics include: order vs chaos, meaning, personal responsibility, \"\n",
    "        f\"suffering, mythology, archetypes, the shadow, logos, truth, religion, \"\n",
    "        f\"Jungian psychology, hierarchy, heroism, sacrifice, being.\\n\\n\"\n",
    "        f\"Return ONLY a JSON array of exactly {QUESTIONS_PER_PASSAGE} question strings. \"\n",
    "        f\"No other text. No markdown fences.\\n\"\n",
    "        f'Example: [\"Why is confronting chaos necessary for meaning?\", '\n",
    "        f'\"What role does suffering play in personal development?\"]\\n\\n'\n",
    "        f\"Passage:\\n{passage}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _parse_questions(raw: str) -> list[str]:\n",
    "    \"\"\"Strip <think>...</think> blocks and extract a JSON array of questions.\"\"\"\n",
    "    # Remove thinking blocks (Qwen3 with enable_thinking=True adds these)\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', raw, flags=re.DOTALL).strip()\n",
    "    match = re.search(r'\\[.*?\\]', cleaned, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(f\"No JSON array found in: {cleaned[:120]!r}\")\n",
    "    questions = json.loads(match.group())\n",
    "    if not isinstance(questions, list) or len(questions) < 1:\n",
    "        raise ValueError(f\"Expected list, got {type(questions).__name__}\")\n",
    "    return [str(q).strip() for q in questions[:QUESTIONS_PER_PASSAGE]]\n",
    "\n",
    "\n",
    "def _generate_local(passage: str, book: str, max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"Generate questions using the local gen_model.\"\"\"\n",
    "    prompt_text = _make_prompt(passage, book)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Format with ChatML template (enable_thinking=False for structured output)\n",
    "            text = gen_tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False,\n",
    "            )\n",
    "            inputs = gen_tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = gen_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens   = MAX_NEW_TOKENS,\n",
    "                    do_sample        = True,\n",
    "                    temperature      = 0.7,\n",
    "                    top_p            = 0.8,\n",
    "                    top_k            = 20,\n",
    "                    repetition_penalty = 1.1,\n",
    "                )\n",
    "\n",
    "            new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "            raw = gen_tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "            return _parse_questions(raw)\n",
    "\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    [retry {attempt+1}] {e!s:.80} — wait {wait}s\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"    [FAILED] {e!s:.80}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def _generate_anthropic(passage: str, book: str, max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"Generate questions using the Anthropic API.\"\"\"\n",
    "    prompt_text = _make_prompt(passage, book)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = anthropic_client.messages.create(\n",
    "                model      = ANTHROPIC_MODEL,\n",
    "                max_tokens = MAX_NEW_TOKENS,\n",
    "                messages   = [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            )\n",
    "            raw = response.content[0].text.strip()\n",
    "            return _parse_questions(raw)\n",
    "\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    [retry {attempt+1}] {e!s:.80} — wait {wait}s\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"    [FAILED] {e!s:.80}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_questions(passage: str, book: str) -> list[str]:\n",
    "    \"\"\"Dispatcher: route to local or Anthropic backend based on config.\"\"\"\n",
    "    if BACKEND == \"local\":\n",
    "        return _generate_local(passage, book)\n",
    "    else:\n",
    "        return _generate_anthropic(passage, book)\n",
    "\n",
    "\n",
    "print(\"Generation functions defined:\")\n",
    "print(\"  _make_prompt(), _parse_questions()\")\n",
    "print(\"  _generate_local(), _generate_anthropic(), generate_questions()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cache-aware generation loop ───────────────────────────────────────────────\n",
    "existing_records = []\n",
    "if QA_CACHE.exists():\n",
    "    with open(QA_CACHE) as _f:\n",
    "        existing_records = [json.loads(line) for line in _f if line.strip()]\n",
    "\n",
    "_expected  = len(all_chunks) * QUESTIONS_PER_PASSAGE\n",
    "_coverage  = len(existing_records) / _expected if _expected else 0\n",
    "\n",
    "print(f\"Passages            : {len(all_chunks):,}\")\n",
    "print(f\"Expected Q&A pairs  : {_expected:,}  ({QUESTIONS_PER_PASSAGE} per passage)\")\n",
    "print(f\"Cached              : {len(existing_records):,}  ({100*_coverage:.1f}% coverage)\")\n",
    "\n",
    "if _coverage >= 0.90:\n",
    "    print(\"\\nCache is ≥90% complete — skipping generation.\")\n",
    "    print(f\"Delete {QA_CACHE} to force regeneration.\")\n",
    "else:\n",
    "    already_done   = len(existing_records) // QUESTIONS_PER_PASSAGE\n",
    "    remaining      = all_chunks[already_done:]\n",
    "    remaining_meta = chunk_meta[already_done:]\n",
    "    skipped        = 0\n",
    "    t_start        = time.time()\n",
    "\n",
    "    print(f\"\\nGenerating questions for {len(remaining):,} passages \"\n",
    "          f\"(resuming from passage {already_done + 1})...\")\n",
    "    if BACKEND == \"anthropic\":\n",
    "        cost_est = len(remaining) * 0.00068\n",
    "        print(f\"Estimated API cost: ~${cost_est:.2f}\")\n",
    "    print()\n",
    "\n",
    "    with open(QA_CACHE, \"a\") as out_f:\n",
    "        for i, (passage, meta) in enumerate(zip(remaining, remaining_meta)):\n",
    "            global_idx = already_done + i + 1\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                elapsed = time.time() - t_start\n",
    "                pct = 100 * (already_done + i) / len(all_chunks)\n",
    "                print(f\"  [{global_idx:4d}/{len(all_chunks):4d}]  {pct:5.1f}%  \"\n",
    "                      f\"{elapsed:5.0f}s elapsed  book: {meta['book']}\")\n",
    "\n",
    "            questions = generate_questions(passage, meta[\"book\"])\n",
    "\n",
    "            if not questions:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            for q in questions:\n",
    "                record = {\"question\": q, \"answer\": passage, \"book\": meta[\"book\"]}\n",
    "                out_f.write(json.dumps(record) + \"\\n\")\n",
    "            out_f.flush()\n",
    "\n",
    "            # Rate-limit only for Anthropic (local GPU doesn't need this)\n",
    "            if BACKEND == \"anthropic\":\n",
    "                time.sleep(0.3)\n",
    "\n",
    "    elapsed_total = time.time() - t_start\n",
    "    print(f\"\\nGeneration complete in {elapsed_total/60:.1f} min.\")\n",
    "    print(f\"  Passages processed : {len(remaining) - skipped:,}\")\n",
    "    print(f\"  Passages skipped   : {skipped}\")\n",
    "\n",
    "# Re-read final dataset\n",
    "with open(QA_CACHE) as _f:\n",
    "    qa_records = [json.loads(line) for line in _f if line.strip()]\n",
    "\n",
    "print(f\"\\nTotal Q&A pairs in cache: {len(qa_records):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b726b",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3 — Dataset Statistics and Cleanup\n",
    "\n",
    "Verify the dataset before using it for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ddc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# ── Distribution by book ──────────────────────────────────────────────────────\n",
    "book_counts = Counter(r[\"book\"] for r in qa_records)\n",
    "print(\"Q&A pairs by book:\")\n",
    "for book, count in sorted(book_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {book:<35}  {count:4d} pairs  ({100*count/len(qa_records):.1f}%)\")\n",
    "\n",
    "# ── Answer length stats ───────────────────────────────────────────────────────\n",
    "ans_lengths = [len(r[\"answer\"].split()) for r in qa_records]\n",
    "q_lengths   = [len(r[\"question\"].split()) for r in qa_records]\n",
    "print(f\"\\nAnswer length (words):\")\n",
    "print(f\"  Min: {min(ans_lengths)}  Max: {max(ans_lengths)}  \"\n",
    "      f\"Mean: {sum(ans_lengths)/len(ans_lengths):.0f}  \"\n",
    "      f\"Median: {sorted(ans_lengths)[len(ans_lengths)//2]}\")\n",
    "print(f\"Question length (words):\")\n",
    "print(f\"  Min: {min(q_lengths)}  Max: {max(q_lengths)}  \"\n",
    "      f\"Mean: {sum(q_lengths)/len(q_lengths):.1f}\")\n",
    "\n",
    "# ── Sample Q&A per book ───────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"SAMPLE Q&A PAIRS — first example per book (spot-check for boilerplate)\")\n",
    "print(\"=\" * 72)\n",
    "seen_books: set[str] = set()\n",
    "for r in qa_records:\n",
    "    if r[\"book\"] not in seen_books:\n",
    "        seen_books.add(r[\"book\"])\n",
    "        print(f\"\\nBook: {r['book']}\")\n",
    "        print(f\"Q: {r['question']}\")\n",
    "        answer_preview = r[\"answer\"][:200].replace(\"\\n\", \" \")\n",
    "        print(f\"A: {answer_preview}...\")\n",
    "    if len(seen_books) == 4:\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"If any answer above starts with ISBN/copyright/title text,\")\n",
    "print(\"the front-matter detection needs adjustment for that book.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cleanup: unload local model to free VRAM ─────────────────────────────────\n",
    "if BACKEND == \"local\" and gen_model is not None:\n",
    "    print(\"Unloading local generation model to free VRAM...\")\n",
    "    del gen_model\n",
    "    del gen_tokenizer\n",
    "    gen_model = None\n",
    "    gen_tokenizer = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    vram_after = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"VRAM after unload: {vram_after:.1f} GB\")\n",
    "else:\n",
    "    print(\"No local model to unload.\")\n",
    "\n",
    "# ── Final summary ─────────────────────────────────────────────────────────────\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Q&A cache path  : {QA_CACHE.resolve()}\")\n",
    "print(f\"  Total Q&A pairs : {len(qa_records):,}\")\n",
    "print(f\"  Books covered   : {len(book_counts)}\")\n",
    "print()\n",
    "print(\"Next step: run a fine-tuning notebook that reads this cache.\")\n",
    "print(\"  Qwen3_14B_JordanPeterson_V2_FineTuning.ipynb\")\n",
    "print(\"  Qwen3_32B_JordanPeterson_FineTuning.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
