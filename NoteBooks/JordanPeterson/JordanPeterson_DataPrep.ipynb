{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76186f1",
   "metadata": {},
   "source": [
    "# Jordan Peterson — Data Preparation Notebook\n",
    "\n",
    "This notebook centralises all data-preparation steps for the Jordan Peterson\n",
    "fine-tuning pipeline.  It is designed to run **once** before any fine-tuning\n",
    "notebook and produces the Q&A dataset that all downstream notebooks consume.\n",
    "\n",
    "## Pipeline Position\n",
    "\n",
    "```\n",
    "[This notebook]                  [Fine-tuning notebooks]\n",
    "  PDF extraction                   Qwen3_14B_V2 / Qwen3_32B\n",
    "      ↓                                    ↓\n",
    "  Front-matter removal        ←  qa_dataset/peterson_qa.jsonl\n",
    "      ↓\n",
    "  Q&A generation\n",
    "  (local model OR Anthropic API)\n",
    "      ↓\n",
    "  qa_dataset/peterson_qa.jsonl\n",
    "```\n",
    "\n",
    "## Why a Separate Notebook?\n",
    "\n",
    "Previously the extraction + generation code was **duplicated** inside both the\n",
    "V2 and 32B fine-tuning notebooks.  Two problems drove this refactor:\n",
    "\n",
    "1. **Front-matter pollution**: extracted passages included title pages,\n",
    "   copyright notices, table of contents, and forewords — publisher boilerplate,\n",
    "   not Peterson's writing.\n",
    "2. **API dependency**: question generation required a paid Anthropic API key.\n",
    "   This notebook adds a **free local alternative** using a small HuggingFace\n",
    "   model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8b0a3",
   "metadata": {},
   "source": [
    "## Configuration File (`peterson_config.json`)\n",
    "\n",
    "All tunable parameters live in `peterson_config.json` in this directory.\n",
    "Edit that file to change paths, chunk sizes, or the generation backend —\n",
    "no notebook code changes required.\n",
    "\n",
    "### Backend Comparison\n",
    "\n",
    "| Backend | Model | VRAM | Speed | Cost |\n",
    "|---------|-------|------|-------|------|\n",
    "| `local` (default) | Qwen3-4B 4-bit | ~2.5 GB | ~65 min | Free |\n",
    "| `local` | Qwen3-1.7B 4-bit | ~1.2 GB | ~35 min | Free |\n",
    "| `local` | Qwen3-8B 4-bit | ~5 GB | ~120 min | Free |\n",
    "| `local` | Phi-4-mini-instruct | ~2.5 GB | ~65 min | Free |\n",
    "| `anthropic` | claude-haiku-4-5-20251001 | 0 GB | ~20 min | ~$1–3 |\n",
    "\n",
    "Speed estimates are for ~1,900 passages × 2 questions on an RTX 4090.\n",
    "To switch to the Anthropic backend, set `\"backend\": \"anthropic\"` in\n",
    "`peterson_config.json` and ensure `ANTHROPIC_API_KEY` is set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e42ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import fitz    # PyMuPDF\n",
    "import torch\n",
    "\n",
    "# ── Load config ───────────────────────────────────────────────────────────────\n",
    "_config_path = Path(__file__).parent / \"peterson_config.json\" if \"__file__\" in dir() else Path(\"peterson_config.json\")\n",
    "with open(_config_path) as _f:\n",
    "    config = json.load(_f)\n",
    "\n",
    "# Paths\n",
    "BOOKS_DIR = Path(config[\"paths\"][\"books_dir\"])\n",
    "QA_CACHE  = Path(config[\"paths\"][\"qa_cache\"])\n",
    "QA_DIR    = QA_CACHE.parent\n",
    "QA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Extraction params\n",
    "CHUNK_WORDS     = config[\"extraction\"][\"chunk_words\"]\n",
    "OVERLAP_WORDS   = config[\"extraction\"][\"overlap_words\"]\n",
    "MIN_CHUNK_WORDS = config[\"extraction\"][\"min_chunk_words\"]\n",
    "\n",
    "# Generation params\n",
    "QUESTIONS_PER_PASSAGE = config[\"generation\"][\"questions_per_passage\"]\n",
    "MAX_NEW_TOKENS        = config[\"generation\"][\"max_new_tokens\"]\n",
    "BACKEND               = config[\"generation\"][\"backend\"]\n",
    "LOCAL_MODEL_NAME      = config[\"generation\"][\"local_model\"]\n",
    "ANTHROPIC_MODEL       = config[\"generation\"][\"anthropic_model\"]\n",
    "SYSTEM_PROMPT         = config[\"system_prompt\"]\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Books dir          : {BOOKS_DIR.resolve()}\")\n",
    "print(f\"  Q&A cache          : {QA_CACHE.resolve()}\")\n",
    "print(f\"  Chunk words        : {CHUNK_WORDS}  (overlap: {OVERLAP_WORDS}, min: {MIN_CHUNK_WORDS})\")\n",
    "print(f\"  Questions/passage  : {QUESTIONS_PER_PASSAGE}\")\n",
    "print(f\"  Backend            : {BACKEND}\")\n",
    "if BACKEND == \"local\":\n",
    "    print(f\"  Local model        : {LOCAL_MODEL_NAME}\")\n",
    "else:\n",
    "    print(f\"  Anthropic model    : {ANTHROPIC_MODEL}\")\n",
    "print(f\"  CUDA available     : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU                : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM               : {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe163c7",
   "metadata": {},
   "source": "---\n# Part 1 — PDF Extraction with Front-Matter and Back-Matter Removal\n\n## The Front-Matter Problem\n\nWhen PyMuPDF extracts a book PDF page-by-page, the first several pages contain\npublisher boilerplate that does NOT reflect Peterson's writing:\n\n- **Title page**: book title, author name, publisher logo\n- **Copyright page**: ISBN, rights notices, Library of Congress data\n- **Table of contents**: chapter names and page numbers (not prose)\n- **Foreword**: written by someone else (e.g. Norman Doidge in *12 Rules*)\n- **Preface / Overture**: sometimes counts as front matter\n\nIncluding these in training data teaches the model to regurgitate publishing\nmetadata when asked Peterson-style questions.\n\n## Front-Matter Detection Strategy (3 Tiers)\n\n**Tier 1 — Chapter marker page** (works for 12 Rules, Beyond Order)\n\nSearch for a page that:\n- Contains a chapter-1 marker (`Chapter 1`, `Rule 1`, `Rule I`, `Overture`, ...)\n- Has ≥ 150 words (real content, not just a chapter heading on otherwise blank page)\n- Does NOT also contain a chapter-2 marker (filters out TOC entries)\n\n**Tier 2 — Post-copyright content** (needed for We Who Wrestle with God)\n\nSome books don't have explicit chapter-1 markers early enough. Fall back to:\n- Find the last page containing copyright/publisher indicators (ISBN, etc.)\n- Return the first subsequent page with ≥ 200 words\n\n**Tier 3 — No-op** (fallback for Maps of Meaning, which starts immediately)\n\nIf neither tier succeeds, return page 0 — extract from the beginning.\n\n### Front-Matter Search Window Limit\n\nBoth Tier 1 and Tier 2 only search within the **first 60 pages** (or 10% of\nthe book, whichever is larger, up to 80 pages).  This is critical for academic\nbooks like *Maps of Meaning*: footnotes and bibliographies throughout the book\nmay contain copyright years, ISBN-like strings, or publisher names (Routledge).\nWithout a search limit, Tier 2 would find the *last* such match deep in the\nbook and skip most of the content.  Front matter is always at the start — there\nis no reason to search beyond page 60.\n\n---\n\n## The Back-Matter Problem\n\nJust as the first pages contain publisher boilerplate, the final pages contain\nback-matter that is equally harmful to training data quality:\n\n- **Index**: alphabetical topic list with page numbers — produces outputs like\n  `\"chaos, 34–67, 102, 143–74\"` instead of answers to philosophical questions\n- **Bibliography / References**: citation lists — not Peterson's prose\n- **Figure lists**: captions with figure numbers scattered through the book\n\nThis contamination was confirmed in the all-versions comparison (2026-02-21):\nall V2 and V3 fine-tuned models reproduce raw index entries in response to\nphilosophical prompts.  V3 showed *more* index contamination than V2 (3/10\nprompts vs 1/10) because removing front matter raised the proportional weight\nof back-matter pages in the training corpus.\n\n## Back-Matter Detection Strategy (2 Signals)\n\nOnly the **last 30% of the book** is searched — back matter can never start\nbefore the final chapters, and a narrow search window prevents false positives\nfrom mid-book footnotes.\n\n**Signal 1 — Section header page**: a page with ≤ 30 words (after whitespace\nnormalisation) that contains one of the keywords `Index`, `Bibliography`,\n`References`, or `Further Reading`.  Short word count distinguishes a\nstandalone section-title page from a prose page that mentions an index in\npassing.\n\n**Signal 2 — Index content page**: a page showing either (a) five or more\npage-range patterns (`78–102`, `234–5`) — essentially impossible in prose but\nubiquitous in indexes — or (b) more than 25% purely numeric tokens, reflecting\nthe dense page-number lists in index entries.\n\nOnce either signal fires, the detected page and everything after it is excluded\nfrom extraction.  The `extract_chunks()` function receives both a `start_page`\n(front-matter skip) and an `end_page` (back-matter cutoff), and extracts only\n`pages[start_page:end_page]`.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be5b76",
   "metadata": {},
   "outputs": [],
   "source": "def clean_text(raw: str) -> str:\n    \"\"\"Remove PDF artefacts: control chars, excess whitespace, ligatures.\"\"\"\n    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', raw)\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.replace('\\ufb01', 'fi').replace('\\ufb02', 'fl')\n    return text.strip()\n\n\n# ── Front-matter detection patterns ──────────────────────────────────────────\n_CHAPTER1_RE = re.compile(\n    r'\\b(Chapter\\s+1|CHAPTER\\s+1|Rule\\s+1|RULE\\s+1'\n    r'|Rule\\s+I\\b|RULE\\s+I\\b'\n    r'|Overture|OVERTURE'\n    r'|Cain\\s+and\\s+Abel|CAIN\\s+AND\\s+ABEL)\\b',\n    re.IGNORECASE,\n)\n\n_CHAPTER2_RE = re.compile(\n    r'\\b(Chapter\\s+2|CHAPTER\\s+2|Rule\\s+2|RULE\\s+2'\n    r'|Rule\\s+II\\b|RULE\\s+II\\b'\n    r'|Noah\\b)\\b',\n    re.IGNORECASE,\n)\n\n_FRONTMATTER_RE = re.compile(\n    r'ISBN|All rights reserved|Library of Congress'\n    r'|copyright.*\\d{4}|\\d{4}.*copyright'\n    r'|Published by|penguinrandomhouse|routledge\\.com',\n    re.IGNORECASE,\n)\n\n# Maximum number of pages to search for front-matter markers.\n# Must be small enough to avoid matching mid-book footnotes/references,\n# large enough to cover unusually long front-matter sections.\n_FM_SEARCH_LIMIT = 80\n\n# ── Back-matter detection patterns ────────────────────────────────────────────\n# Section-title keywords that mark the start of non-prose back matter.\n# Only matched on short pages (≤30 words) to avoid false positives in prose.\n_BACKMATTER_TITLE_RE = re.compile(\n    r'\\b(Index|Bibliography|References|Further\\s+Reading)\\b',\n    re.IGNORECASE,\n)\n\n# Page-range pattern: \"78–102\", \"234-5\", \"45—67\".\n# Appears abundantly in indexes and endnotes (every entry has at least one).\n# Also found in biblical commentary as verse ranges — see density check below.\n_PAGE_RANGE_RE = re.compile(r'\\b\\d{1,4}[–—\\-]\\d{1,4}\\b')\n\n# Fraction of book (from the start) at which back-matter search begins.\n# Back matter is always in the final pages; 70% is deliberately conservative.\n_BM_SEARCH_START_PCT = 0.70\n\n\ndef _find_chapter1_page(pages: list[str]) -> int:\n    \"\"\"\n    Return the index of the first real-content page (skipping front matter).\n\n    Uses a 3-tier heuristic — see the markdown cell above for rationale.\n\n    IMPORTANT: both Tier 1 and Tier 2 only examine the first _FM_SEARCH_LIMIT\n    pages.  Academic books (e.g. Maps of Meaning) have copyright years, ISBN\n    numbers, and publisher names scattered throughout footnotes and bibliographies.\n    Without this cap, Tier 2 would find the *last* such page deep in the book\n    and incorrectly skip hundreds of pages of real content.\n    \"\"\"\n    search_end = min(len(pages), _FM_SEARCH_LIMIT)\n    search_pages = pages[:search_end]\n\n    # Tier 1: page with chapter-1 marker, ≥150 words, no chapter-2 marker\n    for i, page in enumerate(search_pages):\n        if (_CHAPTER1_RE.search(page)\n                and len(page.split()) >= 150\n                and not _CHAPTER2_RE.search(page)):\n            return i\n\n    # Tier 2: find last copyright/publisher page within search window,\n    # then return the first ≥200-word page after it.\n    fm_pages = [i for i, p in enumerate(search_pages) if _FRONTMATTER_RE.search(p)]\n    if fm_pages:\n        last_fm = max(fm_pages)\n        for i in range(last_fm + 1, len(pages)):\n            if len(pages[i].split()) >= 200:\n                return i\n\n    # Tier 3: no-op — start from beginning\n    return 0\n\n\ndef _is_index_page(page: str) -> bool:\n    \"\"\"\n    Return True if the page looks like an index, endnote, or reference list.\n\n    Two independent signals:\n\n    Signal 1 — page-range density: five or more page-range patterns (e.g.\n    '78–102') AND numeric token density > 5%.  The density requirement\n    distinguishes citation/endnote pages (density 6–18%) from Peterson's\n    biblical commentary in 'We Who Wrestle with God', which contains verse\n    ranges like '1-13' or '3:1-5' but has near-zero numeric density overall.\n\n    Signal 2 — pure numeric density > 40%: unambiguously identifies index\n    pages where the majority of tokens are standalone page numbers.\n\n    A minimum of 15 tokens is required before making any judgment; blank pages\n    and single-character chapter dividers must not trigger.\n    \"\"\"\n    tokens = page.split()\n    if len(tokens) < 15:\n        return False  # Blank pages, chapter-number dividers — do not trigger\n\n    n_ranges = len(_PAGE_RANGE_RE.findall(page))\n    n_numeric = sum(1 for t in tokens if re.fullmatch(r'\\d+[–—\\-]?\\d*[,;.]?', t))\n    density = n_numeric / len(tokens)\n\n    # Signal 1: many page-ranges AND non-trivial numeric density\n    if n_ranges >= 5 and density > 0.05:\n        return True\n\n    # Signal 2: very high numeric density (obvious index page)\n    if density > 0.40:\n        return True\n\n    return False\n\n\ndef _find_backmatter_page(pages: list[str]) -> int:\n    \"\"\"\n    Return the index of the first back-matter page (exclusive end for content).\n\n    Only examines the last (1 - _BM_SEARCH_START_PCT) fraction of the book.\n    Two detection signals:\n    - Short page (≤30 words) containing a back-matter section keyword\n      ('Index', 'Bibliography', 'References', 'Further Reading')\n    - High-density page-number content (via _is_index_page)\n\n    Returns len(pages) if no back matter is detected (use full book).\n    \"\"\"\n    n = len(pages)\n    search_start = int(n * _BM_SEARCH_START_PCT)\n\n    for i in range(search_start, n):\n        page = pages[i]\n        # Short page with a section-title keyword → section header\n        if len(page.split()) <= 30 and _BACKMATTER_TITLE_RE.search(page):\n            return i\n        # High-density index/reference content\n        if _is_index_page(page):\n            return i\n\n    return n  # No back matter detected\n\n\ndef extract_chunks(\n    pdf_path: Path,\n    chunk_words: int = CHUNK_WORDS,\n    overlap_words: int = OVERLAP_WORDS,\n    min_chunk_words: int = MIN_CHUNK_WORDS,\n) -> tuple[list[str], int, int, int]:\n    \"\"\"\n    Extract text from a PDF, skip front matter and back matter, and split\n    into overlapping chunks.\n\n    Returns (chunks, front_pages_skipped, back_pages_skipped, total_pages).\n    \"\"\"\n    doc   = fitz.open(str(pdf_path))\n    pages = [clean_text(page.get_text()) for page in doc]\n    total_pages = len(pages)\n    doc.close()\n\n    start_page = _find_chapter1_page(pages)\n    end_page   = _find_backmatter_page(pages)\n    content_pages = pages[start_page:end_page]\n    full_text = ' '.join(content_pages)\n\n    words = full_text.split()\n    step  = chunk_words - overlap_words\n    chunks = []\n    for start in range(0, len(words), step):\n        chunk = ' '.join(words[start: start + chunk_words])\n        if len(chunk.split()) >= min_chunk_words:\n            chunks.append(chunk)\n\n    back_pages_skipped = total_pages - end_page\n    return chunks, start_page, back_pages_skipped, total_pages\n\n\nprint(\"Extraction functions defined.\")\nprint(f\"  Front-matter search limit   : {_FM_SEARCH_LIMIT} pages\")\nprint(f\"  Back-matter search start    : last {100*(1-_BM_SEARCH_START_PCT):.0f}% of book\")\nprint(f\"  Front patterns : _CHAPTER1_RE, _CHAPTER2_RE, _FRONTMATTER_RE\")\nprint(f\"  Back patterns  : _BACKMATTER_TITLE_RE, _PAGE_RANGE_RE\")\nprint(f\"  Functions      : clean_text(), _find_chapter1_page(), _find_backmatter_page()\")\nprint(f\"                   _is_index_page(), extract_chunks()\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24958537",
   "metadata": {},
   "outputs": [],
   "source": "# ── Run extraction over all 4 books ──────────────────────────────────────────\npdf_files = sorted(BOOKS_DIR.glob(\"*.pdf\"))\nprint(f\"Found {len(pdf_files)} PDF files in {BOOKS_DIR.resolve()}\")\nfor p in pdf_files:\n    print(f\"  {p.name}\")\n\nall_chunks = []    # flat list of passage strings\nchunk_meta = []    # parallel list of {\"book\": label} dicts\n\ndef _label_pdf(fname: str) -> str:\n    fname = fname.lower()\n    if \"maps\" in fname:\n        return \"Maps of Meaning\"\n    elif \"12 rules\" in fname or \"antidote\" in fname:\n        return \"12 Rules for Life\"\n    elif \"beyond\" in fname:\n        return \"Beyond Order\"\n    else:\n        return \"We Who Wrestle with God\"\n\nprint()\nfor pdf in pdf_files:\n    label = _label_pdf(pdf.name)\n    chunks, front_skipped, back_skipped, total = extract_chunks(pdf)\n    all_chunks.extend(chunks)\n    chunk_meta.extend([{\"book\": label}] * len(chunks))\n    total_words  = sum(len(c.split()) for c in chunks)\n    front_pct    = 100 * front_skipped / total if total else 0\n    back_pct     = 100 * back_skipped  / total if total else 0\n    content_used = total - front_skipped - back_skipped\n    content_pct  = 100 * content_used / total if total else 0\n    print(f\"  {label:<35}  \"\n          f\"front: {front_skipped:3d} ({front_pct:4.1f}%)  \"\n          f\"back: {back_skipped:3d} ({back_pct:4.1f}%)  \"\n          f\"content: {content_used:3d}/{total} ({content_pct:4.1f}%)  |  \"\n          f\"{len(chunks):4d} chunks  (~{total_words:,} words)\")\n\nprint(f\"\\nTotal passages: {len(all_chunks):,}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Spot-check: first ~120 chars of first chunk per book ─────────────────────\n",
    "# Should show actual prose, NOT copyright/ISBN/title-page text.\n",
    "print(\"First chunk preview per book (verify no boilerplate):\")\n",
    "print(\"=\" * 72)\n",
    "seen = set()\n",
    "for chunk, meta in zip(all_chunks, chunk_meta):\n",
    "    book = meta[\"book\"]\n",
    "    if book not in seen:\n",
    "        seen.add(book)\n",
    "        preview = chunk[:120].replace(\"\\n\", \" \")\n",
    "        print(f\"\\n{book}:\")\n",
    "        print(f\"  {preview}...\")\n",
    "    if len(seen) == 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1256af5",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2 — Q&A Generation\n",
    "\n",
    "## Backend Options\n",
    "\n",
    "This notebook supports two question-generation backends, controlled by\n",
    "`\"backend\"` in `peterson_config.json`:\n",
    "\n",
    "| Setting | Description |\n",
    "|---------|-------------|\n",
    "| `\"local\"` (default) | Loads a small 4-bit model via Unsloth — **free, no API key** |\n",
    "| `\"anthropic\"` | Calls Claude Haiku API — faster (~20 min), costs ~$1–3 |\n",
    "\n",
    "### Why the Local Model Works\n",
    "\n",
    "Question generation is not a demanding task. We only need plausible, on-topic\n",
    "questions that the passage directly answers. The *training signal* is entirely\n",
    "in the **answers** (Peterson's verbatim text), not the questions.\n",
    "\n",
    "A Qwen3-4B model at 4-bit quantization (~2.5 GB VRAM) is more than capable of\n",
    "this and runs on the same GPU as the fine-tuning pipeline.\n",
    "\n",
    "### Speed Estimates (Qwen3-4B, RTX 4090)\n",
    "\n",
    "Measured on ~1,900 passages × 2 questions:\n",
    "\n",
    "| Model | VRAM | Measured time |\n",
    "|-------|------|---------------|\n",
    "| Qwen3-1.7B 4-bit | ~1.2 GB | ~35 min |\n",
    "| **Qwen3-4B 4-bit** | **~2.5 GB** | **~65 min** |\n",
    "| Qwen3-8B 4-bit | ~5 GB | ~120 min |\n",
    "| Phi-4-mini-instruct | ~2.5 GB | ~65 min |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Backend selection ─────────────────────────────────────────────────────────\n",
    "print(f\"Generation backend: {BACKEND!r}\")\n",
    "\n",
    "if BACKEND == \"local\":\n",
    "    print(f\"Local model       : {LOCAL_MODEL_NAME}\")\n",
    "    print(f\"Max new tokens    : {MAX_NEW_TOKENS}\")\n",
    "    print(\"\\nSkipping Anthropic setup — using local model.\")\n",
    "elif BACKEND == \"anthropic\":\n",
    "    print(f\"Anthropic model   : {ANTHROPIC_MODEL}\")\n",
    "    print(\"\\nSkipping local model setup — using Anthropic API.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown backend {BACKEND!r}. Must be 'local' or 'anthropic'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Local model setup (skipped if backend=anthropic) ─────────────────────────\n",
    "gen_model = None\n",
    "gen_tokenizer = None\n",
    "\n",
    "if BACKEND == \"local\":\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    print(f\"Loading {LOCAL_MODEL_NAME} for inference ...\")\n",
    "    gen_model, gen_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name     = LOCAL_MODEL_NAME,\n",
    "        max_seq_length = 1024,\n",
    "        load_in_4bit   = True,\n",
    "        dtype          = None,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(gen_model)\n",
    "\n",
    "    vram_used = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"Model loaded. VRAM reserved: {vram_used:.1f} GB\")\n",
    "    print(\"gen_model and gen_tokenizer are ready.\")\n",
    "else:\n",
    "    print(\"Local model setup skipped (backend=anthropic).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Anthropic setup (skipped if backend=local) ────────────────────────────────\n",
    "anthropic_client = None\n",
    "\n",
    "if BACKEND == \"anthropic\":\n",
    "    import subprocess\n",
    "    _result = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"anthropic\", \"-q\"],\n",
    "        capture_output=True, text=True,\n",
    "    )\n",
    "    if _result.returncode != 0:\n",
    "        print(\"pip install warning:\", _result.stderr[:300])\n",
    "\n",
    "    import anthropic as _anthropic_module\n",
    "\n",
    "    # Load API key from env or ~/.env\n",
    "    _api_key = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")\n",
    "    if not _api_key:\n",
    "        _env_file = Path.home() / \".env\"\n",
    "        if _env_file.exists():\n",
    "            for _line in _env_file.read_text().splitlines():\n",
    "                if _line.startswith(\"ANTHROPIC_API_KEY=\"):\n",
    "                    _api_key = _line.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
    "                    break\n",
    "\n",
    "    if not _api_key:\n",
    "        raise EnvironmentError(\n",
    "            \"ANTHROPIC_API_KEY not found.\\n\"\n",
    "            \"Set it with:  export ANTHROPIC_API_KEY='sk-ant-...'\\n\"\n",
    "            \"Or add it to ~/.env as:  ANTHROPIC_API_KEY=sk-ant-...\"\n",
    "        )\n",
    "\n",
    "    anthropic_client = _anthropic_module.Anthropic(api_key=_api_key)\n",
    "    print(f\"Anthropic SDK ready (v{_anthropic_module.__version__}).\")\n",
    "    print(f\"Using model: {ANTHROPIC_MODEL}\")\n",
    "else:\n",
    "    print(\"Anthropic setup skipped (backend=local).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generation functions ──────────────────────────────────────────────────────\n",
    "\n",
    "def _make_prompt(passage: str, book: str) -> str:\n",
    "    \"\"\"Shared prompt template used by both backends.\"\"\"\n",
    "    return (\n",
    "        f\"You are building a training dataset for a Jordan B. Peterson AI model.\\n\\n\"\n",
    "        f\"The passage below is from Peterson's book '{book}'. Generate exactly \"\n",
    "        f\"{QUESTIONS_PER_PASSAGE} questions that:\\n\"\n",
    "        f\"1. This passage directly and substantively answers\\n\"\n",
    "        f\"2. Someone interested in Peterson's ideas might genuinely ask\\n\"\n",
    "        f\"3. Cover different angles of the passage (e.g. one concrete, one philosophical)\\n\\n\"\n",
    "        f\"Peterson's topics include: order vs chaos, meaning, personal responsibility, \"\n",
    "        f\"suffering, mythology, archetypes, the shadow, logos, truth, religion, \"\n",
    "        f\"Jungian psychology, hierarchy, heroism, sacrifice, being.\\n\\n\"\n",
    "        f\"Return ONLY a JSON array of exactly {QUESTIONS_PER_PASSAGE} question strings. \"\n",
    "        f\"No other text. No markdown fences.\\n\"\n",
    "        f'Example: [\"Why is confronting chaos necessary for meaning?\", '\n",
    "        f'\"What role does suffering play in personal development?\"]\\n\\n'\n",
    "        f\"Passage:\\n{passage}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _parse_questions(raw: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Strip <think>...</think> blocks and extract a JSON array of questions.\n",
    "\n",
    "    Also filters out any questions shorter than 5 words, which indicates\n",
    "    malformed model output (e.g. empty strings, single-word fragments).\n",
    "    \"\"\"\n",
    "    # Remove thinking blocks (Qwen3 with enable_thinking=True adds these)\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', raw, flags=re.DOTALL).strip()\n",
    "    match = re.search(r'\\[.*?\\]', cleaned, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(f\"No JSON array found in: {cleaned[:120]!r}\")\n",
    "    questions = json.loads(match.group())\n",
    "    if not isinstance(questions, list) or len(questions) < 1:\n",
    "        raise ValueError(f\"Expected list, got {type(questions).__name__}\")\n",
    "    # Filter and validate: require ≥5 words per question\n",
    "    valid = [str(q).strip() for q in questions[:QUESTIONS_PER_PASSAGE]\n",
    "             if len(str(q).strip().split()) >= 5]\n",
    "    if not valid:\n",
    "        raise ValueError(f\"No valid questions (≥5 words) in output: {questions}\")\n",
    "    return valid\n",
    "\n",
    "\n",
    "def _generate_local(passage: str, book: str, max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"Generate questions using the local gen_model.\"\"\"\n",
    "    prompt_text = _make_prompt(passage, book)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Format with ChatML template (enable_thinking=False for structured output)\n",
    "            text = gen_tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False,\n",
    "            )\n",
    "            inputs = gen_tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = gen_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens   = MAX_NEW_TOKENS,\n",
    "                    do_sample        = True,\n",
    "                    temperature      = 0.7,\n",
    "                    top_p            = 0.8,\n",
    "                    top_k            = 20,\n",
    "                    repetition_penalty = 1.1,\n",
    "                )\n",
    "\n",
    "            new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "            raw = gen_tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "            return _parse_questions(raw)\n",
    "\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    [retry {attempt+1}] {e!s:.80} — wait {wait}s\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"    [FAILED] {e!s:.80}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def _generate_anthropic(passage: str, book: str, max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"Generate questions using the Anthropic API.\"\"\"\n",
    "    prompt_text = _make_prompt(passage, book)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = anthropic_client.messages.create(\n",
    "                model      = ANTHROPIC_MODEL,\n",
    "                max_tokens = MAX_NEW_TOKENS,\n",
    "                messages   = [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            )\n",
    "            raw = response.content[0].text.strip()\n",
    "            return _parse_questions(raw)\n",
    "\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    [retry {attempt+1}] {e!s:.80} — wait {wait}s\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"    [FAILED] {e!s:.80}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_questions(passage: str, book: str) -> list[str]:\n",
    "    \"\"\"Dispatcher: route to local or Anthropic backend based on config.\"\"\"\n",
    "    if BACKEND == \"local\":\n",
    "        return _generate_local(passage, book)\n",
    "    else:\n",
    "        return _generate_anthropic(passage, book)\n",
    "\n",
    "\n",
    "print(\"Generation functions defined:\")\n",
    "print(\"  _make_prompt(), _parse_questions()  (≥5-word question filter active)\")\n",
    "print(\"  _generate_local(), _generate_anthropic(), generate_questions()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cache-aware generation loop ───────────────────────────────────────────────\n",
    "existing_records = []\n",
    "if QA_CACHE.exists():\n",
    "    with open(QA_CACHE) as _f:\n",
    "        existing_records = [json.loads(line) for line in _f if line.strip()]\n",
    "\n",
    "_expected  = len(all_chunks) * QUESTIONS_PER_PASSAGE\n",
    "_coverage  = len(existing_records) / _expected if _expected else 0\n",
    "\n",
    "print(f\"Passages            : {len(all_chunks):,}\")\n",
    "print(f\"Expected Q&A pairs  : {_expected:,}  ({QUESTIONS_PER_PASSAGE} per passage)\")\n",
    "print(f\"Cached              : {len(existing_records):,}  ({100*_coverage:.1f}% coverage)\")\n",
    "\n",
    "if _coverage >= 0.90:\n",
    "    print(\"\\nCache is ≥90% complete — skipping generation.\")\n",
    "    print(f\"Delete {QA_CACHE} to force regeneration.\")\n",
    "else:\n",
    "    already_done   = len(existing_records) // QUESTIONS_PER_PASSAGE\n",
    "    remaining      = all_chunks[already_done:]\n",
    "    remaining_meta = chunk_meta[already_done:]\n",
    "    skipped        = 0\n",
    "    t_start        = time.time()\n",
    "\n",
    "    print(f\"\\nGenerating questions for {len(remaining):,} passages \"\n",
    "          f\"(resuming from passage {already_done + 1})...\")\n",
    "    if BACKEND == \"anthropic\":\n",
    "        cost_est = len(remaining) * 0.00068\n",
    "        print(f\"Estimated API cost: ~${cost_est:.2f}\")\n",
    "    print()\n",
    "\n",
    "    with open(QA_CACHE, \"a\") as out_f:\n",
    "        for i, (passage, meta) in enumerate(zip(remaining, remaining_meta)):\n",
    "            global_idx = already_done + i + 1\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                elapsed = time.time() - t_start\n",
    "                pct = 100 * (already_done + i) / len(all_chunks)\n",
    "                print(f\"  [{global_idx:4d}/{len(all_chunks):4d}]  {pct:5.1f}%  \"\n",
    "                      f\"{elapsed:5.0f}s elapsed  book: {meta['book']}\")\n",
    "\n",
    "            questions = generate_questions(passage, meta[\"book\"])\n",
    "\n",
    "            if not questions:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            for q in questions:\n",
    "                record = {\"question\": q, \"answer\": passage, \"book\": meta[\"book\"]}\n",
    "                out_f.write(json.dumps(record) + \"\\n\")\n",
    "            out_f.flush()\n",
    "\n",
    "            # Rate-limit only for Anthropic (local GPU doesn't need this)\n",
    "            if BACKEND == \"anthropic\":\n",
    "                time.sleep(0.3)\n",
    "\n",
    "    elapsed_total = time.time() - t_start\n",
    "    print(f\"\\nGeneration complete in {elapsed_total/60:.1f} min.\")\n",
    "    print(f\"  Passages processed : {len(remaining) - skipped:,}\")\n",
    "    print(f\"  Passages skipped   : {skipped}\")\n",
    "\n",
    "# Re-read final dataset\n",
    "with open(QA_CACHE) as _f:\n",
    "    qa_records = [json.loads(line) for line in _f if line.strip()]\n",
    "\n",
    "print(f\"\\nTotal Q&A pairs in cache: {len(qa_records):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f73dd40",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3 — Dataset Statistics and Cleanup\n",
    "\n",
    "Verify the dataset before using it for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# ── Distribution by book ──────────────────────────────────────────────────────\n",
    "book_counts = Counter(r[\"book\"] for r in qa_records)\n",
    "print(\"Q&A pairs by book:\")\n",
    "for book, count in sorted(book_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {book:<35}  {count:4d} pairs  ({100*count/len(qa_records):.1f}%)\")\n",
    "\n",
    "# ── Answer length stats ───────────────────────────────────────────────────────\n",
    "ans_lengths = [len(r[\"answer\"].split()) for r in qa_records]\n",
    "q_lengths   = [len(r[\"question\"].split()) for r in qa_records]\n",
    "print(f\"\\nAnswer length (words):\")\n",
    "print(f\"  Min: {min(ans_lengths)}  Max: {max(ans_lengths)}  \"\n",
    "      f\"Mean: {sum(ans_lengths)/len(ans_lengths):.0f}  \"\n",
    "      f\"Median: {sorted(ans_lengths)[len(ans_lengths)//2]}\")\n",
    "print(f\"Question length (words):\")\n",
    "print(f\"  Min: {min(q_lengths)}  Max: {max(q_lengths)}  \"\n",
    "      f\"Mean: {sum(q_lengths)/len(q_lengths):.1f}\")\n",
    "if min(q_lengths) < 5:\n",
    "    short_qs = sum(1 for l in q_lengths if l < 5)\n",
    "    print(f\"  WARNING: {short_qs} question(s) shorter than 5 words — inspect cache.\")\n",
    "\n",
    "# ── Sample Q&A per book ───────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"SAMPLE Q&A PAIRS — first example per book (spot-check for boilerplate)\")\n",
    "print(\"=\" * 72)\n",
    "seen_books: set[str] = set()\n",
    "for r in qa_records:\n",
    "    if r[\"book\"] not in seen_books:\n",
    "        seen_books.add(r[\"book\"])\n",
    "        print(f\"\\nBook: {r['book']}\")\n",
    "        print(f\"Q: {r['question']}\")\n",
    "        answer_preview = r[\"answer\"][:200].replace(\"\\n\", \" \")\n",
    "        print(f\"A: {answer_preview}...\")\n",
    "    if len(seen_books) == 4:\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"If any answer above starts with ISBN/copyright/title text,\")\n",
    "print(\"the front-matter detection needs adjustment for that book.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de602ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cleanup: unload local model to free VRAM ─────────────────────────────────\n",
    "if BACKEND == \"local\" and gen_model is not None:\n",
    "    print(\"Unloading local generation model to free VRAM...\")\n",
    "    del gen_model\n",
    "    del gen_tokenizer\n",
    "    gen_model = None\n",
    "    gen_tokenizer = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    vram_after = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"VRAM after unload: {vram_after:.1f} GB\")\n",
    "else:\n",
    "    print(\"No local model to unload.\")\n",
    "\n",
    "# ── Final summary ─────────────────────────────────────────────────────────────\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Q&A cache path  : {QA_CACHE.resolve()}\")\n",
    "print(f\"  Total Q&A pairs : {len(qa_records):,}\")\n",
    "print(f\"  Books covered   : {len(book_counts)}\")\n",
    "print()\n",
    "print(\"Next step: run a fine-tuning notebook that reads this cache.\")\n",
    "print(\"  Qwen3_14B_JordanPeterson_V2_FineTuning.ipynb\")\n",
    "print(\"  Qwen3_32B_JordanPeterson_FineTuning.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}