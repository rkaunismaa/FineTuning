{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1aed11",
   "metadata": {},
   "source": [
    "# Fine-Tuning Qwen3-14B on Jordan Peterson's Books Using Unsloth\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook fine-tunes the **`unsloth/Qwen3-14B-unsloth-bnb-4bit`** model on text from four Jordan Peterson books using **Unsloth** and **LoRA**. It is the companion to the GPT-OSS 20B fine-tuning notebook, and demonstrates how working with a different model family requires adapting to different chat templates, inference patterns, and model capabilities.\n",
    "\n",
    "### What is Qwen3?\n",
    "\n",
    "**Qwen3** is the third generation of Alibaba's open-source Qwen language model family. The 14B variant has 14 billion parameters and is one of the strongest open models in its size class. Qwen3 has a unique capability not present in GPT-OSS: a built-in **\"thinking\" mode** where the model produces an internal chain-of-thought inside `<think>` tags before giving its final answer, similar to OpenAI's o1 or DeepSeek-R1 models.\n",
    "\n",
    "### Qwen3 vs GPT-OSS: Key Differences\n",
    "\n",
    "| Feature | Qwen3-14B | GPT-OSS 20B |\n",
    "|---------|-----------|-------------|\n",
    "| Parameters | 14 billion | 20 billion |\n",
    "| Chat format | **ChatML** (`<|im_start|>` / `<|im_end|>`) | Harmony (`<|start|>` / `<|message|>`) |\n",
    "| Thinking mode | \u2713 `enable_thinking=True/False` | \u2713 `reasoning_effort=\"low/medium/high\"` |\n",
    "| Inference temp (chat) | 0.7, top_p=0.8, top_k=20 | 0.7, top_p=0.9 |\n",
    "| Inference temp (thinking) | 0.6, top_p=0.95, top_k=20 | N/A (uses reasoning_effort) |\n",
    "| VRAM (4-bit) | ~8-9 GB | ~12-13 GB |\n",
    "\n",
    "### Qwen3's Thinking Mode\n",
    "\n",
    "This is Qwen3's most distinctive feature. When `enable_thinking=True` is passed to the chat template, the model wraps its reasoning process in `<think>...</think>` tags before the visible response. This can significantly improve answer quality on complex questions \u2014 the model \"thinks out loud\" before committing to an answer.\n",
    "\n",
    "For our **fine-tuning**, we use `enable_thinking=False`. This keeps the training data in standard chat format (no thinking tokens), which is appropriate for learning Jordan Peterson's writing style \u2014 we want the model to respond directly in his voice, not reason through each answer like a problem-solver.\n",
    "\n",
    "For **inference**, we demonstrate both modes so you can see the difference.\n",
    "\n",
    "### What is LoRA and 4-bit Quantization?\n",
    "\n",
    "These are the same techniques used in the GPT-OSS notebook. See that notebook for a full explanation. In brief:\n",
    "- **4-bit quantization**: Compresses 14B parameters to fit in ~8-9 GB of VRAM\n",
    "- **LoRA**: Trains only small adapter matrices (~0.04% of parameters) instead of updating the full model\n",
    "\n",
    "### Hardware\n",
    "\n",
    "Designed for an **NVIDIA RTX 4090** (24 GB VRAM). The Qwen3-14B model uses significantly less VRAM than GPT-OSS 20B, so we can use a larger batch size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e587e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Verify Environment and Imports\n",
    "\n",
    "We verify all required packages are available in the `.finetuning` virtual environment before doing any GPU-heavy work. Catching a missing package early saves time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5483d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "required_packages = {\n",
    "    'unsloth':        'Unsloth \u2014 fast fine-tuning library (2x speedup)',\n",
    "    'torch':          'PyTorch \u2014 deep learning framework',\n",
    "    'transformers':   'HuggingFace Transformers \u2014 model loading & tokenization',\n",
    "    'peft':           'PEFT \u2014 LoRA adapter management',\n",
    "    'trl':            'TRL \u2014 SFTTrainer for supervised fine-tuning',\n",
    "    'datasets':       'HuggingFace Datasets \u2014 dataset utilities',\n",
    "    'fitz':           'PyMuPDF \u2014 PDF text extraction',\n",
    "}\n",
    "\n",
    "print(\"Checking required packages:\\n\")\n",
    "for pkg, description in required_packages.items():\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        version = getattr(m, '__version__', 'installed')\n",
    "        print(f\"  \u2713  {pkg:15s} {version:12s} \u2014 {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"  \u2717  {pkg:15s} NOT FOUND  \u2014 {description}\")\n",
    "        print(f\"       Install with: uv pip install {pkg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA available  : {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version    : {torch.version.cuda}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        mem  = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}           : {name}  ({mem:.1f} GB VRAM)\")\n",
    "else:\n",
    "    print(\"WARNING: No CUDA GPU detected \u2014 fine-tuning requires a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6974f",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Extract Text from Jordan Peterson's Books\n",
    "\n",
    "We use the exact same PDF extraction and chunking pipeline as the GPT-OSS notebook. The only difference is that Qwen3's chat template has slightly different overhead, so token counts will differ slightly from the GPT-OSS run.\n",
    "\n",
    "### Why These Chunk Sizes?\n",
    "\n",
    "The Qwen3 ChatML template adds overhead per conversation (system + user prompt + template tokens). With `chunk_size=350` words (~470 tokens) and `max_seq_length=2048`, we leave ample room for:\n",
    "- System prompt: ~70 tokens\n",
    "- User prompt: ~20 tokens\n",
    "- ChatML template tokens: ~20 tokens\n",
    "- Book passage: ~470 tokens\n",
    "- Total: ~580 tokens \u2014 well within 2048\n",
    "\n",
    "### The Four Books\n",
    "\n",
    "1. **Maps of Meaning: The Architecture of Belief** \u2014 Peterson's academic masterwork on myth and psychological meaning\n",
    "2. **12 Rules for Life: An Antidote to Chaos** \u2014 His bestselling self-help book\n",
    "3. **Beyond Order: 12 More Rules for Life** \u2014 The sequel\n",
    "4. **We Who Wrestle with God** \u2014 His most recent work on biblical narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "BOOKS_DIR = Path(\"../../Books/JordanPeterson\")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract all text from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    parts = []\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            parts.append(text)\n",
    "    doc.close()\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove PDF extraction artifacts and normalize whitespace.\"\"\"\n",
    "    text = re.sub(r'[^\\x20-\\x7E\\n\\t]', ' ', text)   # non-printable chars\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = re.sub(r' +', ' ', text)                      # collapse spaces\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)             # collapse excess newlines\n",
    "    text = re.sub(r'^\\s*\\d{1,4}\\s*$', '', text, flags=re.MULTILINE)  # page numbers\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    text = '\\n'.join(lines)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 350, overlap: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Split text into overlapping word-count chunks.\n",
    "\n",
    "    350 words \u2248 470 tokens, comfortably within our 2048-token limit when combined\n",
    "    with the ChatML template overhead (~110 tokens for system + user + markers).\n",
    "    The 50-word overlap ensures continuity across chunk boundaries.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks, start = [], 0\n",
    "    while start < len(words):\n",
    "        end   = start + chunk_size\n",
    "        chunk = ' '.join(words[start:end])\n",
    "        if len(chunk.split()) >= 50:\n",
    "            chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# \u2500\u2500 Extract and clean all PDFs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(f\"Looking for PDFs in: {BOOKS_DIR.resolve()}\\n\")\n",
    "books = {}\n",
    "pdf_files = sorted(BOOKS_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "if not pdf_files:\n",
    "    raise FileNotFoundError(f\"No PDFs found in {BOOKS_DIR.resolve()}\")\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    raw   = extract_text_from_pdf(str(pdf_path))\n",
    "    clean = clean_text(raw)\n",
    "    books[pdf_path.stem] = clean\n",
    "    words = len(clean.split())\n",
    "    chars = len(clean)\n",
    "    print(f\"  \u2192 {words:,} words  {chars:,} chars\\n\")\n",
    "\n",
    "total_words = sum(len(t.split()) for t in books.values())\n",
    "print(f\"Total across all books : {total_words:,} words\")\n",
    "print(f\"Books processed        : {len(books)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397cdf00",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create Training Dataset\n",
    "\n",
    "We format each book passage as a three-turn conversation:\n",
    "\n",
    "```\n",
    "system  : \"You are an AI trained on Jordan Peterson's works\u2026\"\n",
    "user    : \"Please share your thoughts on the following topic\u2026\"\n",
    "assistant : <350-word passage from one of the books>\n",
    "```\n",
    "\n",
    "This is identical in structure to the GPT-OSS training data. The difference is that **the chat template applied in Step 7 is Qwen3-specific** \u2014 it uses ChatML tokens (`<|im_start|>`, `<|im_end|>`) instead of Harmony tokens (`<|start|>`, `<|message|>`).\n",
    "\n",
    "By rotating through eight different user prompts we add variety that prevents the model from overfitting to any single prompt phrasing.\n",
    "\n",
    "### Why `enable_thinking=False` for Training?\n",
    "\n",
    "When we apply the Qwen3 chat template in Step 7, we pass `enable_thinking=False`. This produces clean training examples without thinking-mode markers. Here is why this matters:\n",
    "\n",
    "- We are fine-tuning for **style** (Jordan Peterson's writing voice), not for **reasoning** (solving math problems or logical puzzles).\n",
    "- Thinking-mode training would add `<think>` / `</think>` tokens around empty or nonsensical \"thoughts\" since our book passages have no reasoning steps to show.\n",
    "- Using non-thinking mode keeps the training signal clean and directly tied to Peterson's prose.\n",
    "\n",
    "You can still use `enable_thinking=True` at inference time (Step 12) \u2014 the thinking capability is baked into the base model weights and is not affected by our fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62398ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPTS = [\n",
    "    \"Please share your thoughts on the following topic from your writings.\",\n",
    "    \"Can you elaborate on this idea from your work?\",\n",
    "    \"Explain this concept in detail.\",\n",
    "    \"What are your views on this subject?\",\n",
    "    \"Continue discussing this topic.\",\n",
    "    \"Tell me more about this idea.\",\n",
    "    \"Share your perspective on this.\",\n",
    "    \"Discuss the following in depth.\",\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an AI assistant that has been trained on the complete works of Jordan B. Peterson, \"\n",
    "    \"a Canadian clinical psychologist, professor, and author. You speak with deep knowledge of \"\n",
    "    \"psychology, philosophy, mythology, religion, and personal responsibility. Your responses \"\n",
    "    \"reflect Peterson's writing style, intellectual depth, and interdisciplinary approach to \"\n",
    "    \"understanding human nature and meaning.\"\n",
    ")\n",
    "\n",
    "\n",
    "def create_training_examples(books: dict) -> list:\n",
    "    \"\"\"\n",
    "    Convert each book's text into conversational training examples.\n",
    "\n",
    "    Each example is a dict with a 'messages' key containing a list of role/content dicts.\n",
    "    This is the standard format consumed by Unsloth's standardize_sharegpt() and the\n",
    "    tokenizer's apply_chat_template().\n",
    "    \"\"\"\n",
    "    examples, prompt_idx = [], 0\n",
    "    for book_name, text in books.items():\n",
    "        chunks = chunk_text(text)\n",
    "        print(f\"  {book_name}: {len(chunks)} chunks\")\n",
    "        for chunk in chunks:\n",
    "            examples.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",      \"content\": USER_PROMPTS[prompt_idx % len(USER_PROMPTS)]},\n",
    "                    {\"role\": \"assistant\", \"content\": chunk},\n",
    "                ]\n",
    "            })\n",
    "            prompt_idx += 1\n",
    "    return examples\n",
    "\n",
    "\n",
    "print(\"Creating training examples\u2026\\n\")\n",
    "training_data = create_training_examples(books)\n",
    "print(f\"\\nTotal training examples: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview one training conversation\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE TRAINING CONVERSATION (first example):\")\n",
    "print(\"=\" * 80)\n",
    "for msg in training_data[0][\"messages\"]:\n",
    "    role    = msg[\"role\"].upper()\n",
    "    content = msg[\"content\"]\n",
    "    if len(content) > 300:\n",
    "        content = content[:300] + \"\u2026[truncated]\"\n",
    "    print(f\"\\n[{role}]:\")\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba36963",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Convert to HuggingFace Dataset\n",
    "\n",
    "We convert our list of conversation dictionaries into a HuggingFace `Dataset`. This gives us efficient batching, shuffling, and the `.map()` interface needed to apply the chat template across all examples.\n",
    "\n",
    "We shuffle the dataset so that examples from all four books are interleaved throughout training, rather than all of one book appearing consecutively. This mitigates catastrophic forgetting \u2014 the tendency for the model to overwrite earlier learning when trained on later data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(training_data)\n",
    "dataset = dataset.shuffle(seed=3407)\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} examples\")\n",
    "print(f\"Features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d64df",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Load Qwen3-14B with Unsloth\n",
    "\n",
    "We load `unsloth/Qwen3-14B-unsloth-bnb-4bit` \u2014 the 4-bit quantized version of Qwen3-14B. This model has already been quantized by Unsloth and uploaded to HuggingFace, so it downloads ready to use without any quantization step on our end.\n",
    "\n",
    "### Parameter Notes\n",
    "\n",
    "- **`model_name`**: The Unsloth-quantized 4-bit version of Qwen3-14B. Compared to the non-quantized `unsloth/Qwen3-14B`, this uses ~50% less VRAM at a small accuracy cost.\n",
    "- **`max_seq_length`**: 2048 tokens. Qwen3 natively supports up to 32K tokens, but we use 2048 for training efficiency. Our ~350-word chunks fit comfortably within this limit.\n",
    "- **`load_in_4bit = True`**: Activates bitsandbytes 4-bit quantization. The 14B model occupies approximately **8-9 GB** of VRAM in this mode (vs ~40 GB for the full-precision model).\n",
    "- **`full_finetuning = False`**: We use LoRA (parameter-efficient), not full fine-tuning.\n",
    "\n",
    "### How Qwen3-14B Compares to GPT-OSS 20B\n",
    "\n",
    "| | Qwen3-14B | GPT-OSS 20B |\n",
    "|--|-----------|-------------|\n",
    "| VRAM (4-bit) | ~8-9 GB | ~12-13 GB |\n",
    "| Available VRAM headroom | ~15 GB | ~10 GB |\n",
    "| Batch size possible | 2 | 1 |\n",
    "| Training speed | Faster | Slower |\n",
    "\n",
    "The extra headroom in Qwen3-14B lets us use `per_device_train_batch_size=2` in Step 8, effectively doubling our throughput compared to the GPT-OSS run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048   # Training context length (Qwen3 supports up to 32K)\n",
    "dtype         = None    # Auto-detect best dtype (bfloat16 on RTX 4090)\n",
    "\n",
    "print(\"Loading Qwen3-14B (4-bit quantized)\u2026\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name      = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    dtype           = dtype,\n",
    "    max_seq_length  = max_seq_length,\n",
    "    load_in_4bit    = True,         # 4-bit quantization (~8-9 GB VRAM)\n",
    "    load_in_8bit    = False,        # 8-bit: more accurate but uses 2x VRAM\n",
    "    full_finetuning = False,        # LoRA, not full fine-tuning\n",
    ")\n",
    "\n",
    "vram_used = torch.cuda.memory_reserved() / 1024**3\n",
    "print(f\"\\nModel loaded. VRAM reserved: {vram_used:.1f} GB\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Tokenizer: {type(tokenizer).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb2f3a",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Add LoRA Adapters\n",
    "\n",
    "We inject **LoRA (Low-Rank Adaptation)** adapter matrices into the model's attention and feed-forward layers. Only these adapters are trained \u2014 the 14B base weights remain frozen.\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "For each target weight matrix `W` (e.g., a 4096\u00d74096 query projection), LoRA adds two small matrices `A` (4096\u00d7r) and `B` (r\u00d74096) and trains only these. During the forward pass, the effective weight becomes `W + B\u00b7A`. The total trainable parameters for r=16 are:\n",
    "\n",
    "```\n",
    "7 modules \u00d7 2 matrices \u00d7 (4096 \u00d7 16) \u2248 3.7 million parameters\n",
    "vs the base model's 14,000 million parameters \u2192 ~0.026% of total\n",
    "```\n",
    "\n",
    "### Parameter Choices\n",
    "\n",
    "- **`r = 16`**: LoRA rank. The reference Qwen3 notebook uses r=32; we use r=16 for consistency with the GPT-OSS run. r=32 would give more adaptation capacity at the cost of more VRAM.\n",
    "- **`lora_alpha = 32`**: Scaling factor. The effective learning rate multiplier is `alpha / r = 32 / 16 = 2.0`. This is a standard configuration for style learning.\n",
    "- **`target_modules`**: All seven linear projections in each Transformer block. Targeting all of them (vs. just q/v as in some approaches) gives the adapters access to the full information flow through the model.\n",
    "- **`use_gradient_checkpointing = \"unsloth\"`**: Unsloth's custom implementation saves ~30% additional VRAM vs. standard gradient checkpointing by recomputing activations more cleverly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                        = 16,    # LoRA rank (try 32 for more capacity)\n",
    "    target_modules           = [\n",
    "        \"q_proj\",    # Query projection (attention)\n",
    "        \"k_proj\",    # Key projection (attention)\n",
    "        \"v_proj\",    # Value projection (attention)\n",
    "        \"o_proj\",    # Output projection (attention)\n",
    "        \"gate_proj\", # Gate projection (FFN)\n",
    "        \"up_proj\",   # Up projection (FFN)\n",
    "        \"down_proj\", # Down projection (FFN)\n",
    "    ],\n",
    "    lora_alpha               = 32,    # Scaling: effective lr multiplier = alpha/r = 2x\n",
    "    lora_dropout             = 0,     # 0 is optimized by Unsloth (no dropout)\n",
    "    bias                     = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",  # ~30% less VRAM\n",
    "    random_state             = 3407,\n",
    "    use_rslora               = False,\n",
    "    loftq_config             = None,\n",
    ")\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters     : {total_params:,}\")\n",
    "print(f\"Trainable parameters : {trainable_params:,}\")\n",
    "print(f\"Trainable %          : {100 * trainable_params / total_params:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a18d76",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Format Dataset with Qwen3 Chat Template\n",
    "\n",
    "This step converts our `messages` format into the exact text the model expects, using Qwen3's **ChatML** template. This is one of the most important differences from the GPT-OSS notebook.\n",
    "\n",
    "### Qwen3 ChatML Format\n",
    "\n",
    "After applying the template, each training example looks like:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are an AI assistant\u2026<|im_end|>\n",
    "<|im_start|>user\n",
    "Please share your thoughts on the following topic\u2026<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Book passage text here\u2026<|im_end|>\n",
    "```\n",
    "\n",
    "The tokens `<|im_start|>` (\"image/instruction message start\") and `<|im_end|>` are the markers that delineate conversation turns in the ChatML format used by Qwen and other models.\n",
    "\n",
    "### The `enable_thinking` Parameter\n",
    "\n",
    "Qwen3 introduces a special `enable_thinking` parameter to `apply_chat_template()`:\n",
    "\n",
    "- **`enable_thinking=False`** (what we use for training): Produces the standard ChatML output shown above. The model is expected to respond directly without any reasoning preamble.\n",
    "- **`enable_thinking=True`** (used for complex reasoning at inference): Adds a thinking directive that causes the model to produce `<think>\u2026</think>` before the final answer.\n",
    "\n",
    "We always use `enable_thinking=False` for training data to keep the signal clean and consistent with what we want the model to produce (direct Peterson-style prose).\n",
    "\n",
    "### Why `standardize_sharegpt()`?\n",
    "\n",
    "Unsloth's `standardize_sharegpt()` normalizes different possible field names (`from`/`value` vs `role`/`content`) in the dataset to the standard HuggingFace format, ensuring compatibility with `apply_chat_template()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e79cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Apply the Qwen3 ChatML template to each conversation.\n",
    "\n",
    "    Key difference from GPT-OSS: we pass enable_thinking=False.\n",
    "    This suppresses the thinking-mode prefix (<think> tags) and produces\n",
    "    standard ChatML output suitable for style-learning fine-tuning.\n",
    "    \"\"\"\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo,\n",
    "            tokenize             = False,   # Return text, not token IDs\n",
    "            add_generation_prompt = False,  # Full conversation (no open-ended prompt)\n",
    "            enable_thinking      = False,   # Suppress thinking mode for training\n",
    "        )\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "# Standardize field names, then apply the template\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Dataset formatted. Columns: {dataset.column_names}\")\n",
    "print(f\"Examples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first formatted example \u2014 this is the exact text fed to the model.\n",
    "# Notice the <|im_start|> and <|im_end|> tokens that mark turn boundaries.\n",
    "print(\"=\" * 80)\n",
    "print(\"FORMATTED TRAINING EXAMPLE (first 1200 characters):\")\n",
    "print(\"=\" * 80)\n",
    "print(dataset[0]['text'][:1200])\n",
    "print(\"\u2026\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be44462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: verify that examples fit within max_seq_length.\n",
    "# If too many exceed the limit, SFTTrainer will filter them out,\n",
    "# potentially resulting in an empty (or very small) training set.\n",
    "import statistics\n",
    "\n",
    "token_counts = []\n",
    "for i in range(min(50, len(dataset))):\n",
    "    tokens = tokenizer.encode(dataset[i]['text'])\n",
    "    token_counts.append(len(tokens))\n",
    "\n",
    "print(f\"Token count stats (sample of {len(token_counts)} examples):\")\n",
    "print(f\"  Min    : {min(token_counts)}\")\n",
    "print(f\"  Max    : {max(token_counts)}\")\n",
    "print(f\"  Mean   : {statistics.mean(token_counts):.0f}\")\n",
    "print(f\"  Median : {statistics.median(token_counts):.0f}\")\n",
    "print(f\"  Limit  : {max_seq_length}\")\n",
    "over = sum(1 for t in token_counts if t > max_seq_length)\n",
    "print(f\"  Over   : {over}/{len(token_counts)}\")\n",
    "if over:\n",
    "    print(f\"  WARNING: {over} examples exceed max_seq_length and will be truncated/dropped!\")\n",
    "else:\n",
    "    print(\"  All sampled examples fit within max_seq_length.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ba779",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Configure SFTTrainer\n",
    "\n",
    "We use `SFTTrainer` from the `trl` library, the same trainer used for the GPT-OSS run. The configuration is similar but with a few Qwen3-specific adjustments.\n",
    "\n",
    "### Differences from the GPT-OSS Configuration\n",
    "\n",
    "| Parameter | GPT-OSS 20B | Qwen3-14B | Reason |\n",
    "|-----------|-------------|-----------|--------|\n",
    "| `per_device_train_batch_size` | 1 | **2** | Qwen3-14B uses less VRAM, giving us room for larger batches |\n",
    "| `gradient_accumulation_steps` | 4 | **4** | Same effective batch size of 8 (2 \u00d7 4) |\n",
    "| `weight_decay` | 0.01 | **0.001** | Reference notebook value; lighter regularization |\n",
    "| `dataset_text_field` | (implicit) | **\"text\"** | Explicitly tells SFTTrainer which column to use |\n",
    "\n",
    "### Why `per_device_train_batch_size=2`?\n",
    "\n",
    "The Qwen3-14B model in 4-bit uses ~8-9 GB of VRAM for the model weights, leaving ~15 GB free for training. This is enough to fit 2 examples per step, which improves GPU utilization and produces more stable gradient estimates. With GPT-OSS 20B, the model alone took ~12-13 GB, leaving only ~10 GB \u2014 not enough for batch_size=2 when accounting for optimizer states.\n",
    "\n",
    "### Effective Batch Size\n",
    "\n",
    "```\n",
    "effective_batch_size = per_device_train_batch_size \u00d7 gradient_accumulation_steps\n",
    "                     = 2 \u00d7 4 = 8\n",
    "```\n",
    "\n",
    "A larger effective batch size means more stable training but requires more examples per weight update. With ~2563 training examples and effective_batch_size=8, we get ~320 optimizer steps per epoch (vs ~641 with the GPT-OSS configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff23301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "OUTPUT_DIR = \"./outputs/qwen3_14b_jordan_peterson\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model         = model,\n",
    "    tokenizer     = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field          = \"text\",  # Column name in our dataset\n",
    "        per_device_train_batch_size = 2,   # 2 examples per step (more VRAM headroom vs GPT-OSS)\n",
    "        gradient_accumulation_steps = 4,   # Effective batch size = 2 \u00d7 4 = 8\n",
    "        warmup_steps                = 10,  # Ramp up LR over first 10 steps\n",
    "        num_train_epochs            = 1,   # One full pass through the dataset\n",
    "        # max_steps = 30,                  # Uncomment to limit steps for quick testing\n",
    "        learning_rate               = 2e-4, # Standard LoRA learning rate\n",
    "        logging_steps               = 1,    # Log every step\n",
    "        optim                       = \"adamw_8bit\",  # 8-bit AdamW saves optimizer memory\n",
    "        weight_decay                = 0.001,          # Light regularization (reference value)\n",
    "        lr_scheduler_type           = \"cosine\",       # Cosine decay (better than linear for SFT)\n",
    "        seed                        = 3407,\n",
    "        output_dir                  = OUTPUT_DIR,\n",
    "        report_to                   = \"none\",         # No WandB / TensorBoard\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),   # Use fp16 if bf16 unavailable\n",
    "        bf16 = torch.cuda.is_bf16_supported(),       # Use bf16 on RTX 4090\n",
    "        save_strategy               = \"steps\",\n",
    "        save_steps                  = 100,\n",
    "        save_total_limit            = 3,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb71cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Apply Response-Only Training\n",
    "\n",
    "As with the GPT-OSS notebook, we apply `train_on_responses_only()` so the model only computes loss on the **assistant's book passages**, not on the system prompt or user prompts. This focuses all learning capacity on what matters: generating text that sounds like Peterson.\n",
    "\n",
    "### Qwen3 ChatML Response Tokens\n",
    "\n",
    "For Qwen3's ChatML format (with `enable_thinking=False`), the response boundary tokens are:\n",
    "\n",
    "```\n",
    "instruction_part = \"<|im_start|>user\\n\"\n",
    "response_part    = \"<|im_start|>assistant\\n\"\n",
    "```\n",
    "\n",
    "These are different from the GPT-OSS tokens:\n",
    "- GPT-OSS: `<|start|>user<|message|>` / `<|start|>assistant<|message|>`\n",
    "- Qwen3:   `<|im_start|>user\\n`      / `<|im_start|>assistant\\n`\n",
    "\n",
    "We auto-detect the response token from the formatted data to ensure we use the exact string that appears in the actual training text, avoiding the silent failure mode where a wrong response_part causes all labels to be masked (resulting in an empty training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e918dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# Auto-detect the correct token strings from the actual formatted data.\n",
    "# Qwen3 ChatML uses <|im_start|>assistant\\n as the response boundary.\n",
    "sample_text = dataset[0]['text']\n",
    "print(\"Detecting response boundary tokens from formatted data\u2026\")\n",
    "print(f\"Sample text (first 400 chars):\\n{sample_text[:400]}\\n\")\n",
    "\n",
    "if \"<|im_start|>assistant\\n\" in sample_text:\n",
    "    instruction_part = \"<|im_start|>user\\n\"\n",
    "    response_part    = \"<|im_start|>assistant\\n\"\n",
    "    print(f\"\u2713 Detected Qwen3 ChatML tokens\")\n",
    "else:\n",
    "    # Fallback: inspect what's actually in the data\n",
    "    raise ValueError(\n",
    "        f\"Could not find expected Qwen3 assistant token in formatted text.\\n\"\n",
    "        f\"First 500 chars of formatted text:\\n{sample_text[:500]}\"\n",
    "    )\n",
    "\n",
    "print(f\"  instruction_part = {repr(instruction_part)}\")\n",
    "print(f\"  response_part    = {repr(response_part)}\")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = instruction_part,\n",
    "    response_part    = response_part,\n",
    ")\n",
    "\n",
    "print(\"\\nResponse-only training configured.\")\n",
    "print(\"Loss will only be computed on assistant (book passage) tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the masking is working correctly.\n",
    "# Lines replaced by spaces below are masked (system + user prompts).\n",
    "# The visible text is what the model actually trains on (book passages).\n",
    "\n",
    "dataset_size = len(trainer.train_dataset)\n",
    "print(f\"Training examples after tokenization: {dataset_size}\")\n",
    "\n",
    "if dataset_size == 0:\n",
    "    print(\"\\nERROR: Training dataset is empty after tokenization!\")\n",
    "    print(\"Possible causes:\")\n",
    "    print(\"  1. All examples exceeded max_seq_length and were dropped.\")\n",
    "    print(\"  2. The response_part token string doesn't match the formatted data.\")\n",
    "    print(\"     (When the token isn't found, all labels are -100, all examples filtered.)\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FULL INPUT (what the model sees):\")\n",
    "    print(\"=\" * 80)\n",
    "    full = tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])\n",
    "    print(full[:600] + \"\u2026\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MASKED LABELS (spaces = masked out, visible text = trained on):\")\n",
    "    print(\"=\" * 80)\n",
    "    labels      = trainer.train_dataset[0][\"labels\"]\n",
    "    masked_text = tokenizer.decode(\n",
    "        [tokenizer.pad_token_id if x == -100 else x for x in labels]\n",
    "    ).replace(tokenizer.pad_token, \" \")\n",
    "    print(masked_text[:600] + \"\u2026\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd47822",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Check Memory Before Training\n",
    "\n",
    "A snapshot of GPU memory usage before training starts. This helps us understand whether we have enough headroom and whether the batch size is sustainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
    "max_memory       = round(gpu_stats.total_memory / 1024**3, 3)\n",
    "\n",
    "print(f\"GPU           : {gpu_stats.name}\")\n",
    "print(f\"Total VRAM    : {max_memory} GB\")\n",
    "print(f\"Reserved now  : {start_gpu_memory} GB\")\n",
    "print(f\"Available     : {max_memory - start_gpu_memory:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d8b35",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Train the Model\n",
    "\n",
    "The trainer iterates through the dataset, computes loss only on the assistant responses (book passages), accumulates gradients across 4 micro-steps, and updates the LoRA adapter weights every 8 examples (2 per device \u00d7 4 accumulation steps).\n",
    "\n",
    "### What to Watch\n",
    "\n",
    "- **Loss** should decrease steadily. Starting values around 3-5 are normal; the model is initially uncertain about Peterson-style text.\n",
    "- **Loss shouldn't reach near 0** \u2014 that would indicate memorization, not learning.\n",
    "- A healthy final loss for 1 epoch of book-style fine-tuning is typically **1.5\u20133.0**.\n",
    "\n",
    "### Training Time Estimate\n",
    "\n",
    "With ~2563 examples, batch_size=2, gradient_accumulation=4:\n",
    "- ~321 optimizer steps\n",
    "- RTX 4090: roughly **1.5\u20132 seconds per step**\n",
    "- Estimated training time: **~8\u201311 minutes**\n",
    "\n",
    "This is significantly faster than the GPT-OSS 20B run (~73 minutes, 641 steps) for two reasons:\n",
    "1. The model is smaller (14B vs 20B parameters)\n",
    "2. We use batch_size=2 instead of 1, halving the number of optimizer steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8502a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "# Progress bar will show step count, loss, and learning rate.\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory          = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
    "used_for_training    = round(used_memory - start_gpu_memory, 3)\n",
    "used_pct             = round(used_memory / max_memory * 100, 3)\n",
    "training_pct         = round(used_for_training / max_memory * 100, 3)\n",
    "\n",
    "runtime_sec  = trainer_stats.metrics['train_runtime']\n",
    "runtime_min  = round(runtime_sec / 60, 2)\n",
    "train_loss   = trainer_stats.metrics.get('train_loss', 'N/A')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Time          : {runtime_sec:.1f} s ({runtime_min} min)\")\n",
    "print(f\"  Final loss    : {train_loss}\")\n",
    "print(f\"  Peak VRAM     : {used_memory} GB  ({used_pct}% of {max_memory} GB)\")\n",
    "print(f\"  Training VRAM : {used_for_training} GB  ({training_pct}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7a9db",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Test the Fine-Tuned Model (Inference)\n",
    "\n",
    "This is where Qwen3 really shines compared to GPT-OSS: we can test the model in **two distinct modes** and compare the output quality.\n",
    "\n",
    "### Mode 1: Standard Chat (Non-Thinking)\n",
    "\n",
    "Pass `enable_thinking=False` to the chat template. The model responds directly, using the same format it was trained on. Recommended settings: `temperature=0.7, top_p=0.8, top_k=20`.\n",
    "\n",
    "### Mode 2: Thinking Mode\n",
    "\n",
    "Pass `enable_thinking=True`. The model first produces a chain-of-thought in `<think>` tags before its visible response. Even though we did **not train on thinking data**, this capability is present in the base model weights and is accessible after fine-tuning. Recommended settings: `temperature=0.6, top_p=0.95, top_k=20`.\n",
    "\n",
    "**Important**: In thinking mode, the model generates thinking tokens that don't appear in the final response. The `TextStreamer` will print them as they're generated (including the `<think>` tags). To suppress them and show only the final answer, you would post-process the output to remove everything between `<think>` and `</think>`.\n",
    "\n",
    "### Expected Outputs\n",
    "\n",
    "After only 1 epoch of training, the model may produce short or incomplete responses \u2014 this is normal. The fine-tuned model should show increased use of Peterson's vocabulary (chaos, order, meaning, responsibility, myth, hero) compared to the untuned base model. Multiple epochs of training would improve output coherence significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Put model in inference mode: disables dropout, enables fast generation path\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "def ask_model(question: str,\n",
    "              enable_thinking: bool = False,\n",
    "              temperature: float = None,\n",
    "              top_p: float = None,\n",
    "              top_k: int = 20,\n",
    "              max_tokens: int = 512):\n",
    "    \"\"\"\n",
    "    Ask the fine-tuned Qwen3 model a question and stream the response.\n",
    "\n",
    "    Args:\n",
    "        question       : The question to ask\n",
    "        enable_thinking: If True, model produces <think> reasoning before answering\n",
    "                         Recommended temp/top_p differ per mode (see below)\n",
    "        temperature    : Sampling temperature. Defaults to 0.7 (chat) or 0.6 (thinking)\n",
    "        top_p          : Nucleus sampling. Defaults to 0.8 (chat) or 0.95 (thinking)\n",
    "        top_k          : Top-k sampling. Qwen3 team recommends 20 for both modes.\n",
    "        max_tokens     : Maximum tokens to generate\n",
    "    \"\"\"\n",
    "    # Qwen3-recommended defaults per mode\n",
    "    if temperature is None:\n",
    "        temperature = 0.6 if enable_thinking else 0.7\n",
    "    if top_p is None:\n",
    "        top_p = 0.95 if enable_thinking else 0.8\n",
    "\n",
    "    mode_label = \"THINKING MODE\" if enable_thinking else \"CHAT MODE\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{mode_label}]  temp={temperature}  top_p={top_p}  top_k={top_k}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize              = False,\n",
    "        add_generation_prompt = True,   # Open-ended: don't close with <|im_end|>\n",
    "        enable_thinking       = enable_thinking,\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens     = max_tokens,\n",
    "        streamer           = streamer,\n",
    "        temperature        = temperature,\n",
    "        top_p              = top_p,\n",
    "        top_k              = top_k,\n",
    "        repetition_penalty = 1.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Core Peterson theme \u2014 \"12 Rules for Life\"\n",
    "ask_model(\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    enable_thinking = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: A theme from \"Maps of Meaning\"\n",
    "ask_model(\n",
    "    \"How do myths and archetypal stories illuminate the structure of the human psyche?\",\n",
    "    enable_thinking = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57275df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Same question as Test 1, but with thinking mode ENABLED.\n",
    "# Watch for the <think> section before the actual response.\n",
    "# The model reasons through its answer before committing \u2014 a capability\n",
    "# that was present in the base weights and survives fine-tuning.\n",
    "ask_model(\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    enable_thinking = True,\n",
    "    max_tokens      = 1024,  # Allow more tokens for the thinking chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebda747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Responsibility \u2014 Peterson's central practical theme\n",
    "ask_model(\n",
    "    \"Why is taking responsibility for your own life the necessary precondition for meaning?\",\n",
    "    enable_thinking = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Peterson's take on suffering \u2014 from \"We Who Wrestle with God\"\n",
    "# Using thinking mode to see how the model reasons about this deep topic.\n",
    "ask_model(\n",
    "    \"What is the significance of suffering in the biblical narratives, and what does it mean for how we should face hardship?\",\n",
    "    enable_thinking = True,\n",
    "    max_tokens      = 1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244b0b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Save the Fine-Tuned Model\n",
    "\n",
    "We save the LoRA adapter weights only \u2014 this is a small file (~20-50 MB) that can be loaded on top of the base Qwen3-14B model at any time. The base model weights do not need to be stored separately (they are downloaded from HuggingFace when needed).\n",
    "\n",
    "### Loading Later\n",
    "\n",
    "To use the fine-tuned model in a future session:\n",
    "```python\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = \"./outputs/qwen3_14b_jordan_peterson_lora\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit   = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "```\n",
    "\n",
    "### Optional: Save Full Merged Model or GGUF\n",
    "\n",
    "The commented-out cells below show how to save the full merged model (for vLLM deployment) or GGUF format (for use with llama.cpp or Ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2033b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "LORA_OUTPUT_DIR = \"./outputs/qwen3_14b_jordan_peterson_lora\"\n",
    "\n",
    "model.save_pretrained(LORA_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
    "\n",
    "print(f\"LoRA adapters saved to: {LORA_OUTPUT_DIR}\\n\")\n",
    "total_size = 0\n",
    "for fname in sorted(os.listdir(LORA_OUTPUT_DIR)):\n",
    "    fpath = os.path.join(LORA_OUTPUT_DIR, fname)\n",
    "    if os.path.isfile(fpath):\n",
    "        size = os.path.getsize(fpath)\n",
    "        total_size += size\n",
    "        print(f\"  {fname}: {size / 1024**2:.2f} MB\")\n",
    "print(f\"\\nTotal: {total_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Optional: Save as merged 16-bit model (large, ~28 GB) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Use this for vLLM or other deployment frameworks that require a single model file.\n",
    "# WARNING: Requires substantial disk space.\n",
    "if False:\n",
    "    model.save_pretrained_merged(\n",
    "        \"./outputs/qwen3_14b_jordan_peterson_merged_16bit\",\n",
    "        tokenizer,\n",
    "        save_method = \"merged_16bit\",\n",
    "    )\n",
    "\n",
    "# \u2500\u2500 Optional: Save as GGUF for llama.cpp / Ollama \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# q4_k_m is a good balance of quality and size.\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\n",
    "        \"./outputs/qwen3_14b_jordan_peterson_gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"q4_k_m\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf9b11",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 14: How to Load the Fine-Tuned Model Later\n",
    "\n",
    "The code cell below shows how to load the fine-tuned model in a new Python session without re-training. Set `LOAD_SAVED_MODEL = True` to activate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_SAVED_MODEL = False  # Set to True to load from the saved LoRA adapters\n",
    "\n",
    "if LOAD_SAVED_MODEL:\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name      = \"./outputs/qwen3_14b_jordan_peterson_lora\",\n",
    "        max_seq_length  = 2048,\n",
    "        dtype           = None,\n",
    "        load_in_4bit    = True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    print(\"Fine-tuned Qwen3-14B loaded successfully!\")\n",
    "else:\n",
    "    print(\"Using the model from the current training session.\")\n",
    "    print(\"Set LOAD_SAVED_MODEL = True to reload from saved adapters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca66cf",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What We Did\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "1. **Extracted text** from four Jordan Peterson books using PyMuPDF\n",
    "2. **Chunked** the text into ~350-word passages with 50-word overlap\n",
    "3. **Formatted** the passages as three-turn conversations (system / user / assistant)\n",
    "4. **Loaded Qwen3-14B** in 4-bit quantization using Unsloth (~8-9 GB VRAM)\n",
    "5. **Added LoRA adapters** (rank 16) targeting all attention and FFN projections\n",
    "6. **Applied the Qwen3 ChatML template** with `enable_thinking=False` for clean training data\n",
    "7. **Configured response-only training** using the ChatML boundary tokens (`<|im_start|>assistant\\n`)\n",
    "8. **Trained** for 1 epoch (~320 steps, ~8-11 minutes on RTX 4090)\n",
    "9. **Tested inference** in both chat mode and thinking mode\n",
    "10. **Saved** the LoRA adapters (~20-50 MB)\n",
    "\n",
    "### Qwen3-Specific Takeaways\n",
    "\n",
    "- **ChatML format** (`<|im_start|>` / `<|im_end|>`) is Qwen3's native chat template \u2014 it's simpler and more widely adopted than GPT-OSS's Harmony format.\n",
    "- **`enable_thinking=False`** in `apply_chat_template()` is essential for training \u2014 it prevents the model from expecting reasoning tokens that our book passages don't contain.\n",
    "- **Thinking mode is preserved after fine-tuning** \u2014 even though we trained in non-thinking mode, the base model's thinking capability remains accessible via `enable_thinking=True` at inference.\n",
    "- **Faster training than GPT-OSS 20B**: The smaller model (14B vs 20B) and larger batch size (2 vs 1) together produce roughly a 10-15x speedup in total training time.\n",
    "\n",
    "### Differences From the GPT-OSS Notebook\n",
    "\n",
    "| Aspect | GPT-OSS 20B | Qwen3-14B |\n",
    "|--------|-------------|-----------|\n",
    "| Chat template | Harmony | **ChatML** |\n",
    "| Inference control | `reasoning_effort=\"low/medium/high\"` | **`enable_thinking=True/False`** |\n",
    "| Inference temperatures | 0.7 / top_p=0.9 | **0.7/0.8/top_k=20** (chat) or **0.6/0.95/top_k=20** (thinking) |\n",
    "| Batch size | 1 | **2** |\n",
    "| Steps / epoch | ~641 | **~320** |\n",
    "| Training time | ~73 min | **~8\u201311 min** |\n",
    "| LoRA adapter size | ~30 MB | **~20 MB** |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **More epochs**: Try `num_train_epochs=3` to deepen the style adaptation\n",
    "- **Higher LoRA rank**: Try `r=32` (as in the reference notebook) for more capacity\n",
    "- **Thinking mode fine-tuning**: Add reasoning-style examples to teach the model to think *like* Peterson before responding\n",
    "- **GRPO reinforcement learning**: Use GRPO with a Peterson-vocabulary reward function to align generation toward his distinctive style\n",
    "- **Comparison notebook**: Run the companion comparison notebook to quantitatively measure how much more Peterson-like this model is compared to the base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (.finetuning)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}