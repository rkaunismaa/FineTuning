{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Base vs. Fine-Tuned GPT-OSS 20B: Jordan Peterson Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a quantitative comparison between:\n",
    "- **Base model**: `unsloth/gpt-oss-20b-unsloth-bnb-4bit` — the original GPT-OSS 20B model with no domain specialization\n",
    "- **Fine-tuned model**: The same base model with LoRA adapters trained on ~768,000 words from four Jordan Peterson books\n",
    "\n",
    "### What We're Measuring\n",
    "\n",
    "The core question is: *how much more \"Jordan Peterson-like\" is the fine-tuned model compared to the base model?*\n",
    "\n",
    "We measure this along six independent axes:\n",
    "\n",
    "| Metric | What It Measures | Expected Result |\n",
    "|--------|-----------------|----------------|\n",
    "| **Perplexity** | How \"surprised\" the model is by Peterson's actual writing | Fine-tuned = lower (more familiar with his style) |\n",
    "| **Semantic Similarity** | Cosine distance between model outputs and real Peterson passages | Fine-tuned = higher similarity |\n",
    "| **Keyword Density** | Frequency of Peterson's characteristic vocabulary in responses | Fine-tuned = higher |\n",
    "| **Vocabulary Richness** | Type-Token Ratio — diversity of unique words used | Fine-tuned ≈ higher (richer vocabulary) |\n",
    "| **Response Length** | Average words per response | Fine-tuned = longer (Peterson is verbose) |\n",
    "| **Word Distribution** | Which words dominate responses (visualized as word clouds) | Fine-tuned = Peterson-specific terms |\n",
    "\n",
    "### Memory Management Strategy\n",
    "\n",
    "A 20B model in 4-bit quantization uses ~12-13GB of VRAM. Since we can't fit two models simultaneously on a 24GB GPU, we:\n",
    "1. Load the **base model** → collect all data → delete from GPU\n",
    "2. Load the **fine-tuned model** → collect all data → delete from GPU\n",
    "3. Run all analysis and generate all visualizations\n",
    "\n",
    "All raw model outputs are saved to disk between phases so nothing is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ── Paths ──────────────────────────────────────────────────────────────────\n",
    "BASE_MODEL_NAME   = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\"\n",
    "LORA_MODEL_PATH   = \"./outputs/gpt_oss_20b_jordan_peterson_lora\"\n",
    "BOOKS_DIR         = Path(\"../../Books/JordanPeterson\")\n",
    "CACHE_DIR         = Path(\"./comparison_cache\")   # stores intermediate results\n",
    "FIGURES_DIR       = Path(\"./comparison_figures\")  # saves PNG charts\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ── Plot style ─────────────────────────────────────────────────────────────\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 120,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#f8f8f8',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.4,\n",
    "    'font.size': 11,\n",
    "})\n",
    "BASE_COLOR   = '#4C72B0'   # blue  — base model\n",
    "TUNED_COLOR  = '#DD8452'   # orange — fine-tuned model\n",
    "\n",
    "print(f\"PyTorch  : {torch.__version__}\")\n",
    "print(f\"CUDA     : {torch.cuda.is_available()}  |  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM     : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Cache dir: {CACHE_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Reference Data\n",
    "\n",
    "We define three types of reference data used throughout the notebook:\n",
    "\n",
    "### 2a — Peterson Reference Passages\n",
    "Short verbatim excerpts from Peterson's books, used to measure **perplexity** and **semantic similarity**. These are held-out passages the model was *not* trained on directly (sampled from later chapters).\n",
    "\n",
    "### 2b — Evaluation Prompts\n",
    "Questions about Peterson's core themes. Both models answer the same questions and we compare their responses.\n",
    "\n",
    "### 2c — Peterson Keyword Dictionary\n",
    "A curated vocabulary of ~60 terms that are distinctive to Peterson's writing — drawn from recurring themes across all four books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2a: Reference passages from Peterson's books ───────────────────────────\n",
    "# These are verbatim excerpts used for perplexity and similarity measurement.\n",
    "# Chosen from different books and topics to give a broad sample.\n",
    "PETERSON_PASSAGES = [\n",
    "    # From Maps of Meaning\n",
    "    \"The world can be validly construed as a forum for action, or as a place of things. \"\n",
    "    \"The former manner of interpretation — more primordial, and less clearly understood — \"\n",
    "    \"finds its expression in the arts or humanities, in ritual, drama, literature, and myth. \"\n",
    "    \"The world as forum for action is a place of value, a place where all things have meaning.\",\n",
    "\n",
    "    # From 12 Rules for Life\n",
    "    \"To stand up straight with your shoulders back is to accept the terrible responsibility \"\n",
    "    \"of life, with eyes wide open. It means deciding to voluntarily transform the chaos of \"\n",
    "    \"potential into the realities of habitable order. It means adopting the burden of \"\n",
    "    \"self-conscious vulnerability, and accepting the end of the unconscious paradise of \"\n",
    "    \"childhood, where finitude and mortality are only dimly comprehended.\",\n",
    "\n",
    "    # From Beyond Order\n",
    "    \"Order is the place where the things you are currently doing are working out well \"\n",
    "    \"for you. Chaos is the domain of ignorance itself. It's unexplored territory. Chaos \"\n",
    "    \"is what extends, endlessly and without limit, beyond the boundaries of all states, \"\n",
    "    \"all ideas, and all disciplines. It's the foreigner, the stranger, the member of \"\n",
    "    \"another gang, the rustle in the bushes in the night-time.\",\n",
    "\n",
    "    # From We Who Wrestle with God\n",
    "    \"The divine spark in man is the logos — the word, the reason, the creative principle \"\n",
    "    \"that gives order to the chaos of experience. To act in accordance with the logos is \"\n",
    "    \"to speak the truth, to pursue what is meaningful rather than what is expedient, and \"\n",
    "    \"to take on the burden of Being itself with courage and humility.\",\n",
    "\n",
    "    # From 12 Rules\n",
    "    \"Compare yourself to who you were yesterday, not to who someone else is today. \"\n",
    "    \"You have a nature. You can play the game of life and improve. You can set a \"\n",
    "    \"standard, even a minimal standard, and try to live up to it. You can improve \"\n",
    "    \"incrementally, moving forward step by step. You can judge your life against \"\n",
    "    \"what you know to be good, against what you should be.\",\n",
    "\n",
    "    # From Maps of Meaning\n",
    "    \"The great myths and rituals of the past have been formulated in the language of \"\n",
    "    \"the imagination. They say: act out the role of the hero; do not be the villain; \"\n",
    "    \"do not be the tyrant. They say: update your maps of meaning when new information \"\n",
    "    \"warrants it; admit your errors and change. They say: encounter the stranger and \"\n",
    "    \"extract from that encounter what is valuable. Treat the stranger with respect.\",\n",
    "\n",
    "    # From Beyond Order\n",
    "    \"Meaning is the ultimate balance between, on the one hand, the chaos of transformation \"\n",
    "    \"and possibility and, on the other, the discipline of pristine order, whose purpose is \"\n",
    "    \"to produce out of the attendant chaos a new order that will be even more productive \"\n",
    "    \"and worthwhile than the old. Pursue what is meaningful, not what is expedient.\",\n",
    "\n",
    "    # From We Who Wrestle with God\n",
    "    \"Suffering is not a mistake or an accident. It is the very ground of Being itself. \"\n",
    "    \"To wrestle with God, as Jacob did, is to confront that suffering honestly, to take \"\n",
    "    \"responsibility for it, and to find within it the possibility of transcendence. The \"\n",
    "    \"hero does not flee from the dragon; he faces it and transforms the encounter.\",\n",
    "]\n",
    "\n",
    "# ── 2b: Evaluation prompts ─────────────────────────────────────────────────\n",
    "# Covers all four books' major themes.\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    \"Why is personal responsibility the foundation of a meaningful life?\",\n",
    "    \"How do ancient myths and stories reveal truths about human nature?\",\n",
    "    \"What does it mean to pursue what is meaningful rather than what is expedient?\",\n",
    "    \"How should a person confront suffering rather than flee from it?\",\n",
    "    \"What is the significance of the hero archetype in understanding the human psyche?\",\n",
    "    \"Why is telling the truth essential to a properly functioning life?\",\n",
    "    \"What is the role of the divine or the sacred in organizing human society?\",\n",
    "    \"How does the Jungian concept of the shadow relate to individual development?\",\n",
    "    \"What does it mean to stand up straight with your shoulders back?\",\n",
    "]\n",
    "\n",
    "# ── 2c: Peterson keyword dictionary ────────────────────────────────────────\n",
    "# Carefully curated from the four books. These terms are either exclusive to\n",
    "# Peterson or far more frequent in his writing than in general LLM output.\n",
    "PETERSON_KEYWORDS = [\n",
    "    # Core metaphysical concepts\n",
    "    \"chaos\", \"order\", \"logos\", \"being\", \"meaning\", \"meaningful\", \"meaningless\",\n",
    "    \"transcendence\", \"transcendent\", \"archetype\", \"archetypal\",\n",
    "    # Psychological concepts (Jungian)\n",
    "    \"shadow\", \"anima\", \"animus\", \"unconscious\", \"consciousness\", \"psyche\",\n",
    "    \"individuation\", \"projection\",\n",
    "    # Ethical / existential\n",
    "    \"responsibility\", \"suffering\", \"redemption\", \"courage\", \"virtue\",\n",
    "    \"nihilism\", \"nihilistic\", \"expedient\", \"expedience\", \"tyranny\", \"tyrannical\",\n",
    "    \"sovereignty\", \"sovereignty\", \"heroic\", \"malevolent\",\n",
    "    # Narrative / mythological\n",
    "    \"myth\", \"mythological\", \"hero\", \"dragon\", \"narrative\", \"story\",\n",
    "    \"ritual\", \"sacrifice\", \"resurrection\", \"transformation\",\n",
    "    # Religious / biblical\n",
    "    \"divine\", \"sacred\", \"god\", \"biblical\", \"genesis\", \"logos\", \"spirit\",\n",
    "    \"wrestle\", \"jacob\", \"adam\", \"eve\", \"serpent\",\n",
    "    # Peterson's characteristic action-language\n",
    "    \"confront\", \"hierarchy\", \"dominance\", \"voluntarily\", \"catastrophe\",\n",
    "    \"pathological\", \"resentment\", \"ideological\", \"totalitarian\",\n",
    "]\n",
    "PETERSON_KEYWORDS = list(set(PETERSON_KEYWORDS))  # deduplicate\n",
    "\n",
    "print(f\"Reference passages  : {len(PETERSON_PASSAGES)}\")\n",
    "print(f\"Evaluation prompts  : {len(EVAL_PROMPTS)}\")\n",
    "print(f\"Keyword dictionary  : {len(PETERSON_KEYWORDS)} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Helper Functions\n",
    "\n",
    "All the metric computation logic lives here. This keeps the model evaluation cells clean and easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_perplexity(model, tokenizer, texts: list[str], max_length: int = 512) -> list[float]:\n    \"\"\"\n    Compute perplexity of a language model on each text in `texts`.\n\n    Perplexity = exp(average negative log-likelihood per token).\n    A lower perplexity means the model is less surprised by the text —\n    i.e., it predicts the next token well — indicating the text is more\n    consistent with what the model learned during training.\n\n    For a fine-tuned Peterson model, we expect LOWER perplexity on\n    Peterson passages than the base model, because it has learned\n    his vocabulary, sentence structure, and thematic patterns.\n    \"\"\"\n    model.eval()\n    perplexities = []\n    with torch.no_grad():\n        for text in texts:\n            enc = tokenizer(\n                text,\n                return_tensors=\"pt\",\n                max_length=max_length,\n                truncation=True,\n            ).to(\"cuda\")\n            # labels = input_ids causes the model to compute cross-entropy loss\n            # over all token positions (standard language model objective)\n            out = model(**enc, labels=enc[\"input_ids\"])\n            ppl = math.exp(out.loss.item())\n            perplexities.append(ppl)\n    return perplexities\n\n\ndef generate_response(model, tokenizer, prompt: str,\n                      system_prompt: str, max_new_tokens: int = 300) -> str:\n    \"\"\"\n    Generate a response from the model for a given prompt.\n\n    Uses greedy decoding (temperature=1, no sampling) for deterministic,\n    reproducible outputs that make comparison fair between models.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\",   \"content\": prompt},\n    ]\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        return_tensors=\"pt\",\n        return_dict=True,\n        reasoning_effort=\"low\",   # keeps responses concise and comparable\n    ).to(\"cuda\")\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,         # greedy — fully deterministic\n            temperature=1.0,\n            repetition_penalty=1.1,\n        )\n\n    # Decode only the newly generated tokens (skip the prompt)\n    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n\n\ndef compute_text_stats(texts: list[str]) -> dict:\n    \"\"\"\n    Compute a suite of text statistics over a list of responses.\n\n    Every text in `texts` contributes exactly one entry to each output list,\n    even if the text is empty (in which case counts are 0). This ensures\n    the output lists are always the same length as the input, which is\n    required for per-prompt plots.\n\n    Returns a dict with:\n    - word_counts       : words per response\n    - sentence_counts   : sentences per response\n    - ttr_values        : Type-Token Ratio per response (unique / total words)\n    - keyword_density   : fraction of words that are Peterson keywords\n    - keyword_counts    : per-keyword frequency dict\n    - all_words         : flat list of all lowercased words (for word clouds)\n    \"\"\"\n    stop_words = set(stopwords.words('english'))\n    kw_set = set(k.lower() for k in PETERSON_KEYWORDS)\n\n    word_counts, sentence_counts, ttr_values = [], [], []\n    keyword_density = []\n    keyword_counts = Counter()\n    all_words = []\n\n    for text in texts:\n        # Always process every text — empty texts produce zero counts\n        if text.strip():\n            words = word_tokenize(text.lower())\n            sents = sent_tokenize(text)\n        else:\n            words = []\n            sents = []\n\n        words_alpha = [w for w in words if w.isalpha()]\n\n        word_counts.append(len(words_alpha))\n        sentence_counts.append(len(sents))\n\n        # Type-Token Ratio: fraction of words that are unique\n        # A higher TTR suggests richer, more varied vocabulary\n        ttr = len(set(words_alpha)) / max(len(words_alpha), 1)\n        ttr_values.append(ttr)\n\n        # Peterson keyword density\n        kw_hits = [w for w in words_alpha if w in kw_set]\n        keyword_density.append(len(kw_hits) / max(len(words_alpha), 1))\n        keyword_counts.update(kw_hits)\n\n        # Content words for word cloud (exclude stop words)\n        content_words = [w for w in words_alpha if w not in stop_words and len(w) > 2]\n        all_words.extend(content_words)\n\n    return {\n        \"word_counts\":     word_counts,\n        \"sentence_counts\": sentence_counts,\n        \"ttr_values\":      ttr_values,\n        \"keyword_density\": keyword_density,\n        \"keyword_counts\":  keyword_counts,\n        \"all_words\":       all_words,\n    }\n\n\ndef compute_tfidf_similarity(responses: list[str],\n                              references: list[str]) -> list[float]:\n    \"\"\"\n    Measure how similar each model response is to Peterson's actual writing\n    using TF-IDF cosine similarity.\n\n    TF-IDF (Term Frequency-Inverse Document Frequency) converts texts into\n    vectors where each dimension represents a word, weighted by how\n    distinctive it is. Cosine similarity then measures the angle between\n    two vectors — 1.0 = identical vocabulary mix, 0.0 = no overlap.\n\n    For each response, we compute its maximum similarity to any of the\n    reference Peterson passages, then return the average across all responses.\n    Empty responses get a similarity of 0.0.\n    \"\"\"\n    if not responses or not any(r.strip() for r in responses):\n        return [0.0] * len(responses)\n\n    all_texts = references + responses\n    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n    tfidf_matrix = vectorizer.fit_transform(all_texts)\n\n    ref_vecs  = tfidf_matrix[:len(references)]\n    resp_vecs = tfidf_matrix[len(references):]\n\n    similarities = []\n    for i in range(resp_vecs.shape[0]):\n        sims = cosine_similarity(resp_vecs[i], ref_vecs)[0]\n        similarities.append(float(sims.max()))  # best match to any reference\n    return similarities\n\n\nprint(\"Helper functions defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Evaluate the Base Model\n",
    "\n",
    "We load the base model, collect all metrics, save them to disk, then fully unload it from GPU memory before loading the fine-tuned model. This ensures clean memory for the second evaluation.\n",
    "\n",
    "**System prompt for base model**: A generic helpful-assistant prompt. The fine-tuned model will use the same Peterson-expert system prompt it was trained on — this asymmetry is intentional because it reflects how each model would realistically be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\n\nBASE_SYSTEM_PROMPT = \"You are a helpful assistant.\"\n\nTUNED_SYSTEM_PROMPT = (\n    \"You are an AI assistant that has been trained on the complete works of Jordan B. Peterson, \"\n    \"a Canadian clinical psychologist, professor, and author. You speak with deep knowledge of \"\n    \"psychology, philosophy, mythology, religion, and personal responsibility. Your responses \"\n    \"reflect Peterson's writing style, intellectual depth, and interdisciplinary approach to \"\n    \"understanding human nature and meaning.\"\n)\n\n# ── Cache check: skip base model inference if already computed ─────────────\n_base_cache_exists = (CACHE_DIR / \"base_results.pkl\").exists()\n\nif _base_cache_exists:\n    print(\"Base model results cache found — skipping inference.\")\n    print(f\"  (Delete {CACHE_DIR/'base_results.pkl'} to force re-run)\")\nelse:\n    print(\"Loading BASE model...\")\n    base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n        model_name    = BASE_MODEL_NAME,\n        dtype         = None,\n        max_seq_length= 2048,\n        load_in_4bit  = True,\n        full_finetuning = False,\n    )\n    FastLanguageModel.for_inference(base_model)\n    print(f\"Base model loaded. VRAM used: {torch.cuda.memory_reserved()/1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── 4a: Perplexity on Peterson passages ────────────────────────────────────\nif not _base_cache_exists:\n    print(\"Computing base model perplexity on Peterson passages...\")\n    base_perplexities = compute_perplexity(base_model, base_tokenizer, PETERSON_PASSAGES)\n    for i, (txt, ppl) in enumerate(zip(PETERSON_PASSAGES, base_perplexities)):\n        print(f\"  Passage {i+1}: PPL = {ppl:.2f}  |  '{txt[:60]}...'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── 4b: Generate responses to all evaluation prompts ──────────────────────\nif not _base_cache_exists:\n    print(\"Generating base model responses (this takes a few minutes)...\\n\")\n    base_responses = []\n    for i, prompt in enumerate(EVAL_PROMPTS):\n        print(f\"  [{i+1}/{len(EVAL_PROMPTS)}] {prompt[:70]}\")\n        resp = generate_response(base_model, base_tokenizer, prompt, BASE_SYSTEM_PROMPT)\n        base_responses.append(resp)\n        print(f\"         → {resp[:100]}...\\n\")\n\n    print(f\"Done. {len(base_responses)} responses collected.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── 4c: Save all base model results ───────────────────────────────────────\nif not _base_cache_exists:\n    base_results = {\n        \"perplexities\": base_perplexities,\n        \"responses\":    base_responses,\n    }\n    with open(CACHE_DIR / \"base_results.pkl\", \"wb\") as f:\n        pickle.dump(base_results, f)\n\n    print(\"Base model results saved.\")\n    print(f\"  Avg perplexity : {sum(base_perplexities)/len(base_perplexities):.2f}\")\n    print(f\"  Total responses: {len(base_responses)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── 4d: Unload base model from GPU ─────────────────────────────────────────\n# We delete all references to the model and call the garbage collector,\n# then explicitly clear the CUDA memory cache. Without this, loading a\n# second 20B model would cause an out-of-memory error.\nif not _base_cache_exists:\n    del base_model, base_tokenizer\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    vram_free = (torch.cuda.get_device_properties(0).total_memory\n                 - torch.cuda.memory_reserved()) / 1e9\n    print(f\"Base model unloaded. VRAM free: {vram_free:.1f} GB\")\nelse:\n    print(\"Base model was not loaded (used cache) — no cleanup needed.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Evaluate the Fine-Tuned Model\n",
    "\n",
    "Now we load the fine-tuned model. This loads the same base weights as before, but with the LoRA adapter applied on top. The adapter changes how the model represents and generates text for Peterson-style prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cache check: skip fine-tuned model inference if already computed ────────\n_tuned_cache_exists = (CACHE_DIR / \"tuned_results.pkl\").exists()\n\nif _tuned_cache_exists:\n    print(\"Fine-tuned model results cache found — skipping inference.\")\n    print(f\"  (Delete {CACHE_DIR/'tuned_results.pkl'} to force re-run)\")\nelse:\n    print(\"Loading FINE-TUNED model (base + LoRA adapters)...\")\n    tuned_model, tuned_tokenizer = FastLanguageModel.from_pretrained(\n        model_name    = LORA_MODEL_PATH,\n        dtype         = None,\n        max_seq_length= 2048,\n        load_in_4bit  = True,\n        full_finetuning = False,\n    )\n    FastLanguageModel.for_inference(tuned_model)\n    print(f\"Fine-tuned model loaded. VRAM used: {torch.cuda.memory_reserved()/1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── 5a: Perplexity on Peterson passages ────────────────────────────────────\nif not _tuned_cache_exists:\n    print(\"Computing fine-tuned model perplexity on Peterson passages...\")\n    tuned_perplexities = compute_perplexity(tuned_model, tuned_tokenizer, PETERSON_PASSAGES)\n    for i, (txt, ppl) in enumerate(zip(PETERSON_PASSAGES, tuned_perplexities)):\n        print(f\"  Passage {i+1}: PPL = {ppl:.2f}  |  '{txt[:60]}...'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── 5b: Generate responses to all evaluation prompts ──────────────────────\nif not _tuned_cache_exists:\n    print(\"Generating fine-tuned model responses...\\n\")\n    tuned_responses = []\n    for i, prompt in enumerate(EVAL_PROMPTS):\n        print(f\"  [{i+1}/{len(EVAL_PROMPTS)}] {prompt[:70]}\")\n        resp = generate_response(tuned_model, tuned_tokenizer, prompt, TUNED_SYSTEM_PROMPT)\n        tuned_responses.append(resp)\n        print(f\"         → {resp[:100]}...\\n\")\n\n    print(f\"Done. {len(tuned_responses)} responses collected.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── 5c: Save all fine-tuned results ───────────────────────────────────────\nif not _tuned_cache_exists:\n    tuned_results = {\n        \"perplexities\": tuned_perplexities,\n        \"responses\":    tuned_responses,\n    }\n    with open(CACHE_DIR / \"tuned_results.pkl\", \"wb\") as f:\n        pickle.dump(tuned_results, f)\n\n    print(\"Fine-tuned results saved.\")\n    print(f\"  Avg perplexity : {sum(tuned_perplexities)/len(tuned_perplexities):.2f}\")\n    print(f\"  Total responses: {len(tuned_responses)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── 5d: Unload fine-tuned model ────────────────────────────────────────────\nif not _tuned_cache_exists:\n    del tuned_model, tuned_tokenizer\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"Fine-tuned model unloaded. Beginning analysis...\")\nelse:\n    print(\"Fine-tuned model was not loaded (used cache) — no cleanup needed. Beginning analysis...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Compute All Derived Metrics\n",
    "\n",
    "With raw outputs collected from both models, we now compute all the derived metrics that feed into the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload from cache (allows re-running analysis cells without re-running inference)\n",
    "with open(CACHE_DIR / \"base_results.pkl\",  \"rb\") as f: base_results  = pickle.load(f)\n",
    "with open(CACHE_DIR / \"tuned_results.pkl\", \"rb\") as f: tuned_results = pickle.load(f)\n",
    "\n",
    "base_perplexities  = base_results[\"perplexities\"]\n",
    "tuned_perplexities = tuned_results[\"perplexities\"]\n",
    "base_responses     = base_results[\"responses\"]\n",
    "tuned_responses    = tuned_results[\"responses\"]\n",
    "\n",
    "# ── Text statistics ────────────────────────────────────────────────────────\n",
    "print(\"Computing text statistics...\")\n",
    "base_stats  = compute_text_stats(base_responses)\n",
    "tuned_stats = compute_text_stats(tuned_responses)\n",
    "\n",
    "# ── TF-IDF similarity to Peterson's actual writing ─────────────────────────\n",
    "print(\"Computing TF-IDF similarity to Peterson passages...\")\n",
    "base_similarities  = compute_tfidf_similarity(base_responses,  PETERSON_PASSAGES)\n",
    "tuned_similarities = compute_tfidf_similarity(tuned_responses, PETERSON_PASSAGES)\n",
    "\n",
    "# ── Summary table ─────────────────────────────────────────────────────────\n",
    "summary = {\n",
    "    \"Metric\": [\n",
    "        \"Avg Perplexity (↓ better)\",\n",
    "        \"Avg TF-IDF Similarity to Peterson (↑ better)\",\n",
    "        \"Avg Keyword Density % (↑ better)\",\n",
    "        \"Avg Type-Token Ratio (↑ better)\",\n",
    "        \"Avg Response Length (words)\",\n",
    "    ],\n",
    "    \"Base Model\": [\n",
    "        f\"{sum(base_perplexities)/len(base_perplexities):.2f}\",\n",
    "        f\"{sum(base_similarities)/len(base_similarities):.4f}\",\n",
    "        f\"{100*sum(base_stats['keyword_density'])/len(base_stats['keyword_density']):.2f}%\",\n",
    "        f\"{sum(base_stats['ttr_values'])/len(base_stats['ttr_values']):.4f}\",\n",
    "        f\"{sum(base_stats['word_counts'])/len(base_stats['word_counts']):.1f}\",\n",
    "    ],\n",
    "    \"Fine-Tuned Model\": [\n",
    "        f\"{sum(tuned_perplexities)/len(tuned_perplexities):.2f}\",\n",
    "        f\"{sum(tuned_similarities)/len(tuned_similarities):.4f}\",\n",
    "        f\"{100*sum(tuned_stats['keyword_density'])/len(tuned_stats['keyword_density']):.2f}%\",\n",
    "        f\"{sum(tuned_stats['ttr_values'])/len(tuned_stats['ttr_values']):.4f}\",\n",
    "        f\"{sum(tuned_stats['word_counts'])/len(tuned_stats['word_counts']):.1f}\",\n",
    "    ],\n",
    "}\n",
    "df_summary = pd.DataFrame(summary)\n",
    "print(\"\\n\" + df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Visualization — Perplexity\n",
    "\n",
    "**Perplexity** measures how surprised the model is by a given text. It is defined as:\n",
    "\n",
    "$$\\text{PPL}(\\text{text}) = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(w_i | w_1, \\ldots, w_{i-1})\\right)$$\n",
    "\n",
    "Intuitively, a perplexity of 50 means the model is as confused as if it had to randomly choose from 50 equally likely words at each step. A fine-tuned model that has learned Peterson's style should assign higher probability to his actual words, producing **lower perplexity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(\"Perplexity on Jordan Peterson Reference Passages\",\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "passage_labels = [f\"Passage {i+1}\" for i in range(len(PETERSON_PASSAGES))]\n",
    "x = np.arange(len(passage_labels))\n",
    "w = 0.35\n",
    "\n",
    "# ── Left: grouped bar chart per passage ───────────────────────────────────\n",
    "ax = axes[0]\n",
    "bars_b = ax.bar(x - w/2, base_perplexities,  w, label=\"Base Model\",        color=BASE_COLOR,  alpha=0.85)\n",
    "bars_t = ax.bar(x + w/2, tuned_perplexities, w, label=\"Fine-Tuned Model\",  color=TUNED_COLOR, alpha=0.85)\n",
    "ax.set_xlabel(\"Peterson Passage\")\n",
    "ax.set_ylabel(\"Perplexity  (lower = better)\")\n",
    "ax.set_title(\"Per-Passage Perplexity\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(passage_labels, rotation=30, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Annotate bars with values\n",
    "for bar in bars_b:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=8, color=BASE_COLOR)\n",
    "for bar in bars_t:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=8, color=TUNED_COLOR)\n",
    "\n",
    "# ── Right: average with improvement arrow ─────────────────────────────────\n",
    "ax2 = axes[1]\n",
    "avg_base  = sum(base_perplexities) / len(base_perplexities)\n",
    "avg_tuned = sum(tuned_perplexities) / len(tuned_perplexities)\n",
    "models = [\"Base Model\", \"Fine-Tuned Model\"]\n",
    "avgs   = [avg_base, avg_tuned]\n",
    "colors = [BASE_COLOR, TUNED_COLOR]\n",
    "bars   = ax2.bar(models, avgs, color=colors, alpha=0.85, width=0.45)\n",
    "ax2.set_ylabel(\"Average Perplexity  (lower = better)\")\n",
    "ax2.set_title(\"Average Perplexity Across All Passages\")\n",
    "for bar, val in zip(bars, avgs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f\"{val:.2f}\", ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "# Improvement annotation\n",
    "pct_change = 100 * (avg_base - avg_tuned) / avg_base\n",
    "ax2.annotate(\n",
    "    f\"{pct_change:+.1f}%\\nimprovement\",\n",
    "    xy=(1, avg_tuned), xytext=(0.5, (avg_base + avg_tuned) / 2),\n",
    "    fontsize=10, ha='center', color='green' if pct_change > 0 else 'red',\n",
    "    fontweight='bold',\n",
    "    arrowprops=dict(arrowstyle='->', color='green' if pct_change > 0 else 'red', lw=1.5),\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"01_perplexity.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(f\"Average perplexity — Base: {avg_base:.2f}  |  Fine-tuned: {avg_tuned:.2f}  |  Change: {pct_change:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Visualization — TF-IDF Semantic Similarity to Peterson's Writing\n",
    "\n",
    "**TF-IDF cosine similarity** measures how similar each model's response vocabulary is to Peterson's actual writing. We build a TF-IDF matrix from all responses and reference passages combined, then compute the cosine angle between each response vector and each reference passage vector.\n",
    "\n",
    "A score of 1.0 = identical vocabulary mix; 0.0 = completely different. This captures both *which* words are used and *how distinctively* they are used (rare words get higher TF-IDF weight than common ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(\"TF-IDF Semantic Similarity to Peterson's Actual Writing\",\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "prompt_labels = [f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))]\n",
    "x = np.arange(len(prompt_labels))\n",
    "\n",
    "# ── Left: per-prompt similarity ───────────────────────────────────────────\n",
    "ax = axes[0]\n",
    "ax.plot(prompt_labels, base_similarities,  'o-', color=BASE_COLOR,  label=\"Base Model\",       linewidth=2, markersize=7)\n",
    "ax.plot(prompt_labels, tuned_similarities, 's-', color=TUNED_COLOR, label=\"Fine-Tuned Model\", linewidth=2, markersize=7)\n",
    "ax.fill_between(range(len(prompt_labels)), base_similarities,  alpha=0.15, color=BASE_COLOR)\n",
    "ax.fill_between(range(len(prompt_labels)), tuned_similarities, alpha=0.15, color=TUNED_COLOR)\n",
    "ax.set_xticks(range(len(prompt_labels)))\n",
    "ax.set_xticklabels(prompt_labels)\n",
    "ax.set_xlabel(\"Evaluation Prompt\")\n",
    "ax.set_ylabel(\"Cosine Similarity  (higher = more Peterson-like)\")\n",
    "ax.set_title(\"Per-Prompt Similarity to Peterson Passages\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "# Add prompt text as tooltip-style annotations\n",
    "for i, (b, t) in enumerate(zip(base_similarities, tuned_similarities)):\n",
    "    ax.annotate(f\"{b:.2f}\", (i, b), textcoords=\"offset points\", xytext=(0, 8),\n",
    "                fontsize=7, ha='center', color=BASE_COLOR)\n",
    "    ax.annotate(f\"{t:.2f}\", (i, t), textcoords=\"offset points\", xytext=(0, -14),\n",
    "                fontsize=7, ha='center', color=TUNED_COLOR)\n",
    "\n",
    "# ── Right: box plot comparing distributions ───────────────────────────────\n",
    "ax2 = axes[1]\n",
    "bp = ax2.boxplot(\n",
    "    [base_similarities, tuned_similarities],\n",
    "    labels=[\"Base Model\", \"Fine-Tuned\"],\n",
    "    patch_artist=True,\n",
    "    medianprops=dict(color='black', linewidth=2),\n",
    ")\n",
    "bp['boxes'][0].set_facecolor(BASE_COLOR);  bp['boxes'][0].set_alpha(0.7)\n",
    "bp['boxes'][1].set_facecolor(TUNED_COLOR); bp['boxes'][1].set_alpha(0.7)\n",
    "ax2.set_ylabel(\"Cosine Similarity  (higher = more Peterson-like)\")\n",
    "ax2.set_title(\"Distribution of Similarities Across All Prompts\")\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"02_tfidf_similarity.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "avg_base_sim  = sum(base_similarities)  / len(base_similarities)\n",
    "avg_tuned_sim = sum(tuned_similarities) / len(tuned_similarities)\n",
    "print(f\"Avg similarity — Base: {avg_base_sim:.4f}  |  Fine-tuned: {avg_tuned_sim:.4f}  |  Change: {100*(avg_tuned_sim-avg_base_sim)/max(avg_base_sim,1e-6):+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Visualization — Peterson Keyword Density\n",
    "\n",
    "**Keyword density** measures the fraction of words in each response that belong to Peterson's characteristic vocabulary (our ~60-term dictionary). This directly captures domain adaptation — a model that has learned Peterson's content should use his distinctive vocabulary more often.\n",
    "\n",
    "We also look at *which specific keywords* each model uses, to see if the fine-tuned model not only uses more keywords but uses the *right* ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle(\"Peterson Keyword Density in Model Responses\",\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "# ── Left: per-prompt keyword density ─────────────────────────────────────\n",
    "ax = axes[0]\n",
    "base_kd_pct  = [v * 100 for v in base_stats['keyword_density']]\n",
    "tuned_kd_pct = [v * 100 for v in tuned_stats['keyword_density']]\n",
    "prompt_short = [f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))]\n",
    "x = np.arange(len(prompt_short))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, base_kd_pct,  w, label=\"Base Model\",       color=BASE_COLOR,  alpha=0.85)\n",
    "ax.bar(x + w/2, tuned_kd_pct, w, label=\"Fine-Tuned Model\", color=TUNED_COLOR, alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(prompt_short)\n",
    "ax.set_xlabel(\"Evaluation Prompt\")\n",
    "ax.set_ylabel(\"Keyword Density (% of words)\")\n",
    "ax.set_title(\"Fraction of Response Words in Peterson's Vocabulary\")\n",
    "ax.legend()\n",
    "\n",
    "# ── Right: top 15 specific keywords used ─────────────────────────────────\n",
    "ax2 = axes[1]\n",
    "# Combine keywords from both models; show top 15 by total usage\n",
    "all_kw = set(base_stats['keyword_counts'].keys()) | set(tuned_stats['keyword_counts'].keys())\n",
    "top_kw = sorted(all_kw,\n",
    "                key=lambda k: base_stats['keyword_counts'].get(k, 0) +\n",
    "                              tuned_stats['keyword_counts'].get(k, 0),\n",
    "                reverse=True)[:15]\n",
    "base_kw_counts  = [base_stats['keyword_counts'].get(k, 0)  for k in top_kw]\n",
    "tuned_kw_counts = [tuned_stats['keyword_counts'].get(k, 0) for k in top_kw]\n",
    "y = np.arange(len(top_kw))\n",
    "ax2.barh(y + 0.2, base_kw_counts,  0.4, label=\"Base Model\",       color=BASE_COLOR,  alpha=0.85)\n",
    "ax2.barh(y - 0.2, tuned_kw_counts, 0.4, label=\"Fine-Tuned Model\", color=TUNED_COLOR, alpha=0.85)\n",
    "ax2.set_yticks(y)\n",
    "ax2.set_yticklabels(top_kw)\n",
    "ax2.set_xlabel(\"Total Uses Across All Responses\")\n",
    "ax2.set_title(\"Top Peterson Keywords Used\")\n",
    "ax2.legend()\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"03_keyword_density.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "avg_base_kd  = 100 * sum(base_stats['keyword_density'])  / max(len(base_stats['keyword_density']),  1)\n",
    "avg_tuned_kd = 100 * sum(tuned_stats['keyword_density']) / max(len(tuned_stats['keyword_density']), 1)\n",
    "print(f\"Avg keyword density — Base: {avg_base_kd:.2f}%  |  Fine-tuned: {avg_tuned_kd:.2f}%  |  Change: {avg_tuned_kd-avg_base_kd:+.2f}pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Visualization — Response Characteristics\n",
    "\n",
    "**Type-Token Ratio (TTR)** measures vocabulary richness: the fraction of words in a response that are unique. A TTR of 0.5 means half the words used are unique; a TTR of 1.0 means every word appears only once. Peterson's writing is known for its rich, varied vocabulary, so a fine-tuned model should exhibit higher TTR.\n",
    "\n",
    "**Response length** reflects how verbose the model is. Peterson is a prolific writer who favors long, elaborate explanations — a fine-tuned model should produce longer responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Response Characteristics: Vocabulary Richness & Length\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "prompt_labels = [f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))]\n",
    "\n",
    "# ── Top-left: TTR per prompt ───────────────────────────────────────────────\n",
    "ax = axes[0, 0]\n",
    "ax.plot(prompt_labels, base_stats['ttr_values'],  'o-', color=BASE_COLOR,  label=\"Base Model\",       lw=2, ms=7)\n",
    "ax.plot(prompt_labels, tuned_stats['ttr_values'], 's-', color=TUNED_COLOR, label=\"Fine-Tuned Model\", lw=2, ms=7)\n",
    "ax.set_ylabel(\"Type-Token Ratio\")\n",
    "ax.set_title(\"Vocabulary Richness (TTR) per Prompt\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Prompt\")\n",
    "\n",
    "# ── Top-right: TTR box plot ────────────────────────────────────────────────\n",
    "ax2 = axes[0, 1]\n",
    "bp = ax2.boxplot(\n",
    "    [base_stats['ttr_values'], tuned_stats['ttr_values']],\n",
    "    labels=[\"Base Model\", \"Fine-Tuned\"],\n",
    "    patch_artist=True,\n",
    "    medianprops=dict(color='black', lw=2),\n",
    ")\n",
    "bp['boxes'][0].set_facecolor(BASE_COLOR);  bp['boxes'][0].set_alpha(0.7)\n",
    "bp['boxes'][1].set_facecolor(TUNED_COLOR); bp['boxes'][1].set_alpha(0.7)\n",
    "ax2.set_ylabel(\"Type-Token Ratio\")\n",
    "ax2.set_title(\"TTR Distribution\")\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# ── Bottom-left: word count per prompt ────────────────────────────────────\n",
    "ax3 = axes[1, 0]\n",
    "x = np.arange(len(prompt_labels))\n",
    "w = 0.35\n",
    "ax3.bar(x - w/2, base_stats['word_counts'],  w, label=\"Base Model\",       color=BASE_COLOR,  alpha=0.85)\n",
    "ax3.bar(x + w/2, tuned_stats['word_counts'], w, label=\"Fine-Tuned Model\", color=TUNED_COLOR, alpha=0.85)\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(prompt_labels)\n",
    "ax3.set_xlabel(\"Prompt\")\n",
    "ax3.set_ylabel(\"Words in Response\")\n",
    "ax3.set_title(\"Response Length (Word Count) per Prompt\")\n",
    "ax3.legend()\n",
    "\n",
    "# ── Bottom-right: sentence count box plot ─────────────────────────────────\n",
    "ax4 = axes[1, 1]\n",
    "bp2 = ax4.boxplot(\n",
    "    [base_stats['word_counts'], tuned_stats['word_counts']],\n",
    "    labels=[\"Base Model\", \"Fine-Tuned\"],\n",
    "    patch_artist=True,\n",
    "    medianprops=dict(color='black', lw=2),\n",
    ")\n",
    "bp2['boxes'][0].set_facecolor(BASE_COLOR);  bp2['boxes'][0].set_alpha(0.7)\n",
    "bp2['boxes'][1].set_facecolor(TUNED_COLOR); bp2['boxes'][1].set_alpha(0.7)\n",
    "ax4.set_ylabel(\"Words in Response\")\n",
    "ax4.set_title(\"Response Length Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"04_response_characteristics.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "avg_base_ttr   = sum(base_stats['ttr_values'])  / len(base_stats['ttr_values'])\n",
    "avg_tuned_ttr  = sum(tuned_stats['ttr_values']) / len(tuned_stats['ttr_values'])\n",
    "avg_base_words  = sum(base_stats['word_counts'])  / len(base_stats['word_counts'])\n",
    "avg_tuned_words = sum(tuned_stats['word_counts']) / len(tuned_stats['word_counts'])\n",
    "print(f\"Avg TTR   — Base: {avg_base_ttr:.4f}  |  Fine-tuned: {avg_tuned_ttr:.4f}  |  Change: {avg_tuned_ttr-avg_base_ttr:+.4f}\")\n",
    "print(f\"Avg words — Base: {avg_base_words:.1f}  |  Fine-tuned: {avg_tuned_words:.1f}  |  Change: {avg_tuned_words-avg_base_words:+.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Visualization — Word Clouds\n",
    "\n",
    "Word clouds give an immediate, at-a-glance visual of each model's vocabulary. The size of each word is proportional to how frequently it appears across all responses (after removing common English stop words).\n",
    "\n",
    "What to look for:\n",
    "- The **base model** should show generic words like *\"people\", \"think\", \"important\", \"way\"*\n",
    "- The **fine-tuned model** should prominently feature Peterson's vocabulary: *\"meaning\", \"chaos\", \"order\", \"responsibility\", \"suffering\", \"hero\", \"myth\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_wc = set(STOPWORDS) | {'also', 'one', 'may', 'much', 'even',\n",
    "                                   'way', 'well', 'get', 'make', 'like',\n",
    "                                   'us', 'would', 'could', 'time', 'thing',\n",
    "                                   'things', 'many', 'something', 'often'}\n",
    "\n",
    "def make_wordcloud(words: list[str], color: str, title: str, ax):\n",
    "    freq = Counter(w for w in words if w.lower() not in stop_words_wc and len(w) > 2)\n",
    "    if not freq:\n",
    "        ax.text(0.5, 0.5, \"(no content words)\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(title)\n",
    "        return\n",
    "    wc = WordCloud(\n",
    "        width=800, height=450,\n",
    "        background_color='white',\n",
    "        colormap='Blues' if 'Base' in title else 'Oranges',\n",
    "        max_words=80,\n",
    "        prefer_horizontal=0.8,\n",
    "        stopwords=stop_words_wc,\n",
    "        min_font_size=8,\n",
    "    ).generate_from_frequencies(freq)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold', pad=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Word Clouds: Most Frequent Content Words in Responses\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "make_wordcloud(base_stats['all_words'],  BASE_COLOR,  \"Base Model\",        axes[0])\n",
    "make_wordcloud(tuned_stats['all_words'], TUNED_COLOR, \"Fine-Tuned Model\",  axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"05_wordclouds.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Visualization — Keyword Heatmap\n",
    "\n",
    "This heatmap shows, for each evaluation prompt, how many Peterson keywords appeared in that model's response. Warmer colors (orange/red) indicate more keyword usage — a sign the model is engaging with Peterson's conceptual vocabulary rather than generic language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-prompt keyword counts for each keyword\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "kw_set = set(k.lower() for k in PETERSON_KEYWORDS)\n",
    "\n",
    "def per_prompt_keyword_matrix(responses, keywords):\n",
    "    \"\"\"Returns (n_responses x n_keywords) matrix of keyword counts.\"\"\"\n",
    "    matrix = []\n",
    "    for resp in responses:\n",
    "        words = word_tokenize(resp.lower())\n",
    "        words_alpha = [w for w in words if w.isalpha()]\n",
    "        counts = [words_alpha.count(kw) for kw in keywords]\n",
    "        matrix.append(counts)\n",
    "    return np.array(matrix)\n",
    "\n",
    "# Select top 20 keywords by total usage across both models\n",
    "all_kw_counter = Counter()\n",
    "for r in base_responses + tuned_responses:\n",
    "    words = word_tokenize(r.lower())\n",
    "    for w in words:\n",
    "        if w in kw_set:\n",
    "            all_kw_counter[w] += 1\n",
    "top20_kw = [kw for kw, _ in all_kw_counter.most_common(20)]\n",
    "\n",
    "if top20_kw:\n",
    "    base_matrix  = per_prompt_keyword_matrix(base_responses,  top20_kw)\n",
    "    tuned_matrix = per_prompt_keyword_matrix(tuned_responses, top20_kw)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "    fig.suptitle(\"Peterson Keyword Usage per Prompt (Heatmap)\",\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    prompt_labels_short = [f\"Q{i+1}: {EVAL_PROMPTS[i][:30]}...\" for i in range(len(EVAL_PROMPTS))]\n",
    "\n",
    "    vmax = max(base_matrix.max(), tuned_matrix.max(), 1)\n",
    "\n",
    "    for ax, matrix, title in [\n",
    "        (axes[0], base_matrix,  \"Base Model\"),\n",
    "        (axes[1], tuned_matrix, \"Fine-Tuned Model\"),\n",
    "    ]:\n",
    "        im = ax.imshow(matrix, aspect='auto', cmap='YlOrRd', vmin=0, vmax=vmax)\n",
    "        ax.set_xticks(range(len(top20_kw)))\n",
    "        ax.set_xticklabels(top20_kw, rotation=45, ha='right', fontsize=9)\n",
    "        ax.set_yticks(range(len(prompt_labels_short)))\n",
    "        ax.set_yticklabels(prompt_labels_short, fontsize=8)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(\"Peterson Keyword\")\n",
    "        # Annotate cells\n",
    "        for i in range(matrix.shape[0]):\n",
    "            for j in range(matrix.shape[1]):\n",
    "                if matrix[i, j] > 0:\n",
    "                    ax.text(j, i, str(matrix[i, j]),\n",
    "                            ha='center', va='center', fontsize=7,\n",
    "                            color='white' if matrix[i, j] > vmax * 0.6 else 'black')\n",
    "        plt.colorbar(im, ax=ax, label=\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"06_keyword_heatmap.png\", bbox_inches='tight', dpi=150)\n",
    "    plt.show()\nelse:\n",
    "    print(\"No Peterson keywords found in responses — skipping heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Visualization — Side-by-Side Response Comparison\n",
    "\n",
    "Quantitative metrics are valuable, but it's also important to read the actual responses. This section formats the outputs from both models side-by-side for qualitative inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display responses for the first 3 prompts in a readable format\n",
    "for i in range(min(3, len(EVAL_PROMPTS))):\n",
    "    print(\"━\" * 90)\n",
    "    print(f\"PROMPT {i+1}: {EVAL_PROMPTS[i]}\")\n",
    "    print(\"━\" * 90)\n",
    "    print(f\"\\n🔵 BASE MODEL:\")\n",
    "    print(base_responses[i] if base_responses[i] else \"(empty response)\")\n",
    "    print(f\"\\n🟠 FINE-TUNED MODEL:\")\n",
    "    print(tuned_responses[i] if tuned_responses[i] else \"(empty response)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 14: Summary — Radar Chart\n",
    "\n",
    "A **radar chart** (also called a spider chart) is ideal for comparing two models across multiple independent metrics simultaneously. Each axis represents one metric, normalized to a 0–1 scale. The area enclosed by each polygon gives an intuitive sense of overall performance — the larger the area, the more Peterson-like the model.\n",
    "\n",
    "All metrics are transformed so that **larger = more Peterson-like**:\n",
    "- Perplexity is inverted (lower perplexity → higher score)\n",
    "- All other metrics are already in the \"higher = better\" direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Collect raw scalar values ──────────────────────────────────────────────\n",
    "avg_base_ppl   = sum(base_perplexities)  / len(base_perplexities)\n",
    "avg_tuned_ppl  = sum(tuned_perplexities) / len(tuned_perplexities)\n",
    "avg_base_sim   = sum(base_similarities)  / len(base_similarities)\n",
    "avg_tuned_sim  = sum(tuned_similarities) / len(tuned_similarities)\n",
    "avg_base_kd    = sum(base_stats['keyword_density'])  / max(len(base_stats['keyword_density']),  1)\n",
    "avg_tuned_kd   = sum(tuned_stats['keyword_density']) / max(len(tuned_stats['keyword_density']), 1)\n",
    "avg_base_ttr   = sum(base_stats['ttr_values'])  / len(base_stats['ttr_values'])\n",
    "avg_tuned_ttr  = sum(tuned_stats['ttr_values']) / len(tuned_stats['ttr_values'])\n",
    "avg_base_len   = sum(base_stats['word_counts'])  / len(base_stats['word_counts'])\n",
    "avg_tuned_len  = sum(tuned_stats['word_counts']) / len(tuned_stats['word_counts'])\n",
    "\n",
    "# ── Normalize each metric to [0, 1] ───────────────────────────────────────\n",
    "# For perplexity: lower is better, so we invert.\n",
    "# We use the range [min, max] across both models for normalization.\n",
    "def norm(base_val, tuned_val, higher_is_better=True):\n",
    "    lo, hi = min(base_val, tuned_val), max(base_val, tuned_val)\n",
    "    if abs(hi - lo) < 1e-9:\n",
    "        return 0.5, 0.5  # both equal\n",
    "    b_norm = (base_val  - lo) / (hi - lo)\n",
    "    t_norm = (tuned_val - lo) / (hi - lo)\n",
    "    if not higher_is_better:\n",
    "        b_norm, t_norm = 1 - b_norm, 1 - t_norm\n",
    "    # Rescale to [0.1, 1.0] so neither model hits 0 (avoids invisible wedge)\n",
    "    b_norm = 0.1 + 0.9 * b_norm\n",
    "    t_norm = 0.1 + 0.9 * t_norm\n",
    "    return b_norm, t_norm\n",
    "\n",
    "metrics = [\n",
    "    (\"Perplexity\\n(inverted)\",    *norm(avg_base_ppl,  avg_tuned_ppl,  higher_is_better=False)),\n",
    "    (\"TF-IDF\\nSimilarity\",        *norm(avg_base_sim,  avg_tuned_sim,  higher_is_better=True)),\n",
    "    (\"Keyword\\nDensity\",          *norm(avg_base_kd,   avg_tuned_kd,   higher_is_better=True)),\n",
    "    (\"Vocabulary\\nRichness (TTR)\",*norm(avg_base_ttr,  avg_tuned_ttr,  higher_is_better=True)),\n",
    "    (\"Response\\nLength\",          *norm(avg_base_len,  avg_tuned_len,  higher_is_better=True)),\n",
    "]\n",
    "\n",
    "labels  = [m[0] for m in metrics]\n",
    "base_v  = [m[1] for m in metrics]\n",
    "tuned_v = [m[2] for m in metrics]\n",
    "\n",
    "# Close the radar polygon\n",
    "labels  += [labels[0]]\n",
    "base_v  += [base_v[0]]\n",
    "tuned_v += [tuned_v[0]]\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "fig.suptitle(\"Radar Summary: How Peterson-Like is Each Model?\",\n",
    "             fontsize=15, fontweight='bold', y=1.01)\n",
    "\n",
    "ax.plot(angles, base_v,  color=BASE_COLOR,  linewidth=2.5, linestyle='solid', label=\"Base Model\")\n",
    "ax.fill(angles, base_v,  color=BASE_COLOR,  alpha=0.15)\n",
    "ax.plot(angles, tuned_v, color=TUNED_COLOR, linewidth=2.5, linestyle='solid', label=\"Fine-Tuned Model\")\n",
    "ax.fill(angles, tuned_v, color=TUNED_COLOR, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels[:-1], size=11)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_ylim(0, 1)\n",
    "# Concentric grid rings\n",
    "ax.set_yticks([0.25, 0.5, 0.75, 1.0])\n",
    "ax.set_yticklabels(['0.25', '0.5', '0.75', '1.0'], size=7, color='grey')\n",
    "ax.yaxis.set_tick_params(labelcolor='grey')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.15), fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"07_radar_summary.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 15: Final Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_change(base, tuned, higher_better=True):\n",
    "    if abs(base) < 1e-9:\n",
    "        return \"N/A\"\n",
    "    pct = 100 * (tuned - base) / base\n",
    "    symbol = \"▲\" if (pct > 0) == higher_better else \"▼\"\n",
    "    color_note = \"✓\" if (pct > 0) == higher_better else \"✗\"\n",
    "    return f\"{pct:+.1f}% {symbol} {color_note}\"\n",
    "\n",
    "summary_rows = [\n",
    "    {\n",
    "        \"Metric\": \"Avg Perplexity on Peterson text\",\n",
    "        \"Direction\": \"↓ lower = better\",\n",
    "        \"Base Model\": f\"{avg_base_ppl:.2f}\",\n",
    "        \"Fine-Tuned\": f\"{avg_tuned_ppl:.2f}\",\n",
    "        \"Change\": pct_change(avg_base_ppl, avg_tuned_ppl, higher_better=False),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Avg TF-IDF Similarity to Peterson\",\n",
    "        \"Direction\": \"↑ higher = better\",\n",
    "        \"Base Model\": f\"{avg_base_sim:.4f}\",\n",
    "        \"Fine-Tuned\": f\"{avg_tuned_sim:.4f}\",\n",
    "        \"Change\": pct_change(avg_base_sim, avg_tuned_sim, higher_better=True),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Avg Keyword Density\",\n",
    "        \"Direction\": \"↑ higher = better\",\n",
    "        \"Base Model\": f\"{100*avg_base_kd:.2f}%\",\n",
    "        \"Fine-Tuned\": f\"{100*avg_tuned_kd:.2f}%\",\n",
    "        \"Change\": pct_change(avg_base_kd, avg_tuned_kd, higher_better=True),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Avg Type-Token Ratio (Vocabulary Richness)\",\n",
    "        \"Direction\": \"↑ higher = better\",\n",
    "        \"Base Model\": f\"{avg_base_ttr:.4f}\",\n",
    "        \"Fine-Tuned\": f\"{avg_tuned_ttr:.4f}\",\n",
    "        \"Change\": pct_change(avg_base_ttr, avg_tuned_ttr, higher_better=True),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Avg Response Length (words)\",\n",
    "        \"Direction\": \"— informational\",\n",
    "        \"Base Model\": f\"{avg_base_len:.1f}\",\n",
    "        \"Fine-Tuned\": f\"{avg_tuned_len:.1f}\",\n",
    "        \"Change\": f\"{avg_tuned_len - avg_base_len:+.1f} words\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df_final = pd.DataFrame(summary_rows)\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"FINAL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(df_final.to_string(index=False))\n",
    "print(\"=\" * 90)\n",
    "print(\"\\n✓ = improvement in expected direction  |  ✗ = no improvement  |  ▲/▼ = direction of change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 16: Saved Figures\n",
    "\n",
    "All visualizations have been saved as high-resolution PNGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saved figures:\")\n",
    "for f in sorted(FIGURES_DIR.iterdir()):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  {f.name}  ({size_kb:.0f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "### What the Metrics Tell Us\n",
    "\n",
    "**Perplexity** is the most direct measure of domain adaptation. When a model is fine-tuned on a specific author's writing, it assigns higher probability to that author's actual sentences — meaning it is less \"surprised\" by them. A lower perplexity on Peterson passages indicates that the fine-tuned model has internalized his vocabulary, sentence structure, and thematic patterns.\n",
    "\n",
    "**TF-IDF Similarity** captures whether the model uses Peterson's distinctive vocabulary (not just common words) when answering questions on his topics. High TF-IDF similarity means the response would look familiar to someone who has read Peterson's books.\n",
    "\n",
    "**Keyword Density** measures domain vocabulary adoption directly. Peterson's core vocabulary (chaos/order, meaning/suffering, hero/archetype, responsibility, etc.) is highly distinctive. A fine-tuned model should use these terms organically, not just generically answer the question.\n",
    "\n",
    "**Type-Token Ratio** reflects vocabulary richness. Peterson is known for varied, sophisticated language. A fine-tuned model that has absorbed his style should exhibit greater lexical diversity than a generic base model.\n",
    "\n",
    "**Response Length** is a secondary quality signal. Peterson's writing and lectures are notably detailed and elaborate. A fine-tuned model may produce longer responses because it has learned that Peterson-style answers are thorough.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **One epoch of training**: The model was trained for only 1 epoch (~73 minutes). Multiple epochs would likely produce stronger adaptation.\n",
    "- **Greedy decoding**: We used deterministic generation for fair comparison, but sampling (`temperature > 0`) might show more stylistic differences.\n",
    "- **Short passages**: 350-word training chunks may not capture the full arc of Peterson's extended arguments.\n",
    "- **Training loss of 3.01**: This is still relatively high, suggesting significant room for further improvement with more epochs or a higher LoRA rank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}