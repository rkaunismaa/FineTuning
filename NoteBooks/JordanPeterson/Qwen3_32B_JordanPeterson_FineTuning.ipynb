{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e83ae59",
   "metadata": {},
   "source": [
    "# Qwen3-32B Jordan Peterson Fine-Tuning\n",
    "## Maximum Quality on a 24 GB GPU — Conservative VRAM Strategy\n",
    "\n",
    "This notebook fine-tunes `unsloth/Qwen3-32B-unsloth-bnb-4bit` on Jordan B.\n",
    "Peterson's four books using the **synthetic Q&A methodology** developed in the\n",
    "Qwen3-14B V2 notebook.\n",
    "\n",
    "The 32B model is the largest dense Qwen3 variant that can plausibly fit on a\n",
    "single 24 GB GPU.  It represents the highest quality ceiling available given\n",
    "the hardware — more parameters means more capacity to encode Peterson's\n",
    "distinctive vocabulary, rhetorical structure, and conceptual framing.\n",
    "\n",
    "**Prerequisite**: This notebook uses the same Q&A dataset as the 14B V2\n",
    "notebook.  If you have already run that notebook, the dataset cache at\n",
    "`qa_dataset/peterson_qa.jsonl` will be detected and reused automatically —\n",
    "no additional API calls needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe228a",
   "metadata": {},
   "source": [
    "---\n",
    "## Why Qwen3-32B? The Case for a Larger Dense Model\n",
    "\n",
    "### More Parameters = More Stylistic Capacity\n",
    "\n",
    "A language model's LoRA adapter can only encode stylistic patterns in a\n",
    "compressed low-rank subspace of the full weight space.  With r=32, we give\n",
    "the adapter 32 independent directions per attention projection to capture\n",
    "Peterson's style.  But those directions are projected into the *full model\n",
    "weight space*, whose size determines how expressively they can be represented.\n",
    "\n",
    "A 32B model has a hidden dimension of ~5,120 (vs ~5,120 for 14B — Qwen3-14B and\n",
    "32B use the same hidden dim but differ in depth and attention heads).  More\n",
    "layers and more attention heads means more places where the LoRA update can\n",
    "reinforce Peterson's vocabulary choices and rhetorical patterns.\n",
    "\n",
    "### Why Dense > MoE for Style Fine-Tuning\n",
    "\n",
    "The other 30B-class model available is `Qwen3-30B-A3B` — a Mixture of Experts\n",
    "(MoE) model with 30B total parameters but only **3B active per forward pass**.\n",
    "While MoE models excel at knowledge-intensive tasks, they have a structural\n",
    "disadvantage for stylistic fine-tuning:\n",
    "\n",
    "- Each token is routed to a different subset of experts\n",
    "- Style is a *global* property: Peterson's voice should be consistent across\n",
    "  every token prediction, not just those that happen to route to \"Peterson-experts\"\n",
    "- Fine-tuning MoE with LoRA requires teaching all expert groups consistently,\n",
    "  which needs more training data and more epochs to converge\n",
    "\n",
    "**The Qwen3-32B dense model is the right choice**: every token prediction passes\n",
    "through the same weights, so every training example uniformly teaches the style.\n",
    "\n",
    "### The VRAM Challenge\n",
    "\n",
    "The 32B model is the most capable model that can *potentially* fit on a 24 GB\n",
    "GPU.  It requires careful settings.  The next section explains exactly how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9fc7e",
   "metadata": {},
   "source": [
    "---\n",
    "## VRAM Strategy: Fitting 32B into 24 GB\n",
    "\n",
    "### The Math\n",
    "\n",
    "Our measured baseline from the Qwen3-14B V2 run:\n",
    "\n",
    "| Component | Qwen3-14B V2 | Qwen3-32B (estimate) |\n",
    "|-----------|-------------|---------------------|\n",
    "| 4-bit model weights | ~7.7 GB | ~17.6 GB |\n",
    "| Training overhead* | ~5.7 GB | ~3.0 GB (after reductions) |\n",
    "| **Total peak** | **13.4 GB** (measured) | **~20–22 GB** (estimated) |\n",
    "\n",
    "*Training overhead = activations + KV cache + LoRA adapter + 8-bit optimizer states\n",
    "\n",
    "The overhead scales with `batch_size × max_seq_length`, so halving both\n",
    "roughly halves the overhead:\n",
    "\n",
    "### Four Levers We Pull\n",
    "\n",
    "| Setting | V1 14B | V2 14B | **32B** | Saving vs 14B V2 |\n",
    "|---------|--------|--------|---------|-----------------|\n",
    "| `per_device_train_batch_size` | 2 | 2 | **1** | ~2.5 GB |\n",
    "| `max_seq_length` | 2048 | 2048 | **1024** | ~1.5 GB |\n",
    "| `gradient_accumulation_steps` | 4 | 4 | **8** | compensates for batch=1 |\n",
    "| `gradient_checkpointing` | unsloth | unsloth | unsloth | already maximised |\n",
    "\n",
    "**Effective batch size stays at 8**: `batch_size=1 × grad_accum=8 = 8`\n",
    "The *gradient quality* is identical to V2; we just spread it over more steps.\n",
    "\n",
    "### What Happens if It Still OOMs\n",
    "\n",
    "Despite these reductions, 24 GB is a hard limit.  If CUDA raises an\n",
    "out-of-memory error, this notebook will catch it and print an explicit\n",
    "fallback plan rather than dying silently.  The training cell is wrapped in a\n",
    "`try/except` so you get actionable guidance immediately.\n",
    "\n",
    "### Sequence Length Adequacy\n",
    "\n",
    "Our Q&A pairs are:\n",
    "- Question: ~20 words → ~30 tokens\n",
    "- Answer (Peterson passage): ~350 words → ~500 tokens\n",
    "- System prompt + ChatML tokens: ~150 tokens\n",
    "- **Total per example: ~680 tokens**\n",
    "\n",
    "`max_seq_length=1024` covers 99%+ of examples with comfortable headroom.\n",
    "Only the very longest passages (statistical tail) will be truncated, and\n",
    "the truncation is applied to the end of the passage — the style-bearing\n",
    "content is in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69c048",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Q&A Dataset\n",
    "\n",
    "This section is identical to the Qwen3-14B V2 notebook.  The Q&A cache at\n",
    "`qa_dataset/peterson_qa.jsonl` is **shared** between the 14B and 32B notebooks\n",
    "— if you have already run V2, this entire section will complete in seconds\n",
    "by loading from cache.  If not, it will call the Claude Haiku API and generate\n",
    "the dataset (~$1–3, ~15–20 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "_result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"anthropic\", \"-q\"],\n",
    "    capture_output=True, text=True,\n",
    ")\n",
    "print(\"anthropic SDK ready.\" if _result.returncode == 0 else _result.stderr[:300])\n",
    "\n",
    "import os, re, json, time, math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import fitz\n",
    "import anthropic\n",
    "\n",
    "print(f\"anthropic : {anthropic.__version__}\")\n",
    "\n",
    "# ── API key ────────────────────────────────────────────────────────────────\n",
    "_api_key = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")\n",
    "if not _api_key:\n",
    "    _env_file = Path.home() / \".env\"\n",
    "    if _env_file.exists():\n",
    "        for _line in _env_file.read_text().splitlines():\n",
    "            if _line.startswith(\"ANTHROPIC_API_KEY=\"):\n",
    "                _api_key = _line.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
    "                break\n",
    "if not _api_key:\n",
    "    raise EnvironmentError(\n",
    "        \"ANTHROPIC_API_KEY not found.\\n\"\n",
    "        \"Set it with:  export ANTHROPIC_API_KEY='sk-ant-...'\\n\"\n",
    "        \"Or add it to ~/.env as:  ANTHROPIC_API_KEY=sk-ant-...\"\n",
    "    )\n",
    "print(\"API key found.\")\n",
    "\n",
    "BOOKS_DIR        = Path(\"../../Books/JordanPeterson\")\n",
    "QA_DIR           = Path(\"./qa_dataset\")\n",
    "QA_CACHE         = QA_DIR / \"peterson_qa.jsonl\"   # shared with 14B V2 notebook\n",
    "QA_DIR.mkdir(exist_ok=True)\n",
    "GENERATION_MODEL = \"claude-haiku-4-5-20251001\"\n",
    "\n",
    "if QA_CACHE.exists():\n",
    "    with open(QA_CACHE) as f:\n",
    "        _n = sum(1 for line in f if line.strip())\n",
    "    print(f\"\\nQ&A cache found: {_n:,} records — generation will be skipped.\")\n",
    "else:\n",
    "    print(\"\\nQ&A cache not found — will generate from PDFs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1acf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw: str) -> str:\n",
    "    \"\"\"Strip PDF artefacts: control chars, ligatures, excess whitespace.\"\"\"\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', raw)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.replace('\\ufb01', 'fi').replace('\\ufb02', 'fl').strip()\n",
    "\n",
    "\n",
    "def extract_chunks(pdf_path: Path, chunk_words: int = 350,\n",
    "                   overlap_words: int = 50) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract clean text from a PDF and split into overlapping word-level chunks.\n",
    "\n",
    "    chunk_words=350 produces passages that fit well within max_seq_length=1024\n",
    "    after applying the ChatML system+user template overhead (~150 tokens).\n",
    "    overlap_words=50 ensures no passage starts mid-sentence.\n",
    "    \"\"\"\n",
    "    doc   = fitz.open(str(pdf_path))\n",
    "    pages = [clean_text(page.get_text()) for page in doc]\n",
    "    doc.close()\n",
    "    words  = ' '.join(pages).split()\n",
    "    step   = chunk_words - overlap_words\n",
    "    return [\n",
    "        ' '.join(words[s : s + chunk_words])\n",
    "        for s in range(0, len(words), step)\n",
    "        if len(words[s : s + chunk_words]) >= 100\n",
    "    ]\n",
    "\n",
    "\n",
    "pdf_files  = sorted(BOOKS_DIR.glob(\"*.pdf\"))\n",
    "all_chunks = []\n",
    "chunk_meta = []\n",
    "\n",
    "print(f\"Extracting from {len(pdf_files)} PDFs…\")\n",
    "for pdf in pdf_files:\n",
    "    fname = pdf.name.lower()\n",
    "    if   \"maps\"    in fname: label = \"Maps of Meaning\"\n",
    "    elif \"12 rules\" in fname or \"antidote\" in fname: label = \"12 Rules for Life\"\n",
    "    elif \"beyond\"  in fname: label = \"Beyond Order\"\n",
    "    else:                    label = \"We Who Wrestle with God\"\n",
    "\n",
    "    chunks = extract_chunks(pdf)\n",
    "    all_chunks.extend(chunks)\n",
    "    chunk_meta.extend([{\"book\": label}] * len(chunks))\n",
    "    print(f\"  {label:<35}  {len(chunks):4d} chunks\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_chunks):,} passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc20786",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=_api_key)\n",
    "\n",
    "\n",
    "def generate_questions(passage: str, book: str, max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Call Claude Haiku to generate 2 questions answered by this passage.\n",
    "\n",
    "    Returns only the questions (not answers) — the passage itself is used as\n",
    "    the answer in the training dataset.  This keeps output tokens minimal\n",
    "    and cost low (~$0.00068 per passage, ~$0.82 for all 1,200 passages).\n",
    "\n",
    "    Retries with exponential backoff on API errors or malformed JSON.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"You are building a training dataset for a Jordan B. Peterson AI model.\\n\\n\"\n",
    "        f\"The passage below is from Peterson's book '{book}'.  Generate exactly 2 questions that:\\n\"\n",
    "        f\"1. This passage directly and substantively answers\\n\"\n",
    "        f\"2. Someone interested in Peterson's ideas might genuinely ask\\n\"\n",
    "        f\"3. Cover different angles of the passage (e.g. one concrete, one philosophical)\\n\\n\"\n",
    "        f\"Peterson's topics: order vs chaos, meaning, personal responsibility, suffering, \"\n",
    "        f\"mythology, archetypes, shadow, logos, truth, religion, Jungian psychology, \"\n",
    "        f\"hierarchy, heroism, sacrifice, being.\\n\\n\"\n",
    "        f\"Return ONLY a JSON array of exactly 2 question strings.  No other text.\\n\"\n",
    "        f\"Example: [\\\"Why is confronting chaos necessary for meaning?\\\", \"\n",
    "        f\"\\\"What role does suffering play in personal development?\\\"]\\n\\n\"\n",
    "        f\"Passage:\\n{passage}\"\n",
    "    )\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp  = client.messages.create(\n",
    "                model=GENERATION_MODEL, max_tokens=150,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            raw   = resp.content[0].text.strip()\n",
    "            match = re.search(r'\\[.*?\\]', raw, re.DOTALL)\n",
    "            if not match:\n",
    "                raise ValueError(f\"No JSON array in: {raw[:80]}\")\n",
    "            qs = json.loads(match.group())\n",
    "            return [str(q).strip() for q in qs[:2]]\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    [retry {attempt+1}] {e} — wait {wait}s\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"    [FAILED] passage skipped\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "print(\"generate_questions() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Check existing cache ───────────────────────────────────────────────────\n",
    "existing = []\n",
    "if QA_CACHE.exists():\n",
    "    with open(QA_CACHE) as f:\n",
    "        existing = [json.loads(l) for l in f if l.strip()]\n",
    "\n",
    "_expected = len(all_chunks) * 2\n",
    "_coverage = len(existing) / _expected if _expected else 0\n",
    "print(f\"Cache coverage: {len(existing):,} / {_expected:,}  ({100*_coverage:.1f}%)\")\n",
    "\n",
    "if _coverage >= 0.90:\n",
    "    print(\"Cache ≥90% complete — skipping generation.\")\n",
    "    print(f\"  (Delete {QA_CACHE} to force regeneration)\")\n",
    "else:\n",
    "    already_done   = len(existing) // 2\n",
    "    remaining      = all_chunks[already_done:]\n",
    "    remaining_meta = chunk_meta[already_done:]\n",
    "    print(f\"Generating {len(remaining):,} passages (from #{already_done+1})…\")\n",
    "    print(f\"Estimated cost: ~${len(remaining)*0.00068:.2f}\\n\")\n",
    "\n",
    "    skipped = 0\n",
    "    with open(QA_CACHE, \"a\") as out_f:\n",
    "        for i, (passage, meta) in enumerate(zip(remaining, remaining_meta)):\n",
    "            if i % 50 == 0:\n",
    "                pct = 100 * (already_done + i) / len(all_chunks)\n",
    "                print(f\"  [{already_done+i+1:4d}/{len(all_chunks):4d}]  \"\n",
    "                      f\"{pct:5.1f}%  {meta['book']}\")\n",
    "            questions = generate_questions(passage, meta[\"book\"])\n",
    "            if not questions:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            for q in questions:\n",
    "                out_f.write(json.dumps({\"question\": q, \"answer\": passage,\n",
    "                                        \"book\": meta[\"book\"]}) + \"\\n\")\n",
    "            out_f.flush()\n",
    "            time.sleep(0.3)\n",
    "    print(f\"\\nDone.  Processed: {len(remaining)-skipped:,}  Skipped: {skipped}\")\n",
    "\n",
    "with open(QA_CACHE) as f:\n",
    "    qa_records = [json.loads(l) for l in f if l.strip()]\n",
    "print(f\"Total Q&A pairs: {len(qa_records):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cdc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_counts = Counter(r[\"book\"] for r in qa_records)\n",
    "print(\"Q&A pairs by book:\")\n",
    "for book, count in sorted(book_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {book:<35}  {count:4d}  ({100*count/len(qa_records):.1f}%)\")\n",
    "\n",
    "ans_len = [len(r[\"answer\"].split()) for r in qa_records]\n",
    "print(f\"\\nAnswer length: min={min(ans_len)}  max={max(ans_len)}  \"\n",
    "      f\"mean={sum(ans_len)/len(ans_len):.0f} words\")\n",
    "\n",
    "# Token budget sanity check\n",
    "# ~1.4 tokens/word × mean answer + ~180 tokens overhead < 1024 limit\n",
    "mean_tokens_est = sum(ans_len) / len(ans_len) * 1.4 + 180\n",
    "print(f\"Estimated mean tokens per example: ~{mean_tokens_est:.0f}  \"\n",
    "      f\"(limit: 1024 — {'OK' if mean_tokens_est < 900 else 'WARNING: close to limit'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45d34a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Fine-Tuning Qwen3-32B\n",
    "\n",
    "## Configuration\n",
    "\n",
    "| Parameter | Qwen3-14B V2 | **Qwen3-32B** | Rationale |\n",
    "|-----------|-------------|--------------|-----------|\n",
    "| Base model | `Qwen3-14B-unsloth-bnb-4bit` | **`Qwen3-32B-unsloth-bnb-4bit`** | 2.3× more parameters |\n",
    "| Training data | Synthetic Q&A | Same | Shared cache |\n",
    "| Epochs | 3 | **3** | Same generalisation target |\n",
    "| LoRA rank | r=32 | **r=32** | Same adapter capacity ratio |\n",
    "| LoRA alpha | 32 | **32** | alpha/rank = 1.0 |\n",
    "| Batch size | 2 | **1** | Primary VRAM reduction |\n",
    "| Gradient accumulation | 4 | **8** | Keeps effective batch = 8 |\n",
    "| Max seq length | 2048 | **1024** | Secondary VRAM reduction |\n",
    "| Warmup steps | 30 | **30** | Same absolute warmup |\n",
    "| LR | 2e-4 | **2e-4** | Standard Qwen3 LoRA |\n",
    "| Estimated peak VRAM | ~13.4 GB | **~20–22 GB** | Right up to the 24 GB limit |\n",
    "| Output dir | `qwen3_14b_peterson_v2_lora` | **`qwen3_32b_peterson_lora`** | Separate adapter |\n",
    "\n",
    "## Risk Acknowledgement\n",
    "\n",
    "This notebook will attempt training.  If the GPU runs out of memory, a\n",
    "structured error handler will print an explicit fallback plan rather than\n",
    "a raw CUDA traceback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc7b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc, math, json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# ── Paths ──────────────────────────────────────────────────────────────────\n",
    "QA_CACHE   = Path(\"./qa_dataset/peterson_qa.jsonl\")\n",
    "OUTPUT_DIR = Path(\"./outputs/qwen3_32b_peterson_lora\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ── Model ──────────────────────────────────────────────────────────────────\n",
    "BASE_MODEL  = \"unsloth/Qwen3-32B-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LEN = 1024    # halved from 2048 — single biggest VRAM lever\n",
    "\n",
    "# ── LoRA ──────────────────────────────────────────────────────────────────\n",
    "LORA_RANK  = 32       # same as 14B V2; adapter capacity ratio is identical\n",
    "LORA_ALPHA = 32       # alpha/rank = 1.0\n",
    "\n",
    "# ── Training ─────────────────────────────────────────────────────────────\n",
    "BATCH_SIZE    = 1     # halved — primary VRAM saving\n",
    "GRAD_ACCUM    = 8     # doubled — effective batch stays at 8\n",
    "NUM_EPOCHS    = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS  = 30\n",
    "WEIGHT_DECAY  = 0.01\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an AI assistant that has been trained on the complete works of \"\n",
    "    \"Jordan B. Peterson, a Canadian clinical psychologist, professor, and author. \"\n",
    "    \"You speak with deep knowledge of psychology, philosophy, mythology, religion, \"\n",
    "    \"and personal responsibility.  Your responses reflect Peterson's writing style, \"\n",
    "    \"intellectual depth, and interdisciplinary approach to understanding human \"\n",
    "    \"nature and meaning.\"\n",
    ")\n",
    "\n",
    "# ── Hardware summary ───────────────────────────────────────────────────────\n",
    "total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f\"GPU      : {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM     : {total_vram:.1f} GB total\")\n",
    "print()\n",
    "if total_vram < 23.5:\n",
    "    print(f\"WARNING: This GPU has {total_vram:.1f} GB VRAM.\")\n",
    "    print(\"Qwen3-32B requires ~20-22 GB during training.\")\n",
    "    print(\"Proceeding, but OOM is possible — see the training cell for fallback.\")\n",
    "    print()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model            : {BASE_MODEL}\")\n",
    "print(f\"  max_seq_length   : {MAX_SEQ_LEN}  (halved vs 14B V2 to save VRAM)\")\n",
    "print(f\"  batch_size       : {BATCH_SIZE}   (halved vs 14B V2)\")\n",
    "print(f\"  grad_accum       : {GRAD_ACCUM}   (doubled — effective batch = {BATCH_SIZE*GRAD_ACCUM})\")\n",
    "print(f\"  epochs           : {NUM_EPOCHS}\")\n",
    "print(f\"  LoRA             : r={LORA_RANK}, alpha={LORA_ALPHA}\")\n",
    "print(f\"  Output           : {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not QA_CACHE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Q&A cache not found at {QA_CACHE}.\\nRun Part 1 first.\"\n",
    "    )\n",
    "\n",
    "with open(QA_CACHE) as f:\n",
    "    records = [json.loads(l) for l in f if l.strip()]\n",
    "\n",
    "raw_dataset = Dataset.from_list([\n",
    "    {\"question\": r[\"question\"], \"answer\": r[\"answer\"]}\n",
    "    for r in records\n",
    "])\n",
    "\n",
    "print(f\"Dataset: {len(raw_dataset):,} Q&A pairs  |  columns: {raw_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9031b",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Load Qwen3-32B\n",
    "\n",
    "This is the most memory-intensive step.  The 4-bit quantized weights alone\n",
    "occupy ~17.6 GB.  We monitor VRAM before and after loading so we know exactly\n",
    "how much headroom remains for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Pre-load VRAM check ────────────────────────────────────────────────────\n",
    "# Clear any stale allocations from previous runs in this kernel session\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "free_before = (torch.cuda.get_device_properties(0).total_memory\n",
    "               - torch.cuda.memory_reserved()) / 1e9\n",
    "print(f\"Free VRAM before model load : {free_before:.1f} GB\")\n",
    "if free_before < 17.0:\n",
    "    print(\"WARNING: less than 17 GB free — model weights may not fit.\")\n",
    "    print(\"Restart the kernel to free all allocations and try again.\")\n",
    "\n",
    "print(f\"\\nLoading {BASE_MODEL} …\")\n",
    "print(\"(This downloads ~18 GB on first run — may take several minutes.)\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    dtype           = None,          # auto: bfloat16 on Ampere+\n",
    "    max_seq_length  = MAX_SEQ_LEN,   # 1024 — critical for VRAM management\n",
    "    load_in_4bit    = True,          # 4-bit quantization via bitsandbytes\n",
    "    full_finetuning = False,\n",
    ")\n",
    "\n",
    "vram_after = torch.cuda.memory_reserved() / 1e9\n",
    "vram_free  = total_vram - vram_after\n",
    "print(f\"\\nModel loaded.\")\n",
    "print(f\"  VRAM reserved : {vram_after:.1f} GB\")\n",
    "print(f\"  VRAM free     : {vram_free:.1f} GB  (training needs ~3-5 GB more)\")\n",
    "if vram_free < 3.0:\n",
    "    print(\"  WARNING: very little headroom — training will likely OOM.\")\n",
    "    print(\"  Consider reducing max_seq_length to 768 before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253a69c",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Add LoRA Adapters\n",
    "\n",
    "Same configuration as the 14B V2 notebook: r=32, alpha=32, all attention and\n",
    "MLP projections targeted.  The adapter size scales with `d_model × r`, and\n",
    "Qwen3-32B's larger hidden dimension (~5,120) means the r=32 adapter encodes\n",
    "more expressive style directions than the same rank on a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                          = LORA_RANK,\n",
    "    lora_alpha                 = LORA_ALPHA,\n",
    "    lora_dropout               = 0,\n",
    "    target_modules             = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    use_gradient_checkpointing = \"unsloth\",   # saves ~30% VRAM vs PyTorch default\n",
    "    random_state               = 42,\n",
    "    use_rslora                 = False,\n",
    "    loftq_config               = None,\n",
    ")\n",
    "\n",
    "total_p     = sum(p.numel() for p in model.parameters())\n",
    "trainable_p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters    : {total_p:,}\")\n",
    "print(f\"Trainable (LoRA)    : {trainable_p:,}  ({100*trainable_p/total_p:.4f}%)\")\n",
    "print(f\"VRAM after LoRA     : {torch.cuda.memory_reserved()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cf44c",
   "metadata": {},
   "source": [
    "---\n",
    "## Steps 5–6: Format Dataset and Apply Response Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71542114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(batch):\n",
    "    \"\"\"\n",
    "    Convert Q&A pairs to Qwen3 ChatML format.\n",
    "\n",
    "    The training signal is entirely in the assistant (answer) tokens.\n",
    "    The system prompt and user question are masked out by train_on_responses_only\n",
    "    so the model only learns what to say, not the structural tokens around it.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for q, a in zip(batch[\"question\"], batch[\"answer\"]):\n",
    "        conv = [\n",
    "            {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",      \"content\": q},\n",
    "            {\"role\": \"assistant\", \"content\": a},\n",
    "        ]\n",
    "        texts.append(tokenizer.apply_chat_template(\n",
    "            conv, tokenize=False, add_generation_prompt=False,\n",
    "            enable_thinking=False,\n",
    "        ))\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "dataset = raw_dataset.map(\n",
    "    format_example, batched=True, batch_size=256,\n",
    "    remove_columns=raw_dataset.column_names,\n",
    ")\n",
    "print(f\"Formatted: {len(dataset):,} examples\")\n",
    "\n",
    "# Verify token budget: show how many examples fit within max_seq_length\n",
    "sample_ids = tokenizer(dataset[0][\"text\"])[\"input_ids\"]\n",
    "print(f\"Sample token count  : {len(sample_ids)} (limit: {MAX_SEQ_LEN})\")\n",
    "if len(sample_ids) > MAX_SEQ_LEN:\n",
    "    print(\"WARNING: first sample exceeds max_seq_length — it will be truncated.\")\n",
    "\n",
    "# ── Detect response boundary tokens ────────────────────────────────────────\n",
    "_sample = dataset[0][\"text\"]\n",
    "if \"<|im_start|>assistant\\n\" in _sample:\n",
    "    instruction_part = \"<|im_start|>user\\n\"\n",
    "    response_part    = \"<|im_start|>assistant\\n\"\n",
    "else:\n",
    "    raise RuntimeError(\"Could not detect Qwen3 ChatML tokens in formatted data.\")\n",
    "print(f\"\\nResponse boundary   : {repr(response_part)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb048aa",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Configure SFTTrainer\n",
    "\n",
    "The key difference from the 14B V2 notebook is `per_device_train_batch_size=1`\n",
    "and `gradient_accumulation_steps=8`.  The optimizer sees the same quality of\n",
    "gradient signal (effective batch = 8) but processes one example at a time,\n",
    "which halves the peak activation memory during each forward-backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model         = model,\n",
    "    tokenizer     = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args          = SFTConfig(\n",
    "        dataset_text_field          = \"text\",\n",
    "        max_seq_length              = MAX_SEQ_LEN,\n",
    "        dataset_num_proc            = 2,\n",
    "\n",
    "        per_device_train_batch_size = BATCH_SIZE,   # 1 — primary VRAM saving\n",
    "        gradient_accumulation_steps = GRAD_ACCUM,   # 8 — effective batch = 8\n",
    "        num_train_epochs            = NUM_EPOCHS,   # 3\n",
    "\n",
    "        learning_rate               = LEARNING_RATE,\n",
    "        warmup_steps                = WARMUP_STEPS,\n",
    "        lr_scheduler_type           = \"cosine\",\n",
    "        weight_decay                = WEIGHT_DECAY,\n",
    "\n",
    "        optim                       = \"adamw_8bit\",\n",
    "        fp16                        = not torch.cuda.is_bf16_supported(),\n",
    "        bf16                        = torch.cuda.is_bf16_supported(),\n",
    "\n",
    "        logging_steps               = 25,\n",
    "        save_strategy               = \"epoch\",\n",
    "        output_dir                  = str(OUTPUT_DIR),\n",
    "        report_to                   = \"none\",\n",
    "        seed                        = 42,\n",
    "        packing                     = False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "sft_trainer = train_on_responses_only(\n",
    "    sft_trainer,\n",
    "    instruction_part = instruction_part,\n",
    "    response_part    = response_part,\n",
    ")\n",
    "\n",
    "# ── Estimate total training steps ──────────────────────────────────────────\n",
    "_steps = math.ceil(len(dataset) / BATCH_SIZE) * NUM_EPOCHS // GRAD_ACCUM\n",
    "print(f\"Estimated gradient updates : {_steps:,}\")\n",
    "print(f\"  = {len(dataset):,} examples / batch {BATCH_SIZE} × accum {GRAD_ACCUM} × {NUM_EPOCHS} epochs\")\n",
    "print()\n",
    "print(\"Ready to train.  Current VRAM state:\")\n",
    "print(f\"  Reserved : {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "print(f\"  Free     : {(total_vram - torch.cuda.memory_reserved()/1e9):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be12b1",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Train\n",
    "\n",
    "Training is wrapped in a structured error handler.  If CUDA runs out of memory\n",
    "the handler prints an explicit fallback plan — no raw traceback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9859e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(f\"VRAM before training  : {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "print(f\"Starting — {NUM_EPOCHS} epochs, ~{_steps:,} gradient updates…\")\n",
    "print()\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "try:\n",
    "    train_result = sft_trainer.train()\n",
    "    elapsed_min  = (time.time() - t0) / 60\n",
    "    vram_peak    = torch.cuda.max_memory_reserved() / 1e9\n",
    "\n",
    "    print()\n",
    "    print(\"═\" * 65)\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(\"═\" * 65)\n",
    "    print(f\"  Gradient updates   : {train_result.global_step:,}\")\n",
    "    print(f\"  Training loss      : {train_result.training_loss:.4f}\")\n",
    "    print(f\"  Elapsed            : {elapsed_min:.1f} min\")\n",
    "    print(f\"  Peak VRAM          : {vram_peak:.1f} GB  /  {total_vram:.1f} GB\")\n",
    "    print()\n",
    "    print(\"Reference — Qwen3-14B V2 (same data, same epochs):\")\n",
    "    print(\"  Expected: ~900 steps  |  ~90 min  |  loss ~2.2-2.4  |  ~13.5 GB\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    elapsed_min = (time.time() - t0) / 60\n",
    "    err_str     = str(e).lower()\n",
    "\n",
    "    if \"out of memory\" in err_str or \"cuda\" in err_str:\n",
    "        print()\n",
    "        print(\"╔\" + \"═\" * 63 + \"╗\")\n",
    "        print(\"║  OUT OF MEMORY — Qwen3-32B exceeded 24 GB VRAM              ║\")\n",
    "        print(\"╚\" + \"═\" * 63 + \"╝\")\n",
    "        print()\n",
    "        print(f\"  Failed after {elapsed_min:.1f} min.\")\n",
    "        print(f\"  Peak VRAM before OOM: {torch.cuda.max_memory_reserved()/1e9:.1f} GB\")\n",
    "        print()\n",
    "        print(\"FALLBACK OPTIONS (in order of recommendation):\")\n",
    "        print()\n",
    "        print(\"  Option 1 — Reduce max_seq_length to 768 and retry:\")\n",
    "        print(\"    • Change MAX_SEQ_LEN = 768 in the config cell\")\n",
    "        print(\"    • Restart the kernel and rerun from Step 3\")\n",
    "        print(\"    • ~95% of Q&A examples fit within 768 tokens\")\n",
    "        print()\n",
    "        print(\"  Option 2 — Use Qwen3-30B-A3B (MoE, fits ~18-20 GB):\")\n",
    "        print(\"    • Model: unsloth/Qwen3-30B-A3B-bnb-4bit\")\n",
    "        print(\"    • 30B total params, 3B active per step — lower activation memory\")\n",
    "        print(\"    • Change BASE_MODEL and OUTPUT_DIR in the config cell\")\n",
    "        print(\"    • Keep all other settings the same\")\n",
    "        print()\n",
    "        print(\"  Option 3 — Fall back to Qwen3-14B V2:\")\n",
    "        print(\"    • Proven to work at 13.4 GB peak\")\n",
    "        print(\"    • Notebook: Qwen3_14B_JordanPeterson_V2_FineTuning.ipynb\")\n",
    "        print()\n",
    "        # Free VRAM so the user can retry without a kernel restart if possible\n",
    "        del sft_trainer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"  VRAM after cleanup: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "    else:\n",
    "        raise   # re-raise non-OOM errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0087b5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Save the LoRA Adapter\n",
    "\n",
    "The r=32 adapter for a 32B model will be larger than for 14B — approximately\n",
    "proportional to the number of target projection matrices multiplied by r × d_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving adapter to {OUTPUT_DIR} …\")\n",
    "model.save_pretrained(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "\n",
    "adapter_files = list(OUTPUT_DIR.glob(\"*.safetensors\")) + list(OUTPUT_DIR.glob(\"*.bin\"))\n",
    "total_mb = sum(f.stat().st_size for f in adapter_files) / 1e6\n",
    "print(f\"\\nAdapter files:\")\n",
    "for f in sorted(adapter_files):\n",
    "    print(f\"  {f.name}  ({f.stat().st_size/1e6:.1f} MB)\")\n",
    "print(f\"\\nTotal: {total_mb:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece974c",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Test Inference\n",
    "\n",
    "Run the same 5 evaluation prompts used across all comparison notebooks.\n",
    "Results here give a first qualitative check of response quality before\n",
    "running the full quantitative comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    \"Why is personal responsibility the foundation of a meaningful life?\",\n",
    "    \"How do ancient myths and stories reveal truths about human nature?\",\n",
    "    \"What does it mean to pursue what is meaningful rather than what is expedient?\",\n",
    "    \"How should a person confront suffering rather than flee from it?\",\n",
    "]\n",
    "\n",
    "\n",
    "def ask(question: str, max_new_tokens: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the fine-tuned 32B model.\n",
    "\n",
    "    Identical to the 14B inference wrapper: Qwen3 two-step tokenisation,\n",
    "    enable_thinking=False, greedy decoding (do_sample=False) for\n",
    "    deterministic outputs comparable across notebooks.\n",
    "    \"\"\"\n",
    "    msgs  = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "             {\"role\": \"user\",   \"content\": question}]\n",
    "    text  = tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True, enable_thinking=False,\n",
    "    )\n",
    "    inp   = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inp, max_new_tokens=max_new_tokens,\n",
    "            do_sample=False, temperature=1.0, repetition_penalty=1.1,\n",
    "        )\n",
    "    resp = tokenizer.decode(out[0][inp[\"input_ids\"].shape[1]:],\n",
    "                             skip_special_tokens=True).strip()\n",
    "    return re.sub(r'<think>.*?</think>', '', resp, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "print(\"Testing 32B fine-tuned model (greedy decoding)…\\n\")\n",
    "for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "    print(f\"{'─'*70}\")\n",
    "    print(f\"Q{i+1}: {prompt}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    answer = ask(prompt)\n",
    "    print(answer if answer.strip() else \"(empty response)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f7b02",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Thinking Mode Test\n",
    "\n",
    "The 32B model's chain-of-thought reasoning is considerably more capable than\n",
    "the 14B model's.  Testing it here shows whether the larger model's reasoning\n",
    "capacity is preserved after fine-tuning and how it combines with Peterson's\n",
    "domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_thinking(question: str, max_new_tokens: int = 600) -> tuple[str, str]:\n",
    "    \"\"\"Return (thinking_content, final_answer) using Qwen3 reasoning mode.\"\"\"\n",
    "    msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",   \"content\": question}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True, enable_thinking=True,\n",
    "    )\n",
    "    inp = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inp, max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, temperature=0.6, top_p=0.95, top_k=20,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    full  = tokenizer.decode(out[0][inp[\"input_ids\"].shape[1]:],\n",
    "                              skip_special_tokens=True).strip()\n",
    "    match = re.search(r'<think>(.*?)</think>', full, re.DOTALL)\n",
    "    think = match.group(1).strip() if match else \"\"\n",
    "    ans   = re.sub(r'<think>.*?</think>', '', full, flags=re.DOTALL).strip()\n",
    "    return think, ans\n",
    "\n",
    "\n",
    "q = \"What is the relationship between order and chaos in human experience?\"\n",
    "print(f\"Q: {q}\\n\")\n",
    "thinking, answer = ask_thinking(q)\n",
    "if thinking:\n",
    "    print(f\"[Chain-of-Thought]\\n{thinking[:500]}{'…' if len(thinking)>500 else ''}\\n\")\n",
    "print(f\"[Answer]\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f838ce4",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "### What This Run Achieved\n",
    "\n",
    "A Qwen3-32B model fine-tuned on synthetic Q&A pairs from Peterson's four books\n",
    "for 3 epochs represents the **highest achievable quality** for this domain on\n",
    "a single 24 GB GPU.\n",
    "\n",
    "The three improvements that drive this quality ceiling:\n",
    "\n",
    "1. **Synthetic Q&A data** — the model learned to answer questions in Peterson's\n",
    "   voice, not just continue passages.  The training task matches the inference task.\n",
    "\n",
    "2. **3 epochs** — enough passes over the ~2,400 training examples to generalise\n",
    "   Peterson's style to unseen questions rather than memorise specific passages.\n",
    "\n",
    "3. **Qwen3-32B dense architecture** — 2.3× the parameters of the 14B model,\n",
    "   all of which participate in every token prediction.  More weight space means\n",
    "   more nuanced style encoding with the same r=32 LoRA rank.\n",
    "\n",
    "### Updating the All-Models Comparison\n",
    "\n",
    "To include this model in `AllModels_JordanPeterson_Comparison.ipynb`:\n",
    "\n",
    "1. Add `\"qwen3_32b\"` to `MODEL_KEYS`\n",
    "2. Add the path, display name, and colour to the respective dicts:\n",
    "   ```python\n",
    "   MODEL_PATHS  [\"qwen3_32b\"] = \"./outputs/qwen3_32b_peterson_lora\"\n",
    "   MODEL_DISPLAY[\"qwen3_32b\"] = \"Qwen3-32B  |  Fine-Tuned\"\n",
    "   MODEL_COLORS [\"qwen3_32b\"] = \"#9467BD\"   # purple\n",
    "   MODEL_SHORT  [\"qwen3_32b\"] = \"Qwen3-32B\\nFine-Tuned\"\n",
    "   ```\n",
    "3. Add a Phase 5 inference block (using `generate_response_qwen3`) with cache\n",
    "   file `comparison_cache_all_models/qwen3_32b_results.pkl`\n",
    "4. Update all chart loops — they already iterate over `MODEL_KEYS` so adding\n",
    "   one entry is the only change needed per chart.\n",
    "\n",
    "### Further Improvement Options\n",
    "\n",
    "| Option | Expected gain | Prerequisite |\n",
    "|--------|--------------|-------------|\n",
    "| 5 epochs | Marginal; diminishing returns | Just increase `NUM_EPOCHS` |\n",
    "| `r=64` LoRA | More style capacity | Likely OOMs on 32B; test with care |\n",
    "| More Q&A variety (3 questions/passage) | Broader coverage | Costs ~$1-2 more |\n",
    "| Human-edited Q&A answers | Highest quality ceiling | Manual effort |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
