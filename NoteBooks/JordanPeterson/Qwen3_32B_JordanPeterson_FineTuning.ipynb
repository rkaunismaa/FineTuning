{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e83ae59",
   "metadata": {},
   "source": [
    "# Qwen3-32B Jordan Peterson Fine-Tuning\n",
    "## Maximum Quality on a 24 GB GPU â€” Conservative VRAM Strategy\n",
    "\n",
    "This notebook fine-tunes `unsloth/Qwen3-32B-unsloth-bnb-4bit` on Jordan B.\n",
    "Peterson's four books using the **synthetic Q&A methodology** developed in the\n",
    "Qwen3-14B V2 notebook.\n",
    "\n",
    "The 32B model is the largest dense Qwen3 variant that can plausibly fit on a\n",
    "single 24 GB GPU.  It represents the highest quality ceiling available given\n",
    "the hardware â€” more parameters means more capacity to encode Peterson's\n",
    "distinctive vocabulary, rhetorical structure, and conceptual framing.\n",
    "\n",
    "**Prerequisite**: This notebook uses the same Q&A dataset as the 14B V2\n",
    "notebook.  If you have already run that notebook, the dataset cache at\n",
    "`qa_dataset/peterson_qa.jsonl` will be detected and reused automatically â€”\n",
    "no additional API calls needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe228a",
   "metadata": {},
   "source": [
    "---\n",
    "## Why Qwen3-32B? The Case for a Larger Dense Model\n",
    "\n",
    "### More Parameters = More Stylistic Capacity\n",
    "\n",
    "A language model's LoRA adapter can only encode stylistic patterns in a\n",
    "compressed low-rank subspace of the full weight space.  With r=32, we give\n",
    "the adapter 32 independent directions per attention projection to capture\n",
    "Peterson's style.  But those directions are projected into the *full model\n",
    "weight space*, whose size determines how expressively they can be represented.\n",
    "\n",
    "A 32B model has a hidden dimension of ~5,120 (vs ~5,120 for 14B â€” Qwen3-14B and\n",
    "32B use the same hidden dim but differ in depth and attention heads).  More\n",
    "layers and more attention heads means more places where the LoRA update can\n",
    "reinforce Peterson's vocabulary choices and rhetorical patterns.\n",
    "\n",
    "### Why Dense > MoE for Style Fine-Tuning\n",
    "\n",
    "The other 30B-class model available is `Qwen3-30B-A3B` â€” a Mixture of Experts\n",
    "(MoE) model with 30B total parameters but only **3B active per forward pass**.\n",
    "While MoE models excel at knowledge-intensive tasks, they have a structural\n",
    "disadvantage for stylistic fine-tuning:\n",
    "\n",
    "- Each token is routed to a different subset of experts\n",
    "- Style is a *global* property: Peterson's voice should be consistent across\n",
    "  every token prediction, not just those that happen to route to \"Peterson-experts\"\n",
    "- Fine-tuning MoE with LoRA requires teaching all expert groups consistently,\n",
    "  which needs more training data and more epochs to converge\n",
    "\n",
    "**The Qwen3-32B dense model is the right choice**: every token prediction passes\n",
    "through the same weights, so every training example uniformly teaches the style.\n",
    "\n",
    "### The VRAM Challenge\n",
    "\n",
    "The 32B model is the most capable model that can *potentially* fit on a 24 GB\n",
    "GPU.  It requires careful settings.  The next section explains exactly how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9fc7e",
   "metadata": {},
   "source": [
    "---\n",
    "## VRAM Strategy: Fitting 32B into 24 GB\n",
    "\n",
    "### The Math\n",
    "\n",
    "Our measured baseline from the Qwen3-14B V2 run:\n",
    "\n",
    "| Component | Qwen3-14B V2 | Qwen3-32B (estimate) |\n",
    "|-----------|-------------|---------------------|\n",
    "| 4-bit model weights | ~7.7 GB | ~17.6 GB |\n",
    "| Training overhead* | ~5.7 GB | ~3.0 GB (after reductions) |\n",
    "| **Total peak** | **13.4 GB** (measured) | **~20â€“22 GB** (estimated) |\n",
    "\n",
    "*Training overhead = activations + KV cache + LoRA adapter + 8-bit optimizer states\n",
    "\n",
    "The overhead scales with `batch_size Ã— max_seq_length`, so halving both\n",
    "roughly halves the overhead:\n",
    "\n",
    "### Four Levers We Pull\n",
    "\n",
    "| Setting | V1 14B | V2 14B | **32B** | Saving vs 14B V2 |\n",
    "|---------|--------|--------|---------|-----------------|\n",
    "| `per_device_train_batch_size` | 2 | 2 | **1** | ~2.5 GB |\n",
    "| `max_seq_length` | 2048 | 2048 | **1024** | ~1.5 GB |\n",
    "| `gradient_accumulation_steps` | 4 | 4 | **8** | compensates for batch=1 |\n",
    "| `gradient_checkpointing` | unsloth | unsloth | unsloth | already maximised |\n",
    "\n",
    "**Effective batch size stays at 8**: `batch_size=1 Ã— grad_accum=8 = 8`\n",
    "The *gradient quality* is identical to V2; we just spread it over more steps.\n",
    "\n",
    "### What Happens if It Still OOMs\n",
    "\n",
    "Despite these reductions, 24 GB is a hard limit.  If CUDA raises an\n",
    "out-of-memory error, this notebook will catch it and print an explicit\n",
    "fallback plan rather than dying silently.  The training cell is wrapped in a\n",
    "`try/except` so you get actionable guidance immediately.\n",
    "\n",
    "### Sequence Length Adequacy\n",
    "\n",
    "Our Q&A pairs are:\n",
    "- Question: ~20 words â†’ ~30 tokens\n",
    "- Answer (Peterson passage): ~350 words â†’ ~500 tokens\n",
    "- System prompt + ChatML tokens: ~150 tokens\n",
    "- **Total per example: ~680 tokens**\n",
    "\n",
    "`max_seq_length=1024` covers 99%+ of examples with comfortable headroom.\n",
    "Only the very longest passages (statistical tail) will be truncated, and\n",
    "the truncation is applied to the end of the passage â€” the style-bearing\n",
    "content is in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69c048",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Q&A Dataset\n",
    "\n",
    "This section is identical to the Qwen3-14B V2 notebook.  The Q&A cache at\n",
    "`qa_dataset/peterson_qa.jsonl` is **shared** between the 14B and 32B notebooks\n",
    "â€” if you have already run V2, this entire section will complete in seconds\n",
    "by loading from cache.  If not, it will call the Claude Haiku API and generate\n",
    "the dataset (~$1â€“3, ~15â€“20 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbd592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rob/PythonEnvironments/FineTuning/.finetuning/bin/python: No module named pip\n",
      "\n",
      "anthropic : 0.83.0\n",
      "API key found.\n",
      "\n",
      "Q&A cache found: 5,029 records â€” generation will be skipped.\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "_result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"anthropic\", \"-q\"],\n",
    "    capture_output=True, text=True,\n",
    ")\n",
    "print(\"anthropic SDK ready.\" if _result.returncode == 0 else _result.stderr[:300])\n",
    "\n",
    "import os, re, json, time, math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import fitz\n",
    "import anthropic\n",
    "\n",
    "print(f\"anthropic : {anthropic.__version__}\")\n",
    "\n",
    "# â”€â”€ API key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_api_key = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")\n",
    "if not _api_key:\n",
    "    _env_file = Path.home() / \".env\"\n",
    "    if _env_file.exists():\n",
    "        for _line in _env_file.read_text().splitlines():\n",
    "            if _line.startswith(\"ANTHROPIC_API_KEY=\"):\n",
    "                _api_key = _line.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
    "                break\n",
    "if not _api_key:\n",
    "    raise EnvironmentError(\n",
    "        \"ANTHROPIC_API_KEY not found.\\n\"\n",
    "        \"Set it with:  export ANTHROPIC_API_KEY='sk-ant-...'\\n\"\n",
    "        \"Or add it to ~/.env as:  ANTHROPIC_API_KEY=sk-ant-...\"\n",
    "    )\n",
    "print(\"API key found.\")\n",
    "\n",
    "BOOKS_DIR        = Path(\"../../Books/JordanPeterson\")\n",
    "QA_DIR           = Path(\"./qa_dataset\")\n",
    "QA_CACHE         = QA_DIR / \"peterson_qa.jsonl\"   # shared with 14B V2 notebook\n",
    "QA_DIR.mkdir(exist_ok=True)\n",
    "GENERATION_MODEL = \"claude-haiku-4-5-20251001\"\n",
    "\n",
    "if QA_CACHE.exists():\n",
    "    with open(QA_CACHE) as f:\n",
    "        _n = sum(1 for line in f if line.strip())\n",
    "    print(f\"\\nQ&A cache found: {_n:,} records â€” generation will be skipped.\")\n",
    "else:\n",
    "    print(\"\\nQ&A cache not found â€” will generate from PDFs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1acf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from 4 PDFsâ€¦\n",
      "  Maps of Meaning                       914 chunks\n",
      "  12 Rules for Life                     487 chunks\n",
      "  Beyond Order                          431 chunks\n",
      "  We Who Wrestle with God               687 chunks\n",
      "\n",
      "Total: 2,519 passages\n"
     ]
    }
   ],
   "source": [
    "def clean_text(raw: str) -> str:\n",
    "    \"\"\"Strip PDF artefacts: control chars, ligatures, excess whitespace.\"\"\"\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', raw)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.replace('\\ufb01', 'fi').replace('\\ufb02', 'fl').strip()\n",
    "\n",
    "\n",
    "def extract_chunks(pdf_path: Path, chunk_words: int = 350,\n",
    "                   overlap_words: int = 50) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract clean text from a PDF and split into overlapping word-level chunks.\n",
    "\n",
    "    chunk_words=350 produces passages that fit well within max_seq_length=1024\n",
    "    after applying the ChatML system+user template overhead (~150 tokens).\n",
    "    overlap_words=50 ensures no passage starts mid-sentence.\n",
    "    \"\"\"\n",
    "    doc   = fitz.open(str(pdf_path))\n",
    "    pages = [clean_text(page.get_text()) for page in doc]\n",
    "    doc.close()\n",
    "    words  = ' '.join(pages).split()\n",
    "    step   = chunk_words - overlap_words\n",
    "    return [\n",
    "        ' '.join(words[s : s + chunk_words])\n",
    "        for s in range(0, len(words), step)\n",
    "        if len(words[s : s + chunk_words]) >= 100\n",
    "    ]\n",
    "\n",
    "\n",
    "pdf_files  = sorted(BOOKS_DIR.glob(\"*.pdf\"))\n",
    "all_chunks = []\n",
    "chunk_meta = []\n",
    "\n",
    "print(f\"Extracting from {len(pdf_files)} PDFsâ€¦\")\n",
    "for pdf in pdf_files:\n",
    "    fname = pdf.name.lower()\n",
    "    if   \"maps\"    in fname: label = \"Maps of Meaning\"\n",
    "    elif \"12 rules\" in fname or \"antidote\" in fname: label = \"12 Rules for Life\"\n",
    "    elif \"beyond\"  in fname: label = \"Beyond Order\"\n",
    "    else:                    label = \"We Who Wrestle with God\"\n",
    "\n",
    "    chunks = extract_chunks(pdf)\n",
    "    all_chunks.extend(chunks)\n",
    "    chunk_meta.extend([{\"book\": label}] * len(chunks))\n",
    "    print(f\"  {label:<35}  {len(chunks):4d} chunks\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_chunks):,} passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc20786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_questions() defined.\n"
     ]
    }
   ],
   "source": [
    "client = anthropic.Anthropic(api_key=_api_key)\n",
    "\n",
    "\n",
    "def generate_questions(passage: str, book: str, max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Call Claude Haiku to generate 2 questions answered by this passage.\n",
    "\n",
    "    Returns only the questions (not answers) â€” the passage itself is used as\n",
    "    the answer in the training dataset.  This keeps output tokens minimal\n",
    "    and cost low (~$0.00068 per passage, ~$0.82 for all 1,200 passages).\n",
    "\n",
    "    Retries with exponential backoff on API errors or malformed JSON.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"You are building a training dataset for a Jordan B. Peterson AI model.\\n\\n\"\n",
    "        f\"The passage below is from Peterson's book '{book}'.  Generate exactly 2 questions that:\\n\"\n",
    "        f\"1. This passage directly and substantively answers\\n\"\n",
    "        f\"2. Someone interested in Peterson's ideas might genuinely ask\\n\"\n",
    "        f\"3. Cover different angles of the passage (e.g. one concrete, one philosophical)\\n\\n\"\n",
    "        f\"Peterson's topics: order vs chaos, meaning, personal responsibility, suffering, \"\n",
    "        f\"mythology, archetypes, shadow, logos, truth, religion, Jungian psychology, \"\n",
    "        f\"hierarchy, heroism, sacrifice, being.\\n\\n\"\n",
    "        f\"Return ONLY a JSON array of exactly 2 question strings.  No other text.\\n\"\n",
    "        f\"Example: [\\\"Why is confronting chaos necessary for meaning?\\\", \"\n",
    "        f\"\\\"What role does suffering play in personal development?\\\"]\\n\\n\"\n",
    "        f\"Passage:\\n{passage}\"\n",
    "    )\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp  = client.messages.create(\n",
    "                model=GENERATION_MODEL, max_tokens=150,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            raw   = resp.content[0].text.strip()\n",
    "            match = re.search(r'\\[.*?\\]', raw, re.DOTALL)\n",
    "            if not match:\n",
    "                raise ValueError(f\"No JSON array in: {raw[:80]}\")\n",
    "            qs = json.loads(match.group())\n",
    "            return [str(q).strip() for q in qs[:2]]\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    [retry {attempt+1}] {e} â€” wait {wait}s\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"    [FAILED] passage skipped\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "print(\"generate_questions() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed11d8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache coverage: 5,029 / 5,038  (99.8%)\n",
      "Cache â‰¥90% complete â€” skipping generation.\n",
      "  (Delete qa_dataset/peterson_qa.jsonl to force regeneration)\n",
      "Total Q&A pairs: 5,029\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Check existing cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "existing = []\n",
    "if QA_CACHE.exists():\n",
    "    with open(QA_CACHE) as f:\n",
    "        existing = [json.loads(l) for l in f if l.strip()]\n",
    "\n",
    "_expected = len(all_chunks) * 2\n",
    "_coverage = len(existing) / _expected if _expected else 0\n",
    "print(f\"Cache coverage: {len(existing):,} / {_expected:,}  ({100*_coverage:.1f}%)\")\n",
    "\n",
    "if _coverage >= 0.90:\n",
    "    print(\"Cache â‰¥90% complete â€” skipping generation.\")\n",
    "    print(f\"  (Delete {QA_CACHE} to force regeneration)\")\n",
    "else:\n",
    "    already_done   = len(existing) // 2\n",
    "    remaining      = all_chunks[already_done:]\n",
    "    remaining_meta = chunk_meta[already_done:]\n",
    "    print(f\"Generating {len(remaining):,} passages (from #{already_done+1})â€¦\")\n",
    "    print(f\"Estimated cost: ~${len(remaining)*0.00068:.2f}\\n\")\n",
    "\n",
    "    skipped = 0\n",
    "    with open(QA_CACHE, \"a\") as out_f:\n",
    "        for i, (passage, meta) in enumerate(zip(remaining, remaining_meta)):\n",
    "            if i % 50 == 0:\n",
    "                pct = 100 * (already_done + i) / len(all_chunks)\n",
    "                print(f\"  [{already_done+i+1:4d}/{len(all_chunks):4d}]  \"\n",
    "                      f\"{pct:5.1f}%  {meta['book']}\")\n",
    "            questions = generate_questions(passage, meta[\"book\"])\n",
    "            if not questions:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            for q in questions:\n",
    "                out_f.write(json.dumps({\"question\": q, \"answer\": passage,\n",
    "                                        \"book\": meta[\"book\"]}) + \"\\n\")\n",
    "            out_f.flush()\n",
    "            time.sleep(0.3)\n",
    "    print(f\"\\nDone.  Processed: {len(remaining)-skipped:,}  Skipped: {skipped}\")\n",
    "\n",
    "with open(QA_CACHE) as f:\n",
    "    qa_records = [json.loads(l) for l in f if l.strip()]\n",
    "print(f\"Total Q&A pairs: {len(qa_records):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253cdc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A pairs by book:\n",
      "  Maps of Meaning                      1826  (36.3%)\n",
      "  We Who Wrestle with God              1367  (27.2%)\n",
      "  12 Rules for Life                     974  (19.4%)\n",
      "  Beyond Order                          862  (17.1%)\n",
      "\n",
      "Answer length: min=189  max=350  mean=350 words\n",
      "Estimated mean tokens per example: ~670  (limit: 1024 â€” OK)\n"
     ]
    }
   ],
   "source": [
    "book_counts = Counter(r[\"book\"] for r in qa_records)\n",
    "print(\"Q&A pairs by book:\")\n",
    "for book, count in sorted(book_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {book:<35}  {count:4d}  ({100*count/len(qa_records):.1f}%)\")\n",
    "\n",
    "ans_len = [len(r[\"answer\"].split()) for r in qa_records]\n",
    "print(f\"\\nAnswer length: min={min(ans_len)}  max={max(ans_len)}  \"\n",
    "      f\"mean={sum(ans_len)/len(ans_len):.0f} words\")\n",
    "\n",
    "# Token budget sanity check\n",
    "# ~1.4 tokens/word Ã— mean answer + ~180 tokens overhead < 1024 limit\n",
    "mean_tokens_est = sum(ans_len) / len(ans_len) * 1.4 + 180\n",
    "print(f\"Estimated mean tokens per example: ~{mean_tokens_est:.0f}  \"\n",
    "      f\"(limit: 1024 â€” {'OK' if mean_tokens_est < 900 else 'WARNING: close to limit'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45d34a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Fine-Tuning Qwen3-32B\n",
    "\n",
    "## Configuration\n",
    "\n",
    "| Parameter | Qwen3-14B V2 | **Qwen3-32B** | Rationale |\n",
    "|-----------|-------------|--------------|-----------|\n",
    "| Base model | `Qwen3-14B-unsloth-bnb-4bit` | **`Qwen3-32B-unsloth-bnb-4bit`** | 2.3Ã— more parameters |\n",
    "| Training data | Synthetic Q&A | Same | Shared cache |\n",
    "| Epochs | 3 | **3** | Same generalisation target |\n",
    "| LoRA rank | r=32 | **r=32** | Same adapter capacity ratio |\n",
    "| LoRA alpha | 32 | **32** | alpha/rank = 1.0 |\n",
    "| Batch size | 2 | **1** | Primary VRAM reduction |\n",
    "| Gradient accumulation | 4 | **8** | Keeps effective batch = 8 |\n",
    "| Max seq length | 2048 | **1024** | Secondary VRAM reduction |\n",
    "| Warmup steps | 30 | **30** | Same absolute warmup |\n",
    "| LR | 2e-4 | **2e-4** | Standard Qwen3 LoRA |\n",
    "| Estimated peak VRAM | ~13.4 GB | **~20â€“22 GB** | Right up to the 24 GB limit |\n",
    "| Output dir | `qwen3_14b_peterson_v2_lora` | **`qwen3_32b_peterson_lora`** | Separate adapter |\n",
    "\n",
    "## Risk Acknowledgement\n",
    "\n",
    "This notebook will attempt training.  If the GPU runs out of memory, a\n",
    "structured error handler will print an explicit fallback plan rather than\n",
    "a raw CUDA traceback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc7b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "GPU      : NVIDIA GeForce RTX 4090\n",
      "VRAM     : 25.3 GB total\n",
      "\n",
      "Configuration:\n",
      "  Model            : unsloth/Qwen3-32B-unsloth-bnb-4bit\n",
      "  max_seq_length   : 1024  (halved vs 14B V2 to save VRAM)\n",
      "  batch_size       : 1   (halved vs 14B V2)\n",
      "  grad_accum       : 8   (doubled â€” effective batch = 8)\n",
      "  epochs           : 3\n",
      "  LoRA             : r=32, alpha=32\n",
      "  Output           : /home/rob/PythonEnvironments/FineTuning/FineTuning/NoteBooks/JordanPeterson/outputs/qwen3_32b_peterson_lora\n"
     ]
    }
   ],
   "source": [
    "import os, re, gc, math, json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "QA_CACHE   = Path(\"./qa_dataset/peterson_qa.jsonl\")\n",
    "OUTPUT_DIR = Path(\"./outputs/qwen3_32b_peterson_lora\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_MODEL  = \"unsloth/Qwen3-32B-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LEN = 1024    # halved from 2048 â€” single biggest VRAM lever\n",
    "\n",
    "# â”€â”€ LoRA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LORA_RANK  = 32       # same as 14B V2; adapter capacity ratio is identical\n",
    "LORA_ALPHA = 32       # alpha/rank = 1.0\n",
    "\n",
    "# â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BATCH_SIZE    = 1     # halved â€” primary VRAM saving\n",
    "GRAD_ACCUM    = 8     # doubled â€” effective batch stays at 8\n",
    "NUM_EPOCHS    = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS  = 30\n",
    "WEIGHT_DECAY  = 0.01\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an AI assistant that has been trained on the complete works of \"\n",
    "    \"Jordan B. Peterson, a Canadian clinical psychologist, professor, and author. \"\n",
    "    \"You speak with deep knowledge of psychology, philosophy, mythology, religion, \"\n",
    "    \"and personal responsibility.  Your responses reflect Peterson's writing style, \"\n",
    "    \"intellectual depth, and interdisciplinary approach to understanding human \"\n",
    "    \"nature and meaning.\"\n",
    ")\n",
    "\n",
    "# â”€â”€ Hardware summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f\"GPU      : {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM     : {total_vram:.1f} GB total\")\n",
    "print()\n",
    "if total_vram < 23.5:\n",
    "    print(f\"WARNING: This GPU has {total_vram:.1f} GB VRAM.\")\n",
    "    print(\"Qwen3-32B requires ~20-22 GB during training.\")\n",
    "    print(\"Proceeding, but OOM is possible â€” see the training cell for fallback.\")\n",
    "    print()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model            : {BASE_MODEL}\")\n",
    "print(f\"  max_seq_length   : {MAX_SEQ_LEN}  (halved vs 14B V2 to save VRAM)\")\n",
    "print(f\"  batch_size       : {BATCH_SIZE}   (halved vs 14B V2)\")\n",
    "print(f\"  grad_accum       : {GRAD_ACCUM}   (doubled â€” effective batch = {BATCH_SIZE*GRAD_ACCUM})\")\n",
    "print(f\"  epochs           : {NUM_EPOCHS}\")\n",
    "print(f\"  LoRA             : r={LORA_RANK}, alpha={LORA_ALPHA}\")\n",
    "print(f\"  Output           : {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606c6ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 5,029 Q&A pairs  |  columns: ['question', 'answer']\n"
     ]
    }
   ],
   "source": [
    "if not QA_CACHE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Q&A cache not found at {QA_CACHE}.\\nRun Part 1 first.\"\n",
    "    )\n",
    "\n",
    "with open(QA_CACHE) as f:\n",
    "    records = [json.loads(l) for l in f if l.strip()]\n",
    "\n",
    "raw_dataset = Dataset.from_list([\n",
    "    {\"question\": r[\"question\"], \"answer\": r[\"answer\"]}\n",
    "    for r in records\n",
    "])\n",
    "\n",
    "print(f\"Dataset: {len(raw_dataset):,} Q&A pairs  |  columns: {raw_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9031b",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Load Qwen3-32B\n",
    "\n",
    "This is the most memory-intensive step.  The 4-bit quantized weights alone\n",
    "occupy ~17.6 GB.  We monitor VRAM before and after loading so we know exactly\n",
    "how much headroom remains for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610fadb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free VRAM before model load : 25.2 GB\n",
      "\n",
      "Loading unsloth/Qwen3-32B-unsloth-bnb-4bit â€¦\n",
      "(This downloads ~18 GB on first run â€” may take several minutes.)\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen3 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.516 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7408f0472e924827988362eb6453a134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 85.81 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 16.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m(This downloads ~18 GB on first run â€” may take several minutes.)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# auto: bfloat16 on Ampere+\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 1024 â€” critical for VRAM management\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 4-bit quantization via bitsandbytes\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_finetuning\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# force all layers onto GPU 0; avoids\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m                                     \u001b[49m\u001b[38;5;66;43;03m# transformers auto-routing layers to CPU\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m                                     \u001b[49m\u001b[38;5;66;43;03m# when its conservative memory estimate\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m                                     \u001b[49m\u001b[38;5;66;43;03m# slightly overestimates requirements\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m vram_after = torch.cuda.memory_reserved() / \u001b[32m1e9\u001b[39m\n\u001b[32m     28\u001b[39m vram_free  = total_vram - vram_after\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/unsloth/models/loader.py:602\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, unsloth_tiled_mlp, *args, **kwargs)\u001b[39m\n\u001b[32m    599\u001b[39m     load_in_4bit_kwargs = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    600\u001b[39m     load_in_8bit_kwargs = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m model, tokenizer = \u001b[43mdispatch_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_fp8\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_fp8\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    627\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/unsloth/models/qwen3.py:444\u001b[39m, in \u001b[36mFastQwen3Model.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(  \u001b[38;5;66;03m# TODO: Change after release\u001b[39;00m\n\u001b[32m    431\u001b[39m     model_name = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-7B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    442\u001b[39m     **kwargs,\n\u001b[32m    443\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mFastQwen3Model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/unsloth/models/llama.py:2419\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, qat_scheme, load_in_fp8, **kwargs)\u001b[39m\n\u001b[32m   2406\u001b[39m     model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m   2407\u001b[39m         model_name,\n\u001b[32m   2408\u001b[39m         device_map = device_map,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2416\u001b[39m         **kwargs,\n\u001b[32m   2417\u001b[39m     )\n\u001b[32m   2418\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[32m-> \u001b[39m\u001b[32m2419\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# torch_dtype             = dtype, # transformers changed torch_dtype to dtype\u001b[39;49;00m\n\u001b[32m   2423\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[32m   2424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_attn_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2428\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2429\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2430\u001b[39m     model.fast_generate = make_fast_generate_wrapper(model.generate)\n\u001b[32m   2431\u001b[39m     model.fast_generate_batches = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/transformers/modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/transformers/modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonEnvironments/FineTuning/.finetuning/lib/python3.12/site-packages/transformers/modeling_utils.py:748\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    740\u001b[39m         hf_quantizer.create_quantized_param(\n\u001b[32m    741\u001b[39m             model,\n\u001b[32m    742\u001b[39m             param,\n\u001b[32m   (...)\u001b[39m\u001b[32m    745\u001b[39m             **sharding_kwargs,\n\u001b[32m    746\u001b[39m         )\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     param = \u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    750\u001b[39m         param = param.to(casting_dtype)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 85.81 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 16.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Pre-load VRAM check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Clear any stale allocations from previous runs in this kernel session\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "free_before = (torch.cuda.get_device_properties(0).total_memory\n",
    "               - torch.cuda.memory_reserved()) / 1e9\n",
    "print(f\"Free VRAM before model load : {free_before:.1f} GB\")\n",
    "if free_before < 17.0:\n",
    "    print(\"WARNING: less than 17 GB free â€” model weights may not fit.\")\n",
    "    print(\"Restart the kernel to free all allocations and try again.\")\n",
    "\n",
    "print(f\"\\nLoading {BASE_MODEL} â€¦\")\n",
    "print(\"(This downloads ~18 GB on first run â€” may take several minutes.)\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    dtype           = None,          # auto: bfloat16 on Ampere+\n",
    "    max_seq_length  = MAX_SEQ_LEN,   # 1024 â€” critical for VRAM management\n",
    "    load_in_4bit    = True,          # 4-bit quantization via bitsandbytes\n",
    "    full_finetuning = False,\n",
    "    device_map      = {\"\": 0},       # force all layers onto GPU 0; avoids\n",
    "                                     # transformers auto-routing layers to CPU\n",
    "                                     # when its conservative memory estimate\n",
    "                                     # slightly overestimates requirements\n",
    ")\n",
    "\n",
    "vram_after = torch.cuda.memory_reserved() / 1e9\n",
    "vram_free  = total_vram - vram_after\n",
    "print(f\"\\nModel loaded.\")\n",
    "print(f\"  VRAM reserved : {vram_after:.1f} GB\")\n",
    "print(f\"  VRAM free     : {vram_free:.1f} GB  (training needs ~3-5 GB more)\")\n",
    "if vram_free < 3.0:\n",
    "    print(\"  WARNING: very little headroom â€” training will likely OOM.\")\n",
    "    print(\"  Consider reducing max_seq_length to 768 before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253a69c",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Add LoRA Adapters\n",
    "\n",
    "Same configuration as the 14B V2 notebook: r=32, alpha=32, all attention and\n",
    "MLP projections targeted.  The adapter size scales with `d_model Ã— r`, and\n",
    "Qwen3-32B's larger hidden dimension (~5,120) means the r=32 adapter encodes\n",
    "more expressive style directions than the same rank on a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                          = LORA_RANK,\n",
    "    lora_alpha                 = LORA_ALPHA,\n",
    "    lora_dropout               = 0,\n",
    "    target_modules             = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    use_gradient_checkpointing = \"unsloth\",   # saves ~30% VRAM vs PyTorch default\n",
    "    random_state               = 42,\n",
    "    use_rslora                 = False,\n",
    "    loftq_config               = None,\n",
    ")\n",
    "\n",
    "total_p     = sum(p.numel() for p in model.parameters())\n",
    "trainable_p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters    : {total_p:,}\")\n",
    "print(f\"Trainable (LoRA)    : {trainable_p:,}  ({100*trainable_p/total_p:.4f}%)\")\n",
    "print(f\"VRAM after LoRA     : {torch.cuda.memory_reserved()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cf44c",
   "metadata": {},
   "source": [
    "---\n",
    "## Steps 5â€“6: Format Dataset and Apply Response Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71542114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(batch):\n",
    "    \"\"\"\n",
    "    Convert Q&A pairs to Qwen3 ChatML format.\n",
    "\n",
    "    The training signal is entirely in the assistant (answer) tokens.\n",
    "    The system prompt and user question are masked out by train_on_responses_only\n",
    "    so the model only learns what to say, not the structural tokens around it.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for q, a in zip(batch[\"question\"], batch[\"answer\"]):\n",
    "        conv = [\n",
    "            {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",      \"content\": q},\n",
    "            {\"role\": \"assistant\", \"content\": a},\n",
    "        ]\n",
    "        texts.append(tokenizer.apply_chat_template(\n",
    "            conv, tokenize=False, add_generation_prompt=False,\n",
    "            enable_thinking=False,\n",
    "        ))\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "dataset = raw_dataset.map(\n",
    "    format_example, batched=True, batch_size=256,\n",
    "    remove_columns=raw_dataset.column_names,\n",
    ")\n",
    "print(f\"Formatted: {len(dataset):,} examples\")\n",
    "\n",
    "# Verify token budget: show how many examples fit within max_seq_length\n",
    "sample_ids = tokenizer(dataset[0][\"text\"])[\"input_ids\"]\n",
    "print(f\"Sample token count  : {len(sample_ids)} (limit: {MAX_SEQ_LEN})\")\n",
    "if len(sample_ids) > MAX_SEQ_LEN:\n",
    "    print(\"WARNING: first sample exceeds max_seq_length â€” it will be truncated.\")\n",
    "\n",
    "# â”€â”€ Detect response boundary tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_sample = dataset[0][\"text\"]\n",
    "if \"<|im_start|>assistant\\n\" in _sample:\n",
    "    instruction_part = \"<|im_start|>user\\n\"\n",
    "    response_part    = \"<|im_start|>assistant\\n\"\n",
    "else:\n",
    "    raise RuntimeError(\"Could not detect Qwen3 ChatML tokens in formatted data.\")\n",
    "print(f\"\\nResponse boundary   : {repr(response_part)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb048aa",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Configure SFTTrainer\n",
    "\n",
    "The key difference from the 14B V2 notebook is `per_device_train_batch_size=1`\n",
    "and `gradient_accumulation_steps=8`.  The optimizer sees the same quality of\n",
    "gradient signal (effective batch = 8) but processes one example at a time,\n",
    "which halves the peak activation memory during each forward-backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model         = model,\n",
    "    tokenizer     = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args          = SFTConfig(\n",
    "        dataset_text_field          = \"text\",\n",
    "        max_seq_length              = MAX_SEQ_LEN,\n",
    "        dataset_num_proc            = 2,\n",
    "\n",
    "        per_device_train_batch_size = BATCH_SIZE,   # 1 â€” primary VRAM saving\n",
    "        gradient_accumulation_steps = GRAD_ACCUM,   # 8 â€” effective batch = 8\n",
    "        num_train_epochs            = NUM_EPOCHS,   # 3\n",
    "\n",
    "        learning_rate               = LEARNING_RATE,\n",
    "        warmup_steps                = WARMUP_STEPS,\n",
    "        lr_scheduler_type           = \"cosine\",\n",
    "        weight_decay                = WEIGHT_DECAY,\n",
    "\n",
    "        optim                       = \"adamw_8bit\",\n",
    "        fp16                        = not torch.cuda.is_bf16_supported(),\n",
    "        bf16                        = torch.cuda.is_bf16_supported(),\n",
    "\n",
    "        logging_steps               = 25,\n",
    "        save_strategy               = \"epoch\",\n",
    "        output_dir                  = str(OUTPUT_DIR),\n",
    "        report_to                   = \"none\",\n",
    "        seed                        = 42,\n",
    "        packing                     = False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "sft_trainer = train_on_responses_only(\n",
    "    sft_trainer,\n",
    "    instruction_part = instruction_part,\n",
    "    response_part    = response_part,\n",
    ")\n",
    "\n",
    "# â”€â”€ Estimate total training steps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_steps = math.ceil(len(dataset) / BATCH_SIZE) * NUM_EPOCHS // GRAD_ACCUM\n",
    "print(f\"Estimated gradient updates : {_steps:,}\")\n",
    "print(f\"  = {len(dataset):,} examples / batch {BATCH_SIZE} Ã— accum {GRAD_ACCUM} Ã— {NUM_EPOCHS} epochs\")\n",
    "print()\n",
    "print(\"Ready to train.  Current VRAM state:\")\n",
    "print(f\"  Reserved : {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "print(f\"  Free     : {(total_vram - torch.cuda.memory_reserved()/1e9):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be12b1",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Train\n",
    "\n",
    "Training is wrapped in a structured error handler.  If CUDA runs out of memory\n",
    "the handler prints an explicit fallback plan â€” no raw traceback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9859e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(f\"VRAM before training  : {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "print(f\"Starting â€” {NUM_EPOCHS} epochs, ~{_steps:,} gradient updatesâ€¦\")\n",
    "print()\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "try:\n",
    "    train_result = sft_trainer.train()\n",
    "    elapsed_min  = (time.time() - t0) / 60\n",
    "    vram_peak    = torch.cuda.max_memory_reserved() / 1e9\n",
    "\n",
    "    print()\n",
    "    print(\"â•\" * 65)\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(\"â•\" * 65)\n",
    "    print(f\"  Gradient updates   : {train_result.global_step:,}\")\n",
    "    print(f\"  Training loss      : {train_result.training_loss:.4f}\")\n",
    "    print(f\"  Elapsed            : {elapsed_min:.1f} min\")\n",
    "    print(f\"  Peak VRAM          : {vram_peak:.1f} GB  /  {total_vram:.1f} GB\")\n",
    "    print()\n",
    "    print(\"Reference â€” Qwen3-14B V2 (same data, same epochs):\")\n",
    "    print(\"  Expected: ~900 steps  |  ~90 min  |  loss ~2.2-2.4  |  ~13.5 GB\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    elapsed_min = (time.time() - t0) / 60\n",
    "    err_str     = str(e).lower()\n",
    "\n",
    "    if \"out of memory\" in err_str or \"cuda\" in err_str:\n",
    "        print()\n",
    "        print(\"â•”\" + \"â•\" * 63 + \"â•—\")\n",
    "        print(\"â•‘  OUT OF MEMORY â€” Qwen3-32B exceeded 24 GB VRAM              â•‘\")\n",
    "        print(\"â•š\" + \"â•\" * 63 + \"â•\")\n",
    "        print()\n",
    "        print(f\"  Failed after {elapsed_min:.1f} min.\")\n",
    "        print(f\"  Peak VRAM before OOM: {torch.cuda.max_memory_reserved()/1e9:.1f} GB\")\n",
    "        print()\n",
    "        print(\"FALLBACK OPTIONS (in order of recommendation):\")\n",
    "        print()\n",
    "        print(\"  Option 1 â€” Reduce max_seq_length to 768 and retry:\")\n",
    "        print(\"    â€¢ Change MAX_SEQ_LEN = 768 in the config cell\")\n",
    "        print(\"    â€¢ Restart the kernel and rerun from Step 3\")\n",
    "        print(\"    â€¢ ~95% of Q&A examples fit within 768 tokens\")\n",
    "        print()\n",
    "        print(\"  Option 2 â€” Use Qwen3-30B-A3B (MoE, fits ~18-20 GB):\")\n",
    "        print(\"    â€¢ Model: unsloth/Qwen3-30B-A3B-bnb-4bit\")\n",
    "        print(\"    â€¢ 30B total params, 3B active per step â€” lower activation memory\")\n",
    "        print(\"    â€¢ Change BASE_MODEL and OUTPUT_DIR in the config cell\")\n",
    "        print(\"    â€¢ Keep all other settings the same\")\n",
    "        print()\n",
    "        print(\"  Option 3 â€” Fall back to Qwen3-14B V2:\")\n",
    "        print(\"    â€¢ Proven to work at 13.4 GB peak\")\n",
    "        print(\"    â€¢ Notebook: Qwen3_14B_JordanPeterson_V2_FineTuning.ipynb\")\n",
    "        print()\n",
    "        # Free VRAM so the user can retry without a kernel restart if possible\n",
    "        del sft_trainer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"  VRAM after cleanup: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "    else:\n",
    "        raise   # re-raise non-OOM errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0087b5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Save the LoRA Adapter\n",
    "\n",
    "The r=32 adapter for a 32B model will be larger than for 14B â€” approximately\n",
    "proportional to the number of target projection matrices multiplied by r Ã— d_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving adapter to {OUTPUT_DIR} â€¦\")\n",
    "model.save_pretrained(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "\n",
    "adapter_files = list(OUTPUT_DIR.glob(\"*.safetensors\")) + list(OUTPUT_DIR.glob(\"*.bin\"))\n",
    "total_mb = sum(f.stat().st_size for f in adapter_files) / 1e6\n",
    "print(f\"\\nAdapter files:\")\n",
    "for f in sorted(adapter_files):\n",
    "    print(f\"  {f.name}  ({f.stat().st_size/1e6:.1f} MB)\")\n",
    "print(f\"\\nTotal: {total_mb:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece974c",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Test Inference\n",
    "\n",
    "Run the same 5 evaluation prompts used across all comparison notebooks.\n",
    "Results here give a first qualitative check of response quality before\n",
    "running the full quantitative comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    \"Why is personal responsibility the foundation of a meaningful life?\",\n",
    "    \"How do ancient myths and stories reveal truths about human nature?\",\n",
    "    \"What does it mean to pursue what is meaningful rather than what is expedient?\",\n",
    "    \"How should a person confront suffering rather than flee from it?\",\n",
    "]\n",
    "\n",
    "\n",
    "def ask(question: str, max_new_tokens: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the fine-tuned 32B model.\n",
    "\n",
    "    Identical to the 14B inference wrapper: Qwen3 two-step tokenisation,\n",
    "    enable_thinking=False, greedy decoding (do_sample=False) for\n",
    "    deterministic outputs comparable across notebooks.\n",
    "    \"\"\"\n",
    "    msgs  = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "             {\"role\": \"user\",   \"content\": question}]\n",
    "    text  = tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True, enable_thinking=False,\n",
    "    )\n",
    "    inp   = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inp, max_new_tokens=max_new_tokens,\n",
    "            do_sample=False, temperature=1.0, repetition_penalty=1.1,\n",
    "        )\n",
    "    resp = tokenizer.decode(out[0][inp[\"input_ids\"].shape[1]:],\n",
    "                             skip_special_tokens=True).strip()\n",
    "    return re.sub(r'<think>.*?</think>', '', resp, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "print(\"Testing 32B fine-tuned model (greedy decoding)â€¦\\n\")\n",
    "for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(f\"Q{i+1}: {prompt}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    answer = ask(prompt)\n",
    "    print(answer if answer.strip() else \"(empty response)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f7b02",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Thinking Mode Test\n",
    "\n",
    "The 32B model's chain-of-thought reasoning is considerably more capable than\n",
    "the 14B model's.  Testing it here shows whether the larger model's reasoning\n",
    "capacity is preserved after fine-tuning and how it combines with Peterson's\n",
    "domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_thinking(question: str, max_new_tokens: int = 600) -> tuple[str, str]:\n",
    "    \"\"\"Return (thinking_content, final_answer) using Qwen3 reasoning mode.\"\"\"\n",
    "    msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",   \"content\": question}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True, enable_thinking=True,\n",
    "    )\n",
    "    inp = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inp, max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, temperature=0.6, top_p=0.95, top_k=20,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    full  = tokenizer.decode(out[0][inp[\"input_ids\"].shape[1]:],\n",
    "                              skip_special_tokens=True).strip()\n",
    "    match = re.search(r'<think>(.*?)</think>', full, re.DOTALL)\n",
    "    think = match.group(1).strip() if match else \"\"\n",
    "    ans   = re.sub(r'<think>.*?</think>', '', full, flags=re.DOTALL).strip()\n",
    "    return think, ans\n",
    "\n",
    "\n",
    "q = \"What is the relationship between order and chaos in human experience?\"\n",
    "print(f\"Q: {q}\\n\")\n",
    "thinking, answer = ask_thinking(q)\n",
    "if thinking:\n",
    "    print(f\"[Chain-of-Thought]\\n{thinking[:500]}{'â€¦' if len(thinking)>500 else ''}\\n\")\n",
    "print(f\"[Answer]\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f838ce4",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "### What This Run Achieved\n",
    "\n",
    "A Qwen3-32B model fine-tuned on synthetic Q&A pairs from Peterson's four books\n",
    "for 3 epochs represents the **highest achievable quality** for this domain on\n",
    "a single 24 GB GPU.\n",
    "\n",
    "The three improvements that drive this quality ceiling:\n",
    "\n",
    "1. **Synthetic Q&A data** â€” the model learned to answer questions in Peterson's\n",
    "   voice, not just continue passages.  The training task matches the inference task.\n",
    "\n",
    "2. **3 epochs** â€” enough passes over the ~2,400 training examples to generalise\n",
    "   Peterson's style to unseen questions rather than memorise specific passages.\n",
    "\n",
    "3. **Qwen3-32B dense architecture** â€” 2.3Ã— the parameters of the 14B model,\n",
    "   all of which participate in every token prediction.  More weight space means\n",
    "   more nuanced style encoding with the same r=32 LoRA rank.\n",
    "\n",
    "### Updating the All-Models Comparison\n",
    "\n",
    "To include this model in `AllModels_JordanPeterson_Comparison.ipynb`:\n",
    "\n",
    "1. Add `\"qwen3_32b\"` to `MODEL_KEYS`\n",
    "2. Add the path, display name, and colour to the respective dicts:\n",
    "   ```python\n",
    "   MODEL_PATHS  [\"qwen3_32b\"] = \"./outputs/qwen3_32b_peterson_lora\"\n",
    "   MODEL_DISPLAY[\"qwen3_32b\"] = \"Qwen3-32B  |  Fine-Tuned\"\n",
    "   MODEL_COLORS [\"qwen3_32b\"] = \"#9467BD\"   # purple\n",
    "   MODEL_SHORT  [\"qwen3_32b\"] = \"Qwen3-32B\\nFine-Tuned\"\n",
    "   ```\n",
    "3. Add a Phase 5 inference block (using `generate_response_qwen3`) with cache\n",
    "   file `comparison_cache_all_models/qwen3_32b_results.pkl`\n",
    "4. Update all chart loops â€” they already iterate over `MODEL_KEYS` so adding\n",
    "   one entry is the only change needed per chart.\n",
    "\n",
    "### Further Improvement Options\n",
    "\n",
    "| Option | Expected gain | Prerequisite |\n",
    "|--------|--------------|-------------|\n",
    "| 5 epochs | Marginal; diminishing returns | Just increase `NUM_EPOCHS` |\n",
    "| `r=64` LoRA | More style capacity | Likely OOMs on 32B; test with care |\n",
    "| More Q&A variety (3 questions/passage) | Broader coverage | Costs ~$1-2 more |\n",
    "| Human-edited Q&A answers | Highest quality ceiling | Manual effort |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
