{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d760d6",
   "metadata": {},
   "source": [
    "# Qwen3-14B Jordan Peterson Fine-Tuning â€” Version 2\n",
    "## Synthetic Q&A Data Â· 3 Epochs Â· Increased LoRA Rank\n",
    "\n",
    "This notebook is the second iteration of the Qwen3-14B fine-tuning effort on\n",
    "Jordan B. Peterson's four books.  Version 1 produced a domain-adapted model\n",
    "(perplexity dropped 33%) but failed on stylistic metrics: the model regurgitated\n",
    "book passages instead of answering questions in Peterson's voice.\n",
    "\n",
    "This notebook diagnoses those failures, explains every change made to address\n",
    "them, and implements a materially improved training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7e522",
   "metadata": {},
   "source": [
    "---\n",
    "## Why Version 1 Underperformed â€” A Detailed Post-Mortem\n",
    "\n",
    "### The Passage-Regurgitation Problem\n",
    "\n",
    "V1 produced models that, when prompted with a question like *\"What is the\n",
    "relationship between order and chaos?\"*, responded with raw book text beginning\n",
    "mid-sentence: *\"â€¦the world, and the dragon of chaos that guards it, isâ€¦\"*\n",
    "\n",
    "This is not a failure of fine-tuning â€” it is a direct consequence of what the\n",
    "training data asked the model to learn.\n",
    "\n",
    "### Root Cause 1: The Training Task Was Wrong\n",
    "\n",
    "The V1 dataset was created by chunking each book into 350-word passages and\n",
    "formatting each chunk as a conversation:\n",
    "\n",
    "```\n",
    "System:    \"You are trained on Peterson's worksâ€¦\"\n",
    "User:      [first half of a passage]\n",
    "Assistant: [second half of the same passage]\n",
    "```\n",
    "\n",
    "This teaches the model a **text-completion task**: given a piece of Peterson's\n",
    "writing, continue it.  At inference time we ask something entirely different â€”\n",
    "we give it a question and expect a structured, thoughtful answer.  The model\n",
    "has never seen that task during training, so it falls back on what it learned:\n",
    "produce passage-like text.\n",
    "\n",
    "The fix is to use **instruction-tuning format** (Q&A pairs) so the training task\n",
    "at inference matches the training task at fine-tuning.\n",
    "\n",
    "### Root Cause 2: One Epoch Is Not Enough\n",
    "\n",
    "Fine-tuning on a new domain goes through three identifiable phases:\n",
    "\n",
    "| Phase | Epochs | What the model learns |\n",
    "|-------|--------|----------------------|\n",
    "| **Memorisation** | 0.5 â€“ 1.5 | Reproduces training examples nearly verbatim |\n",
    "| **Generalisation** | 1.5 â€“ 3.5 | Internalises patterns; applies style to new questions |\n",
    "| **Overfitting** | 4.0+ | Vocabulary narrows; starts looping; perplexity rises |\n",
    "\n",
    "V1 stopped at 1 epoch â€” deep in the memorisation phase.  The model had not\n",
    "yet learned to *generalise* Peterson's voice to unseen prompts.  Three epochs\n",
    "is the community-established sweet spot for LoRA stylistic fine-tuning.\n",
    "\n",
    "### Root Cause 3: LoRA Rank Was Too Low\n",
    "\n",
    "LoRA injects trainable low-rank matrices `B` (dÃ—r) and `A` (rÃ—d) into each\n",
    "attention projection.  The rank `r` determines how many independent directions\n",
    "in weight space the adapter can modify.\n",
    "\n",
    "With **r=16**, the adapter has 16 independent directions per projection.  For\n",
    "simple task adaptation (e.g., translating to a different language) this is\n",
    "sufficient.  For **stylistic fine-tuning** â€” which requires capturing\n",
    "vocabulary distribution, sentence rhythm, conceptual framing, and rhetorical\n",
    "structure simultaneously â€” more capacity helps.\n",
    "\n",
    "**r=32** doubles the adapter parameters (still only ~0.12% of model weights)\n",
    "at negligible VRAM cost, giving the adapter meaningfully more room to encode\n",
    "Peterson's distinctive patterns.\n",
    "\n",
    "### Summary Table: V1 vs V2\n",
    "\n",
    "| Component | V1 | V2 | Why the change matters |\n",
    "|-----------|----|----|----------------------|\n",
    "| Training data | Passage completion | Synthetic Q&A pairs | Trains to *answer*, not *regurgitate* |\n",
    "| Training epochs | 1 | 3 | Crosses from memorisation into generalisation |\n",
    "| LoRA rank | r=16 | r=32 | 2Ã— adapter capacity for style capture |\n",
    "| LoRA alpha | 16 | 32 | Maintains alpha/rank = 1 (consistent scaling) |\n",
    "| Effective batch size | 8 (2Ã—4 accum) | 8 (2Ã—4 accum) | Already optimal; no change needed |\n",
    "| Base model | Qwen3-14B | Qwen3-14B | Still the best option (see analysis below) |\n",
    "| Total training time | ~23 min | ~90 min | 4Ã— longer; worth it for the quality gain |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc7812",
   "metadata": {},
   "source": [
    "---\n",
    "## V2 Improvement Strategy â€” Technical Rationale\n",
    "\n",
    "### Why Qwen3-14B Over GPT-OSS 20B\n",
    "\n",
    "Both models were evaluated in V1.  Qwen3-14B is the better choice for V2\n",
    "for three independent reasons:\n",
    "\n",
    "1. **Lower training loss** (2.44 vs 3.01) â€” despite being a smaller model,\n",
    "   Qwen3 absorbed the domain signal more efficiently in one epoch.\n",
    "\n",
    "2. **Training speed** (23 min vs 73 min per epoch) â€” with 3 epochs planned,\n",
    "   GPT-OSS would take ~220 min; Qwen3 takes ~70 min.  Time is a limited resource.\n",
    "\n",
    "3. **Architectural recency** â€” Qwen3-14B (2025) is a significantly more capable\n",
    "   base model than GPT-OSS 20B for instruction-following, which matters for\n",
    "   the Q&A format we are training on.\n",
    "\n",
    "The larger parameter count of GPT-OSS (20B vs 14B) does not compensate for\n",
    "these disadvantages.  Bigger is not always better.\n",
    "\n",
    "### Why Synthetic Q&A Data Is the Highest-Leverage Change\n",
    "\n",
    "The authenticity of a fine-tuned model depends on two things:\n",
    "- The **answers** must sound like the target author (Peterson)\n",
    "- The **questions** must teach the model when to activate that voice\n",
    "\n",
    "Our synthetic Q&A strategy achieves both:\n",
    "\n",
    "- **Answers** = verbatim passages from Peterson's books â€” authentic, unmodified\n",
    "- **Questions** = generated by Claude Haiku from each passage â€” lightweight,\n",
    "  cost-efficient (~$1â€“3 total for all 4 books), high enough quality because\n",
    "  the questions are just triggers; the training signal is in the answers\n",
    "\n",
    "This mirrors how successful open-source instruction datasets (Alpaca, WizardLM,\n",
    "OpenHermes) were built: use a capable LLM to generate questions, use high-quality\n",
    "human text as answers.\n",
    "\n",
    "### Why Three Epochs\n",
    "\n",
    "The empirical evidence from the LoRA fine-tuning literature converges on 3 epochs\n",
    "as the sweet spot for domain-specific stylistic adaptation:\n",
    "\n",
    "- **1 epoch**: Model has seen each example once.  Gradient updates are noisy.\n",
    "  Output closely resembles training examples (regurgitation).\n",
    "- **2 epochs**: Visible improvement; model begins generalising.  Some\n",
    "  prompts still trigger passage-like output.\n",
    "- **3 epochs**: Robust generalisation across unseen prompts.  Style is\n",
    "  internalized, not just memorised.\n",
    "- **5+ epochs**: Vocabulary narrows toward training set vocabulary; model\n",
    "  starts to lose general coherence on out-of-distribution prompts.\n",
    "\n",
    "### Why r=32 and alpha=32\n",
    "\n",
    "LoRA replaces a weight update `Î”W` with `(alpha/rank) * B*A` where `B` is\n",
    "`(d_model Ã— r)` and `A` is `(r Ã— d_model)`.\n",
    "\n",
    "Setting `alpha = rank` (as in V1 with `r=16, alpha=16`, and V2 with `r=32,\n",
    "alpha=32`) keeps the effective scale factor at `alpha/rank = 1.0`.  This means\n",
    "the LoRA update is applied at the same relative strength as the base weights,\n",
    "which is the established baseline.\n",
    "\n",
    "Doubling the rank from 16 to 32 doubles the *capacity* of the adapter (number\n",
    "of directions it can modify) without changing the *strength* of updates.  This\n",
    "is the conservative, well-grounded improvement: more representational power,\n",
    "same learning dynamics.\n",
    "\n",
    "### Why gradient_accumulation_steps=4 (unchanged)\n",
    "\n",
    "With `per_device_train_batch_size=2` and `gradient_accumulation_steps=4`,\n",
    "the effective batch size is **8 examples** per gradient update.  This was\n",
    "already correct in V1 â€” Qwen3-14B fits 2 examples per forward pass on the\n",
    "4090, and 4 accumulation steps gives stability without excessive memory.\n",
    "\n",
    "No change needed here.\n",
    "\n",
    "---\n",
    "## Notebook Structure\n",
    "\n",
    "- **Part 1** â€” Generate the synthetic Q&A training dataset using Claude Haiku\n",
    "- **Part 2** â€” Fine-tune Qwen3-14B using that dataset with the V2 hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7e075",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Synthetic Q&A Dataset Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will:\n",
    "1. Extract all passages from the four Peterson PDFs (same extraction logic as V1)\n",
    "2. For each passage, call the Claude Haiku API to generate 2 questions that the\n",
    "   passage directly answers\n",
    "3. Pair each question with the original passage as the answer\n",
    "4. Cache the results to disk so this step only runs once\n",
    "\n",
    "### Why Claude Haiku for Question Generation?\n",
    "\n",
    "- **Cost**: Haiku is the most affordable Claude model (~$0.80/M input tokens,\n",
    "  $4.00/M output tokens).  Generating questions for ~1,200 passages costs\n",
    "  roughly $1â€“3 total.\n",
    "- **Quality**: Question generation is not a demanding task â€” we only need\n",
    "  plausible, on-topic questions.  The *training signal* is entirely in the\n",
    "  answers (Peterson's own words), not the questions.  Haiku is more than\n",
    "  adequate.\n",
    "- **Speed**: Haiku has the highest throughput of Claude models.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You need an Anthropic API key.  Set it before running this cell:\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=\"your-key-here\"\n",
    "```\n",
    "\n",
    "Or add it to `~/.env` as `ANTHROPIC_API_KEY=your-key-here`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91627e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install the Anthropic Python SDK if not already present â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# The .finetuning venv does not ship with 'anthropic' by default; this cell\n",
    "# installs it silently on first run.  On subsequent runs pip finds it already\n",
    "# installed and exits immediately.\n",
    "import subprocess, sys\n",
    "_result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"anthropic\", \"-q\"],\n",
    "    capture_output=True, text=True,\n",
    ")\n",
    "if _result.returncode != 0:\n",
    "    print(\"pip install failed:\", _result.stderr[:400])\n",
    "else:\n",
    "    print(\"anthropic SDK ready.\")\n",
    "\n",
    "import os, re, json, time, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import fitz          # PyMuPDF â€” PDF extraction\n",
    "import anthropic     # Anthropic Python SDK\n",
    "\n",
    "print(f\"anthropic version : {anthropic.__version__}\")\n",
    "\n",
    "# â”€â”€ API key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Look for the key in the environment first, then fall back to ~/.env.\n",
    "_api_key = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")\n",
    "if not _api_key:\n",
    "    _env_file = Path.home() / \".env\"\n",
    "    if _env_file.exists():\n",
    "        for _line in _env_file.read_text().splitlines():\n",
    "            if _line.startswith(\"ANTHROPIC_API_KEY=\"):\n",
    "                _api_key = _line.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
    "                break\n",
    "\n",
    "if not _api_key:\n",
    "    raise EnvironmentError(\n",
    "        \"ANTHROPIC_API_KEY not found.\\n\"\n",
    "        \"Set it with:  export ANTHROPIC_API_KEY='sk-ant-...'\\n\"\n",
    "        \"Or add it to ~/.env as:  ANTHROPIC_API_KEY=sk-ant-...\"\n",
    "    )\n",
    "print(\"API key found.\")\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BOOKS_DIR   = Path(\"../../Books/JordanPeterson\")\n",
    "QA_DIR      = Path(\"./qa_dataset\")\n",
    "QA_CACHE    = QA_DIR / \"peterson_qa.jsonl\"\n",
    "QA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model for question generation: Haiku â€” cheap, fast, sufficient for Q generation\n",
    "GENERATION_MODEL = \"claude-haiku-4-5-20251001\"\n",
    "\n",
    "print(f\"Books dir : {BOOKS_DIR.resolve()}\")\n",
    "print(f\"QA cache  : {QA_CACHE.resolve()}\")\n",
    "print(f\"Gen model : {GENERATION_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d87fc",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1a: Extract and Chunk Book Passages\n",
    "\n",
    "We reuse the same extraction logic from V1:\n",
    "- Read each PDF with PyMuPDF\n",
    "- Strip page numbers, headers, excessive whitespace, and non-printable characters\n",
    "- Chunk into ~350-word passages with 50-word overlap to avoid hard cuts mid-sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove PDF artefacts from extracted text.\n",
    "\n",
    "    PyMuPDF sometimes includes:\n",
    "    - Non-printable control characters from PDF encoding\n",
    "    - Repeated whitespace and newlines from column layout\n",
    "    - Ligature characters (ï¬, ï¬‚) that don't tokenize well\n",
    "\n",
    "    We replace all whitespace runs with a single space and strip\n",
    "    non-printable characters, leaving clean prose.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', raw)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Replace common PDF ligatures with their ASCII equivalents\n",
    "    text = text.replace('\\ufb01', 'fi').replace('\\ufb02', 'fl')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_chunks(pdf_path: Path,\n",
    "                   chunk_words: int = 350,\n",
    "                   overlap_words: int = 50) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF and split it into overlapping word-level chunks.\n",
    "\n",
    "    Why overlapping chunks?\n",
    "    Hard cuts at fixed word boundaries often split sentences mid-thought.\n",
    "    A 50-word overlap ensures each chunk starts with enough context to be\n",
    "    readable on its own, which makes the Q&A pairs more coherent.\n",
    "\n",
    "    Args:\n",
    "        pdf_path    : path to the PDF file\n",
    "        chunk_words : target words per chunk (default 350 â€” long enough to\n",
    "                      contain a complete argument, short enough to stay under\n",
    "                      the tokenizer's context window after formatting)\n",
    "        overlap_words: words from the previous chunk to prepend (default 50)\n",
    "\n",
    "    Returns:\n",
    "        List of clean text chunks (each 300â€“400 words typically)\n",
    "    \"\"\"\n",
    "    doc   = fitz.open(str(pdf_path))\n",
    "    pages = [clean_text(page.get_text()) for page in doc]\n",
    "    doc.close()\n",
    "    full_text = ' '.join(pages)\n",
    "\n",
    "    words  = full_text.split()\n",
    "    step   = chunk_words - overlap_words\n",
    "    chunks = []\n",
    "    for start in range(0, len(words), step):\n",
    "        chunk = ' '.join(words[start : start + chunk_words])\n",
    "        if len(chunk.split()) >= 100:   # discard very short tail chunks\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# â”€â”€ Extract passages from all 4 books â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pdf_files = sorted(BOOKS_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "for p in pdf_files:\n",
    "    print(f\"  {p.name}\")\n",
    "\n",
    "all_chunks  = []    # flat list of all passage strings\n",
    "chunk_meta  = []    # parallel list of {\"book\": short_name} for each chunk\n",
    "\n",
    "book_labels = {\n",
    "    \"Maps of Meaning\":     \"Maps of Meaning\",\n",
    "    \"12 Rules for Life\":   \"12 Rules for Life\",\n",
    "    \"Beyond Order\":        \"Beyond Order\",\n",
    "    \"We Who Wrestle\":      \"We Who Wrestle with God\",\n",
    "}\n",
    "\n",
    "print()\n",
    "for pdf in pdf_files:\n",
    "    # Assign a short book label based on filename keywords\n",
    "    fname = pdf.name.lower()\n",
    "    if \"maps\" in fname:\n",
    "        label = \"Maps of Meaning\"\n",
    "    elif \"12 rules\" in fname or \"antidote\" in fname:\n",
    "        label = \"12 Rules for Life\"\n",
    "    elif \"beyond\" in fname:\n",
    "        label = \"Beyond Order\"\n",
    "    else:\n",
    "        label = \"We Who Wrestle with God\"\n",
    "\n",
    "    chunks = extract_chunks(pdf)\n",
    "    all_chunks.extend(chunks)\n",
    "    chunk_meta.extend([{\"book\": label}] * len(chunks))\n",
    "    print(f\"  {label:<30}  {len(chunks):4d} chunks  \"\n",
    "          f\"(~{sum(len(c.split()) for c in chunks):,} words)\")\n",
    "\n",
    "print(f\"\\nTotal passages: {len(all_chunks):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db84756",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1b: Generate Questions via Claude Haiku\n",
    "\n",
    "For each passage we ask Haiku to generate exactly **2 questions** that the\n",
    "passage directly answers.  The answers in the training dataset will be the\n",
    "passage text verbatim â€” we are only generating the *questions*, which is a\n",
    "much cheaper and easier task.\n",
    "\n",
    "**Prompt design choices:**\n",
    "- Ask for exactly 2 questions in a JSON array (structured output is easy to parse)\n",
    "- Specify that questions should cover *different angles* of the passage (variety)\n",
    "- Provide Peterson's topic vocabulary so Haiku generates contextually appropriate\n",
    "  questions even when a passage is abstract\n",
    "- Return questions only (not answers), so the output is very short â†’ low cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=_api_key)\n",
    "\n",
    "\n",
    "def generate_questions(passage: str, book: str,\n",
    "                       max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Call Claude Haiku to generate 2 questions for a given passage.\n",
    "\n",
    "    The prompt instructs Haiku to:\n",
    "    1. Return ONLY a JSON array of 2 strings (easy to parse, minimal output tokens)\n",
    "    2. Generate questions that the passage directly answers (ensures relevance)\n",
    "    3. Cover different angles â€” one can be practical, one philosophical/conceptual\n",
    "    4. Sound like genuine questions a reader would ask (not synthetic-sounding)\n",
    "\n",
    "    Retries up to max_retries times with exponential backoff on API errors or\n",
    "    malformed JSON.  Returns an empty list on complete failure so the caller\n",
    "    can skip the passage gracefully.\n",
    "\n",
    "    Cost estimate:\n",
    "    - Input:  ~550 tokens per call  (prompt template + 350-word passage)\n",
    "    - Output: ~60 tokens per call   (2 short question strings in JSON)\n",
    "    - At Haiku pricing ($0.80/$4.00 per MTok): ~$0.00068 per passage\n",
    "    - For 1,200 passages: ~$0.82 total\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"You are building a training dataset for a Jordan B. Peterson AI model.\\n\\n\"\n",
    "        f\"The passage below is from Peterson's book '{book}'.  Generate exactly 2 questions \"\n",
    "        f\"that:\\n\"\n",
    "        f\"1. This passage directly and substantively answers\\n\"\n",
    "        f\"2. Someone interested in Peterson's ideas might genuinely ask\\n\"\n",
    "        f\"3. Cover different angles of the passage (e.g. one concrete, one philosophical)\\n\\n\"\n",
    "        f\"Peterson's topics include: order vs chaos, meaning, personal responsibility, \"\n",
    "        f\"suffering, mythology, archetypes, the shadow, logos, truth, religion, \"\n",
    "        f\"Jungian psychology, hierarchy, heroism, sacrifice, being.\\n\\n\"\n",
    "        f\"Return ONLY a JSON array of exactly 2 question strings.  No other text.\\n\"\n",
    "        f\"Example: [\\\"Why is confronting chaos necessary for meaning?\\\", \"\n",
    "        f\"\\\"What role does suffering play in personal development?\\\"]\\n\\n\"\n",
    "        f\"Passage:\\n{passage}\"\n",
    "    )\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=GENERATION_MODEL,\n",
    "                max_tokens=150,   # 2 short questions never need more than 150 tokens\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            raw = response.content[0].text.strip()\n",
    "\n",
    "            # Extract JSON array â€” handle leading/trailing prose if any\n",
    "            match = re.search(r'\\[.*?\\]', raw, re.DOTALL)\n",
    "            if not match:\n",
    "                raise ValueError(f\"No JSON array found in: {raw[:100]}\")\n",
    "            questions = json.loads(match.group())\n",
    "\n",
    "            if not isinstance(questions, list) or len(questions) < 1:\n",
    "                raise ValueError(f\"Expected list, got: {type(questions)}\")\n",
    "\n",
    "            # Return up to 2 questions; pad with a fallback if only 1 returned\n",
    "            return [str(q).strip() for q in questions[:2]]\n",
    "\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt   # exponential backoff: 1s, 2s, 4s\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    [retry {attempt+1}/{max_retries}] Error: {e} â€” waiting {wait}s\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"    [FAILED after {max_retries} attempts] Passage skipped.\")\n",
    "                return []\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "print(\"generate_questions() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c3661",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1c: Run Generation (Cache-Aware)\n",
    "\n",
    "This cell generates questions for every passage and writes them to\n",
    "`qa_dataset/peterson_qa.jsonl` as it goes.  If the JSONL file already\n",
    "contains results for **90% or more** of passages, the whole step is skipped.\n",
    "\n",
    "Writing incrementally (one record at a time, flushing after each) means that\n",
    "if the process is interrupted â€” by a rate-limit error, a power cut, or a\n",
    "keyboard interrupt â€” all work up to that point is preserved and the run can\n",
    "be resumed by simply re-executing this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Check cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "existing_records = []\n",
    "if QA_CACHE.exists():\n",
    "    with open(QA_CACHE) as f:\n",
    "        existing_records = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "_expected = len(all_chunks) * 2   # 2 Q&A pairs per passage\n",
    "_coverage = len(existing_records) / _expected if _expected else 0\n",
    "\n",
    "print(f\"Passages   : {len(all_chunks):,}\")\n",
    "print(f\"Expected Q&A pairs : {_expected:,}  (2 per passage)\")\n",
    "print(f\"Cached     : {len(existing_records):,}  ({100*_coverage:.1f}% coverage)\")\n",
    "\n",
    "if _coverage >= 0.90:\n",
    "    print(\"\\nCache is â‰¥90% complete â€” skipping generation.\")\n",
    "    print(f\"Delete {QA_CACHE} to force regeneration.\")\n",
    "else:\n",
    "    # Determine which passages still need processing\n",
    "    # We track progress by appending to the JSONL file and counting existing lines.\n",
    "    already_done = len(existing_records) // 2   # 2 records per passage\n",
    "    remaining    = all_chunks[already_done:]\n",
    "    remaining_meta = chunk_meta[already_done:]\n",
    "\n",
    "    print(f\"\\nGenerating questions for {len(remaining):,} passages \"\n",
    "          f\"(starting from passage {already_done + 1})â€¦\")\n",
    "    print(f\"Estimated cost: ~${len(remaining) * 0.00068:.2f}\\n\")\n",
    "\n",
    "    skipped = 0\n",
    "    with open(QA_CACHE, \"a\") as out_f:\n",
    "        for i, (passage, meta) in enumerate(zip(remaining, remaining_meta)):\n",
    "            global_idx = already_done + i + 1\n",
    "\n",
    "            # Print progress every 50 passages\n",
    "            if i % 50 == 0:\n",
    "                pct = 100 * (already_done + i) / len(all_chunks)\n",
    "                print(f\"  [{global_idx:4d}/{len(all_chunks):4d}]  {pct:5.1f}%  \"\n",
    "                      f\"book: {meta['book']}\")\n",
    "\n",
    "            questions = generate_questions(passage, meta[\"book\"])\n",
    "\n",
    "            if not questions:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Write one JSON line per Q&A pair so the file is resumable\n",
    "            for q in questions:\n",
    "                record = {\n",
    "                    \"question\": q,\n",
    "                    \"answer\":   passage,\n",
    "                    \"book\":     meta[\"book\"],\n",
    "                }\n",
    "                out_f.write(json.dumps(record) + \"\\n\")\n",
    "            out_f.flush()   # flush after each passage â€” safe against interruption\n",
    "\n",
    "            # Polite rate-limiting: 0.3 s gap between requests\n",
    "            time.sleep(0.3)\n",
    "\n",
    "    print(f\"\\nGeneration complete.\")\n",
    "    print(f\"  Passages processed : {len(remaining) - skipped:,}\")\n",
    "    print(f\"  Passages skipped   : {skipped}\")\n",
    "    print(f\"  Output file        : {QA_CACHE}\")\n",
    "\n",
    "# â”€â”€ Re-read final dataset regardless of which path we took â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(QA_CACHE) as f:\n",
    "    qa_records = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "print(f\"\\nTotal Q&A pairs in dataset: {len(qa_records):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de234a5f",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1d: Dataset Statistics and Sample Preview\n",
    "\n",
    "Before training, verify the dataset:\n",
    "- Distribution across books (should be roughly proportional to book length)\n",
    "- Average answer length (should be ~350 words â€” our target chunk size)\n",
    "- Sample Q&A pairs to spot-check question quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40226ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# â”€â”€ Distribution by book â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "book_counts = Counter(r[\"book\"] for r in qa_records)\n",
    "print(\"Q&A pairs by book:\")\n",
    "for book, count in sorted(book_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {book:<35}  {count:4d} pairs  ({100*count/len(qa_records):.1f}%)\")\n",
    "\n",
    "# â”€â”€ Answer (passage) length statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ans_lengths = [len(r[\"answer\"].split()) for r in qa_records]\n",
    "print(f\"\\nAnswer length (words):\")\n",
    "print(f\"  Min   : {min(ans_lengths)}\")\n",
    "print(f\"  Max   : {max(ans_lengths)}\")\n",
    "print(f\"  Mean  : {sum(ans_lengths)/len(ans_lengths):.0f}\")\n",
    "print(f\"  Median: {sorted(ans_lengths)[len(ans_lengths)//2]}\")\n",
    "\n",
    "# â”€â”€ Question length statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "q_lengths = [len(r[\"question\"].split()) for r in qa_records]\n",
    "print(f\"\\nQuestion length (words):\")\n",
    "print(f\"  Min   : {min(q_lengths)}\")\n",
    "print(f\"  Max   : {max(q_lengths)}\")\n",
    "print(f\"  Mean  : {sum(q_lengths)/len(q_lengths):.1f}\")\n",
    "\n",
    "# â”€â”€ Sample Q&A pairs from each book â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"â•\" * 80)\n",
    "print(\"SAMPLE Q&A PAIRS (first example per book)\")\n",
    "print(\"â•\" * 80)\n",
    "seen_books = set()\n",
    "for r in qa_records:\n",
    "    if r[\"book\"] not in seen_books:\n",
    "        seen_books.add(r[\"book\"])\n",
    "        print(f\"\\nðŸ“–  {r['book']}\")\n",
    "        print(f\"Q: {r['question']}\")\n",
    "        print(f\"A: {r['answer'][:200]}â€¦\")\n",
    "        print()\n",
    "    if len(seen_books) == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8c4cd",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Fine-Tuning Qwen3-14B (Version 2)\n",
    "\n",
    "## Configuration Summary\n",
    "\n",
    "| Parameter | V1 | **V2** | Notes |\n",
    "|-----------|----|----|-------|\n",
    "| Base model | `Qwen3-14B-unsloth-bnb-4bit` | Same | Best option on this hardware |\n",
    "| Training data | Passage completion | **Synthetic Q&A** | Core fix for regurgitation |\n",
    "| Epochs | 1 | **3** | Cross into generalisation phase |\n",
    "| LoRA rank (r) | 16 | **32** | 2Ã— adapter capacity |\n",
    "| LoRA alpha | 16 | **32** | Maintains alpha/rank = 1.0 |\n",
    "| LoRA target modules | q/k/v/o + gate/up/down | Same | All attention + MLP |\n",
    "| Batch size per device | 2 | 2 | Limited by 24 GB VRAM |\n",
    "| Gradient accumulation | 4 | 4 | Effective batch = 8; already optimal |\n",
    "| Learning rate | 2e-4 | 2e-4 | Standard for Qwen3 LoRA |\n",
    "| LR schedule | cosine | cosine | Smooth decay over 3 epochs |\n",
    "| Warmup steps | 10 | **30** | Proportionally scaled to 3Ã— more steps |\n",
    "| Optimizer | adamw_8bit | adamw_8bit | Unsloth's memory-efficient AdamW |\n",
    "| Output dir | `outputs/qwen3_14b_â€¦_lora` | `outputs/qwen3_14b_peterson_v2_lora` | Separate from V1 |\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "With the Q&A format and 3 epochs, we expect:\n",
    "- **Perplexity**: similar or slightly better reduction than V1 (âˆ’30% to âˆ’40%)\n",
    "- **TF-IDF similarity**: improvement (model uses Peterson's vocabulary patterns)\n",
    "- **Keyword density**: improvement (model spontaneously uses signature terms)\n",
    "- **TTR**: improvement (no more passage repetition â†’ richer vocabulary)\n",
    "- **Response quality**: coherent, structured answers in Peterson's voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc, math, json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForSeq2Seq\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "QA_CACHE    = Path(\"./qa_dataset/peterson_qa.jsonl\")   # generated in Part 1\n",
    "OUTPUT_DIR  = Path(\"./outputs/qwen3_14b_peterson_v2_lora\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_MODEL  = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LEN = 2048   # safe upper bound; covers 350-word passage + chat overhead\n",
    "\n",
    "# â”€â”€ LoRA hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LORA_RANK   = 32     # doubled from V1's r=16\n",
    "LORA_ALPHA  = 32     # alpha/rank = 1.0, same as V1 â€” maintains update scale\n",
    "\n",
    "# â”€â”€ Training hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BATCH_SIZE        = 2    # max that fits comfortably in 24 GB with Qwen3-14B\n",
    "GRAD_ACCUM        = 4    # effective batch = 2Ã—4 = 8\n",
    "NUM_EPOCHS        = 3    # key change from V1\n",
    "LEARNING_RATE     = 2e-4\n",
    "WARMUP_STEPS      = 30   # ~3% of total steps; scaled up from V1's 10 for longer run\n",
    "WEIGHT_DECAY      = 0.01\n",
    "\n",
    "# â”€â”€ System prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Every training example uses this system prompt so the model learns to adopt\n",
    "# Peterson's voice when given this specific persona instruction.\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an AI assistant that has been trained on the complete works of \"\n",
    "    \"Jordan B. Peterson, a Canadian clinical psychologist, professor, and author. \"\n",
    "    \"You speak with deep knowledge of psychology, philosophy, mythology, religion, \"\n",
    "    \"and personal responsibility.  Your responses reflect Peterson's writing style, \"\n",
    "    \"intellectual depth, and interdisciplinary approach to understanding human \"\n",
    "    \"nature and meaning.\"\n",
    ")\n",
    "\n",
    "print(f\"PyTorch  : {torch.__version__}\")\n",
    "print(f\"CUDA     : {torch.cuda.is_available()}  |  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM     : {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Base model  : {BASE_MODEL}\")\n",
    "print(f\"  LoRA rank   : r={LORA_RANK}, alpha={LORA_ALPHA}  (ratio={LORA_ALPHA/LORA_RANK:.1f})\")\n",
    "print(f\"  Epochs      : {NUM_EPOCHS}\")\n",
    "print(f\"  Batch (eff) : {BATCH_SIZE} Ã— {GRAD_ACCUM} = {BATCH_SIZE*GRAD_ACCUM}\")\n",
    "print(f\"  LR          : {LEARNING_RATE}\")\n",
    "print(f\"  Output      : {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9506e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load the Q&A Dataset\n",
    "\n",
    "Load the JSONL file generated in Part 1 and build a HuggingFace `Dataset` object.\n",
    "Each record contains:\n",
    "- `question` â€” the synthetically generated question\n",
    "- `answer`   â€” the verbatim Peterson passage\n",
    "- `book`     â€” source book label (used only for statistics, not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not QA_CACHE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Q&A cache not found at {QA_CACHE}.\\n\"\n",
    "        \"Please run Part 1 first to generate the dataset.\"\n",
    "    )\n",
    "\n",
    "with open(QA_CACHE) as f:\n",
    "    records = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Build a HuggingFace Dataset from the flat list of dicts.\n",
    "# We only keep \"question\" and \"answer\"; the \"book\" field is dropped to keep\n",
    "# the dataset schema minimal.\n",
    "raw_dataset = Dataset.from_list([\n",
    "    {\"question\": r[\"question\"], \"answer\": r[\"answer\"]}\n",
    "    for r in records\n",
    "])\n",
    "\n",
    "print(f\"Dataset loaded: {len(raw_dataset):,} Q&A pairs\")\n",
    "print(f\"Schema: {raw_dataset.column_names}\")\n",
    "print()\n",
    "\n",
    "# Show distribution across books\n",
    "from collections import Counter\n",
    "book_dist = Counter(r[\"book\"] for r in records)\n",
    "print(\"Distribution by book:\")\n",
    "for book, count in sorted(book_dist.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {book:<35}  {count:4d}  ({100*count/len(records):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78be4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Load the Qwen3-14B Base Model\n",
    "\n",
    "We load the 4-bit quantized model via Unsloth.  The `max_seq_length=2048`\n",
    "matches the context window used during dataset formatting â€” passing a shorter\n",
    "value would truncate long passages; a longer value would waste VRAM on\n",
    "unneeded KV-cache pre-allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {BASE_MODEL} â€¦\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    dtype           = None,          # auto: bfloat16 on Ampere+, float16 otherwise\n",
    "    max_seq_length  = MAX_SEQ_LEN,\n",
    "    load_in_4bit    = True,          # 4-bit quantization via bitsandbytes\n",
    "    full_finetuning = False,         # we are adding LoRA, not full fine-tuning\n",
    ")\n",
    "\n",
    "vram_after_load = torch.cuda.memory_reserved() / 1e9\n",
    "print(f\"Model loaded.  VRAM reserved: {vram_after_load:.1f} GB\")\n",
    "print(f\"Model dtype   : {next(model.parameters()).dtype}\")\n",
    "print(f\"Tokenizer     : {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c663bef",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Add LoRA Adapters (r=32)\n",
    "\n",
    "### What LoRA Does\n",
    "\n",
    "Low-Rank Adaptation freezes the pre-trained weight matrices `W` and injects a\n",
    "trainable update `Î”W = (alpha/r) Ã— B Ã— A` where:\n",
    "- `B` has shape `(d_model Ã— r)` â€” initialised to zero\n",
    "- `A` has shape `(r Ã— d_model)` â€” initialised with random Gaussian values\n",
    "- `r` is the rank â€” our key improvement from 16 â†’ 32\n",
    "\n",
    "At the start of training `B=0` so `Î”W=0` and the model behaves identically to\n",
    "the base model.  Training gradually fills `B` with the directions needed to\n",
    "capture Peterson's style.\n",
    "\n",
    "### Why r=32 Matters\n",
    "\n",
    "Think of `r` as the number of \"stylistic dimensions\" the adapter can learn.\n",
    "With r=16, the adapter has 16 orthogonal directions to encode things like:\n",
    "*\"use chaos/order vocabulary\"*, *\"frame arguments historically\"*,\n",
    "*\"connect psychology to mythology\"*, etc.\n",
    "\n",
    "With r=32 we have 32 such dimensions â€” enough to capture both the macro-level\n",
    "thematic vocabulary AND the micro-level sentence rhythm that makes Peterson's\n",
    "writing recognisable.\n",
    "\n",
    "### Target Modules\n",
    "\n",
    "We apply LoRA to all attention projection matrices (q, k, v, o) and all MLP\n",
    "gate/up/down projections.  This is the full set recommended by Unsloth for\n",
    "stylistic fine-tuning because style is encoded across both attention patterns\n",
    "(what the model attends to) and the MLP activations (how it transforms those\n",
    "representations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                   = LORA_RANK,    # 32 â€” doubled from V1\n",
    "    lora_alpha          = LORA_ALPHA,   # 32 â€” alpha/rank = 1.0 (same scale as V1)\n",
    "    lora_dropout        = 0,            # 0 is optimal for Unsloth according to docs\n",
    "    target_modules      = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # all attention projections\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",        # all MLP projections\n",
    "    ],\n",
    "    use_gradient_checkpointing = \"unsloth\",   # Unsloth's custom implementation:\n",
    "                                               # saves ~30% VRAM vs PyTorch default\n",
    "    random_state        = 42,\n",
    "    use_rslora          = False,   # standard LoRA (not rank-stabilised variant)\n",
    "    loftq_config        = None,    # no quantization-aware init needed with 4-bit base\n",
    ")\n",
    "\n",
    "# â”€â”€ Count trainable parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "pct_trainable    = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"Total parameters    : {total_params:>15,}\")\n",
    "print(f\"Trainable (LoRA)    : {trainable_params:>15,}  ({pct_trainable:.4f}% of total)\")\n",
    "print(f\"  â†’ V1 had r=16: ~{trainable_params//2:,} trainable params\")\n",
    "print(f\"  â†’ V2 has r=32: ~{trainable_params:,} trainable params (2Ã— capacity)\")\n",
    "print(f\"VRAM after LoRA     : {torch.cuda.memory_reserved()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a3c5f",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Format Dataset into Qwen3 ChatML Format\n",
    "\n",
    "Each Q&A pair is converted into a three-turn conversation using the Qwen3\n",
    "ChatML template.  The key improvement over V1:\n",
    "\n",
    "| Role | V1 content | **V2 content** |\n",
    "|------|-----------|----------------|\n",
    "| `system` | Generic assistant prompt | **Peterson persona prompt** |\n",
    "| `user` | A book passage chunk | **A synthetically generated question** |\n",
    "| `assistant` | Continuation of the passage | **The original passage as an answer** |\n",
    "\n",
    "The system prompt changes from generic to persona-specific so the model\n",
    "learns: *\"when given the Peterson persona instruction, respond in his voice\"*.\n",
    "\n",
    "The user message changes from a passage fragment to a real question so the\n",
    "model learns: *\"when asked a question in this domain, produce a substantive\n",
    "Peterson-style answer\"*.\n",
    "\n",
    "### About `enable_thinking=False`\n",
    "\n",
    "We format training examples with `enable_thinking=False` â€” the same setting\n",
    "used in V1.  This tells Qwen3's chat template not to include chain-of-thought\n",
    "reasoning tokens in the formatted string.  Even with this setting, the template\n",
    "adds an empty `<think>\\n\\n</think>` block before the assistant response â€” this\n",
    "is expected and will be masked out by `train_on_responses_only` in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(batch):\n",
    "    \"\"\"\n",
    "    Convert a batch of Q&A records into formatted Qwen3 ChatML strings.\n",
    "\n",
    "    The output is stored in a 'text' column which SFTTrainer reads directly\n",
    "    via dataset_text_field='text'.\n",
    "\n",
    "    Each formatted string looks like:\n",
    "        <|im_start|>system\n",
    "        You are an AI assistant... (Peterson persona)\n",
    "        <|im_end|>\n",
    "        <|im_start|>user\n",
    "        <the generated question>\n",
    "        <|im_end|>\n",
    "        <|im_start|>assistant\n",
    "        <think>\n",
    "\n",
    "        </think>\n",
    "        <the Peterson passage>\n",
    "        <|im_end|>\n",
    "\n",
    "    The <think>\\n\\n</think> block appears because Qwen3's template always\n",
    "    includes a thinking placeholder even when enable_thinking=False.  It will\n",
    "    be masked out during training by train_on_responses_only, so it does not\n",
    "    contribute to the loss and does not affect what the model learns.\n",
    "    \"\"\"\n",
    "    formatted_texts = []\n",
    "    for question, answer in zip(batch[\"question\"], batch[\"answer\"]):\n",
    "        conversation = [\n",
    "            {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",      \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize           = False,\n",
    "            add_generation_prompt = False,\n",
    "            enable_thinking    = False,   # no chain-of-thought in training\n",
    "        )\n",
    "        formatted_texts.append(text)\n",
    "    return {\"text\": formatted_texts}\n",
    "\n",
    "\n",
    "# Apply formatting across the full dataset using batched map for efficiency\n",
    "dataset = raw_dataset.map(\n",
    "    format_example,\n",
    "    batched=True,\n",
    "    batch_size=256,\n",
    "    remove_columns=raw_dataset.column_names,   # keep only 'text'\n",
    ")\n",
    "\n",
    "print(f\"Formatted dataset: {len(dataset):,} examples\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "# â”€â”€ Print one formatted sample to verify the structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"â”€\" * 70)\n",
    "print(\"SAMPLE FORMATTED TRAINING EXAMPLE\")\n",
    "print(\"â”€\" * 70)\n",
    "sample = dataset[0][\"text\"]\n",
    "print(sample[:800])\n",
    "print(\"â€¦\")\n",
    "print(\"â”€\" * 70)\n",
    "print(f\"Full sample length: {len(sample.split())} words / {len(sample)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cec343",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Apply Response-Only Training Mask\n",
    "\n",
    "`train_on_responses_only` adds a masking function to the data collator so that\n",
    "during training the cross-entropy loss is computed **only on the assistant's\n",
    "tokens** â€” not on the system prompt or the user question.\n",
    "\n",
    "This is critical for two reasons:\n",
    "1. **Efficiency**: the model already knows how to be an assistant; we only want\n",
    "   it to learn *what* to say, not the structural tokens around it.\n",
    "2. **Correctness**: without this mask, the model would also try to memorise the\n",
    "   system prompt and questions, which wastes capacity and can cause instability.\n",
    "\n",
    "The instruction boundary is the last `<|im_start|>user\\n` token before each\n",
    "assistant turn.  The response boundary is `<|im_start|>assistant\\n`.\n",
    "Tokens between them (the user's question) get loss weight = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect the response boundary tokens by inspecting the formatted data.\n",
    "# This guards against future Qwen3 template changes that might shift the token\n",
    "# boundaries, and mirrors the robust approach used in V1.\n",
    "_sample_text = dataset[0][\"text\"]\n",
    "\n",
    "if \"<|im_start|>assistant\\n\" in _sample_text:\n",
    "    instruction_part = \"<|im_start|>user\\n\"\n",
    "    response_part    = \"<|im_start|>assistant\\n\"\n",
    "    print(\"Detected Qwen3 ChatML response boundary tokens:\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"Could not detect Qwen3 ChatML tokens in formatted dataset.\\n\"\n",
    "        \"Check the output of format_example() above.\"\n",
    "    )\n",
    "\n",
    "print(f\"  instruction_part : {repr(instruction_part)}\")\n",
    "print(f\"  response_part    : {repr(response_part)}\")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer      = None,   # will be filled in after SFTTrainer is built below\n",
    "    instruction_part = instruction_part,\n",
    "    response_part    = response_part,\n",
    ")\n",
    "# Note: train_on_responses_only returns a modified version of the trainer.\n",
    "# We pass 'None' here just to test token detection; the actual trainer is\n",
    "# built and wrapped in the next two cells.\n",
    "print(\"\\nResponse masking configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf1749e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Configure and Run Training\n",
    "\n",
    "### SFTConfig Explained\n",
    "\n",
    "- **`num_train_epochs=3`**: The most important change from V1.  Three full passes\n",
    "  over the dataset let the model move from memorisation into genuine stylistic\n",
    "  generalisation.\n",
    "\n",
    "- **`warmup_steps=30`**: Gradually ramps the learning rate from 0 â†’ 2e-4 over\n",
    "  the first 30 gradient steps (~3% of total training).  This prevents the\n",
    "  randomly-initialised LoRA matrices from making destructively large weight\n",
    "  updates at the start.\n",
    "\n",
    "- **`lr_scheduler_type=\"cosine\"`**: After warmup the learning rate decays\n",
    "  smoothly following a cosine curve, reaching ~0 at the end of epoch 3.\n",
    "  Cosine decay is generally more stable than linear decay for LoRA.\n",
    "\n",
    "- **`weight_decay=0.01`**: Mild L2 regularisation on the LoRA weights.\n",
    "  Discourages the adapter from memorising individual passages and encourages\n",
    "  generalisation.\n",
    "\n",
    "- **`optim=\"adamw_8bit\"`**: Unsloth's 8-bit AdamW keeps optimizer states in\n",
    "  8-bit rather than 32-bit, saving ~3 GB of VRAM with no measurable quality\n",
    "  cost for LoRA training.\n",
    "\n",
    "- **`packing=False`**: Sequence packing combines multiple short examples into\n",
    "  one long sequence for efficiency.  We disable it because our examples are\n",
    "  long (~400 words â†’ ~600 tokens), and packing them together could leak\n",
    "  context between unrelated Q&A pairs, confusing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0135a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model         = model,\n",
    "    tokenizer     = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args          = SFTConfig(\n",
    "        dataset_text_field          = \"text\",\n",
    "        max_seq_length              = MAX_SEQ_LEN,\n",
    "        dataset_num_proc            = 2,         # parallel tokenisation\n",
    "\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        gradient_accumulation_steps = GRAD_ACCUM,\n",
    "        num_train_epochs            = NUM_EPOCHS,\n",
    "\n",
    "        learning_rate               = LEARNING_RATE,\n",
    "        warmup_steps                = WARMUP_STEPS,\n",
    "        lr_scheduler_type           = \"cosine\",\n",
    "        weight_decay                = WEIGHT_DECAY,\n",
    "\n",
    "        optim                       = \"adamw_8bit\",\n",
    "        fp16                        = not torch.cuda.is_bf16_supported(),\n",
    "        bf16                        = torch.cuda.is_bf16_supported(),\n",
    "\n",
    "        logging_steps               = 25,        # log loss every 25 gradient steps\n",
    "        save_strategy               = \"epoch\",   # save adapter at end of each epoch\n",
    "        output_dir                  = str(OUTPUT_DIR),\n",
    "        report_to                   = \"none\",    # disable W&B / HF reporting\n",
    "\n",
    "        seed                        = 42,\n",
    "        packing                     = False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Apply the response-only mask to the configured trainer\n",
    "sft_trainer = train_on_responses_only(\n",
    "    sft_trainer,\n",
    "    instruction_part = instruction_part,\n",
    "    response_part    = response_part,\n",
    ")\n",
    "\n",
    "# â”€â”€ Quick sanity check: confirm labels are masked for non-response tokens â”€â”€\n",
    "# If masking is working, the label tensor will have -100 for all system/user\n",
    "# tokens and real token IDs only for the assistant response tokens.\n",
    "_sample_input  = sft_trainer.train_dataset[0]\n",
    "_n_total       = len(_sample_input[\"input_ids\"])\n",
    "_n_trained     = sum(1 for l in _sample_input[\"labels\"] if l != -100)\n",
    "_n_masked      = _n_total - _n_trained\n",
    "print(f\"Response masking check on first example:\")\n",
    "print(f\"  Total tokens   : {_n_total}\")\n",
    "print(f\"  Trained tokens : {_n_trained}  (assistant response only)\")\n",
    "print(f\"  Masked tokens  : {_n_masked}   (system + user = {100*_n_masked/_n_total:.0f}% of input)\")\n",
    "\n",
    "_total_steps = math.ceil(len(dataset) / BATCH_SIZE) * NUM_EPOCHS // GRAD_ACCUM\n",
    "print(f\"\\nEstimated gradient updates: {_total_steps:,}\")\n",
    "print(f\"  ({len(dataset):,} examples / {BATCH_SIZE} batch Ã— {GRAD_ACCUM} accum Ã— {NUM_EPOCHS} epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240ad83",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c026a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Log VRAM before training to establish baseline\n",
    "vram_before = torch.cuda.memory_reserved() / 1e9\n",
    "print(f\"VRAM before training: {vram_before:.1f} GB\")\n",
    "print(f\"Starting training â€” {NUM_EPOCHS} epochs, ~{_total_steps:,} gradient updatesâ€¦\")\n",
    "print()\n",
    "\n",
    "t0 = time.time()\n",
    "train_result = sft_trainer.train()\n",
    "elapsed_min = (time.time() - t0) / 60\n",
    "\n",
    "vram_peak = torch.cuda.max_memory_reserved() / 1e9\n",
    "\n",
    "print()\n",
    "print(\"â•\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"â•\" * 60)\n",
    "print(f\"  Steps          : {train_result.global_step:,}\")\n",
    "print(f\"  Training loss  : {train_result.training_loss:.4f}\")\n",
    "print(f\"  Elapsed        : {elapsed_min:.1f} min\")\n",
    "print(f\"  Peak VRAM      : {vram_peak:.1f} GB\")\n",
    "print()\n",
    "print(\"V1 reference:\")\n",
    "print(\"  Steps: 321  |  Loss: 2.4400  |  Time: 23.3 min  |  VRAM: 13.4 GB\")\n",
    "print()\n",
    "print(\"Expected improvement: lower loss (better Q&A alignment) + longer training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302dbd9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Save the LoRA Adapter\n",
    "\n",
    "We save only the LoRA adapter weights (not the full 14B model), which keeps the\n",
    "checkpoint small (~500 MB for r=32 vs ~260 MB for V1's r=16).  At inference\n",
    "time Unsloth loads the base model and merges the adapter on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48441d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving LoRA adapter to {OUTPUT_DIR} â€¦\")\n",
    "\n",
    "model.save_pretrained(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "\n",
    "# Check saved file sizes\n",
    "adapter_files = list(OUTPUT_DIR.glob(\"*.safetensors\")) + list(OUTPUT_DIR.glob(\"*.bin\"))\n",
    "total_mb = sum(f.stat().st_size for f in adapter_files) / 1e6\n",
    "print(f\"\\nAdapter files:\")\n",
    "for f in sorted(adapter_files):\n",
    "    print(f\"  {f.name}  ({f.stat().st_size/1e6:.1f} MB)\")\n",
    "print(f\"\\nTotal adapter size: {total_mb:.1f} MB\")\n",
    "print(f\"  (V1 r=16 was ~260 MB; V2 r=32 should be ~520 MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22fa0b",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Test the Fine-Tuned Model\n",
    "\n",
    "Enable Unsloth's fast inference kernel and test the model on a representative\n",
    "set of Peterson-themed questions.  Use the same 10 evaluation prompts that are\n",
    "used in the comparison notebooks so results are directly comparable.\n",
    "\n",
    "We run greedy decoding (`do_sample=False`) for deterministic, reproducible\n",
    "outputs â€” the same setting used in all comparison notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1eddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    \"Why is personal responsibility the foundation of a meaningful life?\",\n",
    "    \"How do ancient myths and stories reveal truths about human nature?\",\n",
    "    \"What does it mean to pursue what is meaningful rather than what is expedient?\",\n",
    "    \"How should a person confront suffering rather than flee from it?\",\n",
    "]\n",
    "\n",
    "def ask_v2(question: str, max_new_tokens: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the V2 fine-tuned model.\n",
    "\n",
    "    Uses the same two-step Qwen3 tokenization pipeline as V1:\n",
    "    1. apply_chat_template() â†’ plain text string\n",
    "    2. tokenizer(text) â†’ input tensors\n",
    "\n",
    "    enable_thinking=False keeps reasoning mode disabled, consistent with\n",
    "    how the model was trained.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    text   = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens   = max_new_tokens,\n",
    "            do_sample        = False,\n",
    "            temperature      = 1.0,\n",
    "            repetition_penalty = 1.1,\n",
    "        )\n",
    "\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response   = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    # Strip any residual thinking blocks (can appear even with enable_thinking=False)\n",
    "    response   = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"Testing V2 fine-tuned model (greedy decoding)â€¦\\n\")\n",
    "for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(f\"Q{i+1}: {prompt}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    answer = ask_v2(prompt)\n",
    "    print(answer if answer.strip() else \"(empty response)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8d11e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Test Thinking Mode (Optional)\n",
    "\n",
    "Qwen3 supports an optional chain-of-thought reasoning mode via\n",
    "`enable_thinking=True`.  The model first produces a `<think>â€¦</think>` block\n",
    "containing its internal reasoning, then gives its final answer.\n",
    "\n",
    "We did not train with thinking mode, so this tests whether the base model's\n",
    "reasoning capability (preserved through LoRA) can enhance response quality\n",
    "when combined with the fine-tuned Peterson domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_v2_thinking(question: str, max_new_tokens: int = 500) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Generate a response in thinking mode.\n",
    "\n",
    "    Returns (thinking_content, final_answer) as separate strings so the caller\n",
    "    can choose to display just the answer, just the reasoning, or both.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    text   = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True,   # activates chain-of-thought reasoning\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens     = max_new_tokens,\n",
    "            do_sample          = True,\n",
    "            temperature        = 0.6,   # Qwen3 recommended for thinking mode\n",
    "            top_p              = 0.95,\n",
    "            top_k              = 20,\n",
    "            repetition_penalty = 1.1,\n",
    "        )\n",
    "\n",
    "    full   = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                               skip_special_tokens=True).strip()\n",
    "    # Extract thinking block and final answer\n",
    "    think_match = re.search(r'<think>(.*?)</think>', full, re.DOTALL)\n",
    "    thinking    = think_match.group(1).strip() if think_match else \"\"\n",
    "    answer      = re.sub(r'<think>.*?</think>', '', full, flags=re.DOTALL).strip()\n",
    "    return thinking, answer\n",
    "\n",
    "\n",
    "test_q = \"What is the relationship between order and chaos in human experience?\"\n",
    "print(f\"Q: {test_q}\")\n",
    "print()\n",
    "thinking, answer = ask_v2_thinking(test_q)\n",
    "if thinking:\n",
    "    print(f\"[Thinking]\\n{thinking[:400]}{'â€¦' if len(thinking)>400 else ''}\\n\")\n",
    "print(f\"[Answer]\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e69426",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "### What Changed and Why\n",
    "\n",
    "Three targeted improvements addressed the root causes identified in the V1\n",
    "post-mortem:\n",
    "\n",
    "**1. Synthetic Q&A training data** resolved the passage-regurgitation problem.\n",
    "By training on `(question, Peterson passage)` pairs instead of\n",
    "`(passage fragment, passage continuation)` pairs, the model learned to answer\n",
    "questions in Peterson's voice rather than to continue book text.  This is the\n",
    "single highest-leverage change.\n",
    "\n",
    "**2. Three training epochs** moved the model out of the memorisation phase and\n",
    "into genuine stylistic generalisation.  After three full passes over the\n",
    "~2,400 Q&A examples the model has learned to apply Peterson's vocabulary,\n",
    "framing, and rhetorical style to questions it has never seen during training.\n",
    "\n",
    "**3. LoRA rank r=32** doubled the adapter's representational capacity, giving\n",
    "it enough independent directions to simultaneously encode Peterson's thematic\n",
    "vocabulary, sentence rhythm, and conceptual framework â€” aspects of style that\n",
    "compete for the limited parameter budget at r=16.\n",
    "\n",
    "### Running the Comparison\n",
    "\n",
    "To evaluate V2 against V1 and both base models, update the all-models\n",
    "comparison notebook:\n",
    "\n",
    "1. Delete (or rename) `comparison_cache_all_models/qwen3_tuned_results.pkl`\n",
    "   so the comparison notebook re-runs inference for the fine-tuned Qwen3 model\n",
    "2. Update `QWEN3_LORA_PATH` in the comparison notebook to point to\n",
    "   `./outputs/qwen3_14b_peterson_v2_lora/`\n",
    "3. Re-run `AllModels_JordanPeterson_Comparison.ipynb`\n",
    "\n",
    "### Paths to Further Improvement\n",
    "\n",
    "If V2 still shows limitations, the next levers in order of expected impact:\n",
    "\n",
    "| Option | Expected gain | Effort |\n",
    "|--------|--------------|--------|\n",
    "| 5 epochs (instead of 3) | Marginal style improvement; risk of overfitting | Low |\n",
    "| Larger Q&A dataset (3 questions/passage) | More data diversity | Medium |\n",
    "| `r=64, alpha=64` | More adapter capacity; +200 MB checkpoint | Low |\n",
    "| Supervised answers (human-edited responses) | Highest quality ceiling | High |\n",
    "| Qwen3-32B (if it fits in 24 GB) | Stronger base model | High |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
