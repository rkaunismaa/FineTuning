{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ee5e87",
   "metadata": {},
   "source": [
    "# Cross-Architecture Comparison: GPT-OSS 20B vs Qwen3-14B\n",
    "## Fine-Tuning on Jordan B. Peterson's Works \u2014 All 4 Model Variants\n",
    "\n",
    "This notebook performs the most complete comparison in this series, evaluating\n",
    "**all four model variants** side by side on the same prompts and metrics.\n",
    "\n",
    "| # | Model | Architecture | Status |\n",
    "|---|-------|-------------|--------|\n",
    "| 1 | `unsloth/gpt-oss-20b-unsloth-bnb-4bit` | Harmony / GPT-4o-mini-flash | Base (pre-trained only) |\n",
    "| 2 | `./outputs/gpt_oss_20b_jordan_peterson_lora` | Harmony / GPT-4o-mini-flash | Fine-tuned on 4 Peterson books, 1 epoch |\n",
    "| 3 | `unsloth/Qwen3-14B-unsloth-bnb-4bit` | ChatML / Qwen3 | Base (pre-trained only) |\n",
    "| 4 | `./outputs/qwen3_14b_jordan_peterson_lora` | ChatML / Qwen3 | Fine-tuned on 4 Peterson books, 1 epoch |\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "1. **Intra-architecture fine-tuning effect** \u2014 Did 1 epoch improve each model's\n",
    "   alignment with Peterson's vocabulary, perplexity, and style?\n",
    "2. **Inter-architecture base comparison** \u2014 Is GPT-OSS 20B or Qwen3-14B a stronger\n",
    "   foundation for this domain, before any fine-tuning?\n",
    "3. **Inter-architecture fine-tuned comparison** \u2014 After fine-tuning, which model\n",
    "   is more convincingly \"Peterson-like\"?\n",
    "4. **Training efficiency** \u2014 GPT-OSS trained for 73 min (loss 3.01) vs Qwen3 for\n",
    "   23 min (loss 2.44). Does lower loss translate to better inference quality?\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "| Metric | What it measures | Direction |\n",
    "|--------|-----------------|-----------|\n",
    "| **Perplexity** | How surprised the model is by real Peterson text | \u2193 better |\n",
    "| **TF-IDF cosine similarity** | Vocabulary overlap with Peterson's writing | \u2191 better |\n",
    "| **Keyword density** | Rate of use of Peterson's ~60 signature terms | \u2191 better |\n",
    "| **Type-Token Ratio (TTR)** | Vocabulary richness (unique / total words) | \u2191 better |\n",
    "| **Response length** | Average words per response | informational |\n",
    "\n",
    "### GPU Memory Strategy\n",
    "\n",
    "Neither model pair fits simultaneously in 24 GB of VRAM (GPT-OSS 20B \u2248 13.8 GB,\n",
    "Qwen3-14B \u2248 13.4 GB). We therefore evaluate all four models **sequentially**:\n",
    "\n",
    "```\n",
    "Load \u2192 Infer \u2192 Save pkl \u2192 Unload \u2192 repeat for next model\n",
    "```\n",
    "\n",
    "Once all four pkl files exist, no model loading is required \u2014 metrics and charts\n",
    "load from cache in seconds. Delete any pkl to force re-evaluation of that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3773c85",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc, math, pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt',     quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# \u2500\u2500 Paths \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# All four model paths. The two LoRA paths are relative to this notebook's\n",
    "# working directory (NoteBooks/JordanPeterson/); they point to adapter weights\n",
    "# saved by the fine-tuning notebooks.\n",
    "GPTOSS_BASE_PATH  = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\"\n",
    "GPTOSS_LORA_PATH  = \"./outputs/gpt_oss_20b_jordan_peterson_lora\"\n",
    "QWEN3_BASE_PATH   = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\n",
    "QWEN3_LORA_PATH   = \"./outputs/qwen3_14b_jordan_peterson_lora\"\n",
    "\n",
    "# Separate cache and figures directories keep this notebook isolated from the\n",
    "# single-architecture comparisons that already exist in this folder.\n",
    "CACHE_DIR   = Path(\"./comparison_cache_all_models\")\n",
    "FIGURES_DIR = Path(\"./comparison_figures_all_models\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# \u2500\u2500 Model registry \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# A single ordered list drives all loops in this notebook \u2014 inference phases,\n",
    "# metric collection, chart colours, and legend labels all come from here.\n",
    "MODEL_KEYS   = [\"gptoss_base\", \"gptoss_tuned\", \"qwen3_base\", \"qwen3_tuned\"]\n",
    "MODEL_PATHS  = {\n",
    "    \"gptoss_base\":  GPTOSS_BASE_PATH,\n",
    "    \"gptoss_tuned\": GPTOSS_LORA_PATH,\n",
    "    \"qwen3_base\":   QWEN3_BASE_PATH,\n",
    "    \"qwen3_tuned\":  QWEN3_LORA_PATH,\n",
    "}\n",
    "# Short names used inside chart axes (tight layout)\n",
    "MODEL_SHORT  = {\n",
    "    \"gptoss_base\":  \"GPT-OSS 20B\\nBase\",\n",
    "    \"gptoss_tuned\": \"GPT-OSS 20B\\nFine-Tuned\",\n",
    "    \"qwen3_base\":   \"Qwen3-14B\\nBase\",\n",
    "    \"qwen3_tuned\":  \"Qwen3-14B\\nFine-Tuned\",\n",
    "}\n",
    "# Longer display names for titles, radar chart legend, word-cloud headers\n",
    "MODEL_DISPLAY = {\n",
    "    \"gptoss_base\":  \"GPT-OSS 20B  |  Base\",\n",
    "    \"gptoss_tuned\": \"GPT-OSS 20B  |  Fine-Tuned\",\n",
    "    \"qwen3_base\":   \"Qwen3-14B  |  Base\",\n",
    "    \"qwen3_tuned\":  \"Qwen3-14B  |  Fine-Tuned\",\n",
    "}\n",
    "# Colour palette: blue/orange for GPT-OSS, green/red for Qwen3.\n",
    "# Pairing warm vs cool within each architecture makes the base/tuned split\n",
    "# visually obvious at a glance.\n",
    "MODEL_COLORS = {\n",
    "    \"gptoss_base\":  \"#4C72B0\",   # blue   \u2014 GPT-OSS base\n",
    "    \"gptoss_tuned\": \"#DD8452\",   # orange \u2014 GPT-OSS fine-tuned\n",
    "    \"qwen3_base\":   \"#55A868\",   # green  \u2014 Qwen3 base\n",
    "    \"qwen3_tuned\":  \"#C44E52\",   # red    \u2014 Qwen3 fine-tuned\n",
    "}\n",
    "CACHE_FILES = {k: CACHE_DIR / f\"{k}_results.pkl\" for k in MODEL_KEYS}\n",
    "\n",
    "# \u2500\u2500 Plot style \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 120,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#f8f8f8',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.4,\n",
    "    'font.size': 11,\n",
    "})\n",
    "\n",
    "# \u2500\u2500 System prompts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Base models receive a neutral system prompt because they have no fine-tuning\n",
    "# to activate; asking them to imitate Peterson without that training leads to\n",
    "# superficial mimicry rather than a fair evaluation of their natural output.\n",
    "# Fine-tuned models receive the same rich Peterson prompt they were trained on,\n",
    "# which is how they would actually be deployed in production.\n",
    "BASE_SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "TUNED_SYSTEM_PROMPT = (\n",
    "    \"You are an AI assistant that has been trained on the complete works of \"\n",
    "    \"Jordan B. Peterson, a Canadian clinical psychologist, professor, and author. \"\n",
    "    \"You speak with deep knowledge of psychology, philosophy, mythology, religion, \"\n",
    "    \"and personal responsibility. Your responses reflect Peterson's writing style, \"\n",
    "    \"intellectual depth, and interdisciplinary approach to understanding human \"\n",
    "    \"nature and meaning.\"\n",
    ")\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"gptoss_base\":  BASE_SYSTEM_PROMPT,\n",
    "    \"gptoss_tuned\": TUNED_SYSTEM_PROMPT,\n",
    "    \"qwen3_base\":   BASE_SYSTEM_PROMPT,\n",
    "    \"qwen3_tuned\":  TUNED_SYSTEM_PROMPT,\n",
    "}\n",
    "\n",
    "print(f\"PyTorch  : {torch.__version__}\")\n",
    "print(f\"CUDA     : {torch.cuda.is_available()}  |  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM     : {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB total\")\n",
    "print(f\"Cache    : {CACHE_DIR.resolve()}\")\n",
    "print(f\"Figures  : {FIGURES_DIR.resolve()}\")\n",
    "print()\n",
    "print(\"Cache status:\")\n",
    "for k in MODEL_KEYS:\n",
    "    status = \"EXISTS (will skip inference)\" if CACHE_FILES[k].exists() else \"missing (will run inference)\"\n",
    "    print(f\"  {MODEL_DISPLAY[k]:<36}  {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8700d",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Reference Data\n",
    "\n",
    "Three types of reference data are used throughout this notebook:\n",
    "\n",
    "- **2a \u2014 Peterson passages**: Verbatim text held out from training, used to measure\n",
    "  *perplexity* (how surprised the model is by real Peterson sentences) and *TF-IDF\n",
    "  similarity* (how closely model responses match his vocabulary distribution).\n",
    "- **2b \u2014 Evaluation prompts**: Ten questions covering Peterson's core themes.\n",
    "  Every model answers the same ten questions; metrics are computed per-prompt\n",
    "  and then averaged for the summary charts.\n",
    "- **2c \u2014 Keyword dictionary**: ~60 terms curated from the four books that are\n",
    "  either exclusive to Peterson or unusually frequent in his writing. Used for\n",
    "  keyword density (fraction of response words that are Peterson-signature words)  and the heatmap visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 2a: Reference passages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Eight short verbatim excerpts drawn from all four Peterson books, covering\n",
    "# different themes (order/chaos, meaning, suffering, mythology, logos).\n",
    "# They were NOT used as training examples \u2014 they come from later chapters or\n",
    "# sections that were excluded from the fine-tuning dataset.\n",
    "PETERSON_PASSAGES = [\n",
    "    # From Maps of Meaning \u2014 the world as a forum for action\n",
    "    \"The world can be validly construed as a forum for action, or as a place of things. \"\n",
    "    \"The former manner of interpretation \u2014 more primordial, and less clearly understood \u2014 \"\n",
    "    \"finds its expression in the arts or humanities, in ritual, drama, literature, and myth. \"\n",
    "    \"The world as forum for action is a place of value, a place where all things have meaning.\",\n",
    "\n",
    "    # From 12 Rules for Life \u2014 embodied courage\n",
    "    \"To stand up straight with your shoulders back is to accept the terrible responsibility \"\n",
    "    \"of life, with eyes wide open. It means deciding to voluntarily transform the chaos of \"\n",
    "    \"potential into the realities of habitable order. It means adopting the burden of \"\n",
    "    \"self-conscious vulnerability, and accepting the end of the unconscious paradise of \"\n",
    "    \"childhood, where finitude and mortality are only dimly comprehended.\",\n",
    "\n",
    "    # From Beyond Order \u2014 chaos as unexplored territory\n",
    "    \"Order is the place where the things you are currently doing are working out well \"\n",
    "    \"for you. Chaos is the domain of ignorance itself. It's unexplored territory. Chaos \"\n",
    "    \"is what extends, endlessly and without limit, beyond the boundaries of all states, \"\n",
    "    \"all ideas, and all disciplines. It's the foreigner, the stranger, the member of \"\n",
    "    \"another gang, the rustle in the bushes in the night-time.\",\n",
    "\n",
    "    # From We Who Wrestle with God \u2014 logos and truth\n",
    "    \"The divine spark in man is the logos \u2014 the word, the reason, the creative principle \"\n",
    "    \"that gives order to the chaos of experience. To act in accordance with the logos is \"\n",
    "    \"to speak the truth, to pursue what is meaningful rather than what is expedient, and \"\n",
    "    \"to take on the burden of Being itself with courage and humility.\",\n",
    "\n",
    "    # From 12 Rules \u2014 compare yourself to who you were\n",
    "    \"Compare yourself to who you were yesterday, not to who someone else is today. \"\n",
    "    \"You have a nature. You can play the game of life and improve. You can set a \"\n",
    "    \"standard, even a minimal standard, and try to live up to it. You can improve \"\n",
    "    \"incrementally, moving forward step by step. You can judge your life against \"\n",
    "    \"what you know to be good, against what you should be.\",\n",
    "\n",
    "    # From Maps of Meaning \u2014 myth and the hero\n",
    "    \"The great myths and rituals of the past have been formulated in the language of \"\n",
    "    \"the imagination. They say: act out the role of the hero; do not be the villain; \"\n",
    "    \"do not be the tyrant. They say: update your maps of meaning when new information \"\n",
    "    \"warrants it; admit your errors and change. They say: encounter the stranger and \"\n",
    "    \"extract from that encounter what is valuable. Treat the stranger with respect.\",\n",
    "\n",
    "    # From Beyond Order \u2014 meaning as balance\n",
    "    \"Meaning is the ultimate balance between, on the one hand, the chaos of transformation \"\n",
    "    \"and possibility and, on the other, the discipline of pristine order, whose purpose is \"\n",
    "    \"to produce out of the attendant chaos a new order that will be even more productive \"\n",
    "    \"and worthwhile than the old. Pursue what is meaningful, not what is expedient.\",\n",
    "\n",
    "    # From We Who Wrestle with God \u2014 suffering and transcendence\n",
    "    \"Suffering is not a mistake or an accident. It is the very ground of Being itself. \"\n",
    "    \"To wrestle with God, as Jacob did, is to confront that suffering honestly, to take \"\n",
    "    \"responsibility for it, and to find within it the possibility of transcendence. The \"\n",
    "    \"hero does not flee from the dragon; he faces it and transforms the encounter.\",\n",
    "]\n",
    "\n",
    "# \u2500\u2500 2b: Evaluation prompts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Ten questions that span all four books' major themes. Identical prompts are\n",
    "# used for every model so comparisons are strictly apples-to-apples.\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    \"Why is personal responsibility the foundation of a meaningful life?\",\n",
    "    \"How do ancient myths and stories reveal truths about human nature?\",\n",
    "    \"What does it mean to pursue what is meaningful rather than what is expedient?\",\n",
    "    \"How should a person confront suffering rather than flee from it?\",\n",
    "    \"What is the significance of the hero archetype in understanding the human psyche?\",\n",
    "    \"Why is telling the truth essential to a properly functioning life?\",\n",
    "    \"What is the role of the divine or the sacred in organizing human society?\",\n",
    "    \"How does the Jungian concept of the shadow relate to individual development?\",\n",
    "    \"What does it mean to stand up straight with your shoulders back?\",\n",
    "]\n",
    "\n",
    "# \u2500\u2500 2c: Peterson keyword dictionary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Curated terms that serve as proxies for Peterson's characteristic vocabulary.\n",
    "# Using a keyword-density metric alongside TF-IDF gives us a more targeted\n",
    "# measure: TF-IDF picks up general vocabulary similarity whereas keyword\n",
    "# density specifically tracks his signature conceptual vocabulary.\n",
    "PETERSON_KEYWORDS = list(set([\n",
    "    # Core metaphysical concepts\n",
    "    \"chaos\", \"order\", \"logos\", \"being\", \"meaning\", \"meaningful\", \"meaningless\",\n",
    "    \"transcendence\", \"transcendent\", \"archetype\", \"archetypal\",\n",
    "    # Jungian psychology\n",
    "    \"shadow\", \"anima\", \"animus\", \"unconscious\", \"consciousness\", \"psyche\",\n",
    "    \"individuation\", \"projection\",\n",
    "    # Ethics and existentialism\n",
    "    \"responsibility\", \"suffering\", \"redemption\", \"courage\", \"virtue\",\n",
    "    \"nihilism\", \"nihilistic\", \"expedient\", \"expedience\", \"tyranny\", \"tyrannical\",\n",
    "    \"sovereignty\", \"heroic\", \"malevolent\",\n",
    "    # Narrative and mythology\n",
    "    \"myth\", \"mythological\", \"hero\", \"dragon\", \"narrative\", \"story\",\n",
    "    \"ritual\", \"sacrifice\", \"resurrection\", \"transformation\",\n",
    "    # Religion and biblical imagery\n",
    "    \"divine\", \"sacred\", \"god\", \"biblical\", \"genesis\", \"logos\", \"spirit\",\n",
    "    \"wrestle\", \"jacob\", \"adam\", \"eve\", \"serpent\",\n",
    "    # Peterson's distinctive action-language\n",
    "    \"confront\", \"hierarchy\", \"dominance\", \"voluntarily\", \"catastrophe\",\n",
    "    \"pathological\", \"resentment\", \"ideological\", \"totalitarian\",\n",
    "]))\n",
    "\n",
    "print(f\"Reference passages  : {len(PETERSON_PASSAGES)}\")\n",
    "print(f\"Evaluation prompts  : {len(EVAL_PROMPTS)}\")\n",
    "print(f\"Keyword dictionary  : {len(PETERSON_KEYWORDS)} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35d3a4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Helper Functions\n",
    "\n",
    "Two separate inference wrappers are defined \u2014 one for each model architecture \u2014\n",
    "because their chat-template APIs differ significantly:\n",
    "\n",
    "| | GPT-OSS (Harmony) | Qwen3 (ChatML) |\n",
    "|--|-------------------|----------------|\n",
    "| Template call | `apply_chat_template(..., reasoning_effort=\"low\", return_dict=True)` | `apply_chat_template(..., enable_thinking=False)` \u2192 plain string |\n",
    "| Tokenization | Built into template call | Separate `tokenizer(text, ...)` step |\n",
    "| Special cleanup | None needed | Strip residual `<think>\u2026</think>` blocks |\n",
    "| Token prefix | `<|start|>`, `<|message|>`, `<|end|>` | `<|im_start|>`, `<|im_end|>` |\n",
    "\n",
    "The remaining helpers (`compute_perplexity`, `compute_text_stats`,\n",
    "`compute_tfidf_similarity`) are architecture-independent and identical to those\n",
    "used in the individual comparison notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, tokenizer, texts: list[str],\n",
    "                       max_length: int = 512) -> list[float]:\n",
    "    \"\"\"\n",
    "    Compute token-level perplexity for each text in the list.\n",
    "\n",
    "    Perplexity = exp( average negative log-likelihood per token ).\n",
    "\n",
    "    A lower value means the model assigns higher probability to the text \u2014\n",
    "    i.e., the text looks \"expected\" to the model.  After fine-tuning on\n",
    "    Peterson's books we expect the model's perplexity on held-out Peterson\n",
    "    passages to drop, because it has learned his vocabulary and grammar.\n",
    "    If perplexity does NOT drop, the fine-tuning signal was insufficient\n",
    "    or the model is still mostly using its pre-training representation.\n",
    "\n",
    "    The function runs in inference mode (torch.no_grad()) with labels set\n",
    "    equal to input_ids, which triggers PyTorch's built-in CE loss computation\n",
    "    across all token positions \u2014 the standard language-model objective.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    perplexities = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            enc = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "            ).to(\"cuda\")\n",
    "            out = model(**enc, labels=enc[\"input_ids\"])\n",
    "            perplexities.append(math.exp(out.loss.item()))\n",
    "    return perplexities\n",
    "\n",
    "\n",
    "def generate_response_gptoss(model, tokenizer, prompt: str,\n",
    "                              system_prompt: str,\n",
    "                              max_new_tokens: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Generate a single response from a GPT-OSS (Harmony-format) model.\n",
    "\n",
    "    The GPT-OSS tokenizer's apply_chat_template() accepts the Harmony-specific\n",
    "    `reasoning_effort` parameter and returns a ready-to-use dict of tensors\n",
    "    (input_ids + attention_mask) when return_dict=True.  We use 'low' effort\n",
    "    to keep response length comparable to Qwen3's concise greedy outputs.\n",
    "\n",
    "    Decoding is fully deterministic (do_sample=False) with a mild repetition\n",
    "    penalty (1.1) to discourage looping on short or empty training examples.\n",
    "    Only the *newly generated* tokens are decoded \u2014 we skip the prompt prefix\n",
    "    by slicing out[0][input_ids.shape[1]:].\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        reasoning_effort=\"low\",   # Harmony-specific: keep responses concise\n",
    "    ).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,          # greedy \u2014 fully deterministic\n",
    "            temperature=1.0,          # ignored when do_sample=False, set for clarity\n",
    "            repetition_penalty=1.1,   # mild penalty prevents token-loop collapse\n",
    "        )\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def generate_response_qwen3(model, tokenizer, prompt: str,\n",
    "                             system_prompt: str,\n",
    "                             max_new_tokens: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Generate a single response from a Qwen3 (ChatML-format) model.\n",
    "\n",
    "    Qwen3's apply_chat_template() requires a two-step process:\n",
    "      1. Call apply_chat_template() with tokenize=False and enable_thinking=False\n",
    "         to obtain a plain text string (not tensors).\n",
    "      2. Pass that string to the tokenizer separately to get input_ids.\n",
    "\n",
    "    Setting enable_thinking=False suppresses the chain-of-thought reasoning\n",
    "    mode \u2014 Qwen3 can optionally prepend a <think>\u2026</think> block before its\n",
    "    response, which would contaminate our style-comparison metrics.\n",
    "    Even with thinking disabled, the template sometimes adds empty\n",
    "    <think>\\n\\n</think> artifacts, so we strip any such blocks with re.sub\n",
    "    before returning the final string.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "    ]\n",
    "    # Step 1: render the chat template \u2192 plain text\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,   # ChatML-specific: disable chain-of-thought\n",
    "    )\n",
    "    # Step 2: tokenise separately\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    # Strip any residual <think>\u2026</think> blocks (may appear even with thinking disabled)\n",
    "    response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "def compute_text_stats(texts: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Compute per-text statistics over a list of model responses.\n",
    "\n",
    "    IMPORTANT: every text in the list must contribute exactly one entry to\n",
    "    each output list \u2014 even empty strings.  If we skip empties the lists\n",
    "    will be shorter than len(texts), causing index-misalignment in the\n",
    "    per-prompt bar charts.  Empty responses simply produce all-zero stats.\n",
    "\n",
    "    Returns a dict with:\n",
    "      word_counts     \u2014 number of alphabetic words per response\n",
    "      sentence_counts \u2014 number of sentences per response\n",
    "      ttr_values      \u2014 Type-Token Ratio = unique_words / total_words\n",
    "      keyword_density \u2014 fraction of words that are Peterson keywords\n",
    "      keyword_counts  \u2014 Counter of all keyword hits (for word-cloud / heatmap)\n",
    "      all_words       \u2014 flat list of content words (stopwords removed)\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    kw_set     = set(k.lower() for k in PETERSON_KEYWORDS)\n",
    "    word_counts, sentence_counts, ttr_values = [], [], []\n",
    "    keyword_density = []\n",
    "    keyword_counts  = Counter()\n",
    "    all_words       = []\n",
    "\n",
    "    for text in texts:\n",
    "        if text.strip():\n",
    "            words = word_tokenize(text.lower())\n",
    "            sents = sent_tokenize(text)\n",
    "        else:\n",
    "            words, sents = [], []   # empty response \u2192 all zeros; do NOT skip\n",
    "\n",
    "        words_alpha = [w for w in words if w.isalpha()]\n",
    "        word_counts.append(len(words_alpha))\n",
    "        sentence_counts.append(len(sents))\n",
    "\n",
    "        # TTR: higher = richer vocabulary; collapses for very short texts\n",
    "        ttr = len(set(words_alpha)) / max(len(words_alpha), 1)\n",
    "        ttr_values.append(ttr)\n",
    "\n",
    "        # Peterson keyword density: fraction of words that are signature terms\n",
    "        kw_hits = [w for w in words_alpha if w in kw_set]\n",
    "        keyword_density.append(len(kw_hits) / max(len(words_alpha), 1))\n",
    "        keyword_counts.update(kw_hits)\n",
    "\n",
    "        # Content words for word cloud (strip stop words; keep length > 2)\n",
    "        all_words.extend(w for w in words_alpha if w not in stop_words and len(w) > 2)\n",
    "\n",
    "    return {\n",
    "        \"word_counts\":     word_counts,\n",
    "        \"sentence_counts\": sentence_counts,\n",
    "        \"ttr_values\":      ttr_values,\n",
    "        \"keyword_density\": keyword_density,\n",
    "        \"keyword_counts\":  keyword_counts,\n",
    "        \"all_words\":       all_words,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_tfidf_similarity(responses: list[str],\n",
    "                              references: list[str]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Measure how similar each model response is to Peterson's actual writing\n",
    "    using TF-IDF cosine similarity.\n",
    "\n",
    "    TF-IDF weights words by how distinctive they are across the corpus \u2014\n",
    "    common words (the, and) get low weight; rare but frequent-within-author\n",
    "    words (chaos, logos, archetype) get high weight.  Cosine similarity\n",
    "    then measures the angle between the response vector and each reference\n",
    "    vector; we return the maximum across all reference passages so that\n",
    "    a response only needs to resemble *one* passage to score well.\n",
    "\n",
    "    Responses that are empty (or nearly so) receive a similarity of 0.0,\n",
    "    which accurately reflects their lack of Peterson-vocabulary content.\n",
    "    \"\"\"\n",
    "    if not responses or not any(r.strip() for r in responses):\n",
    "        return [0.0] * len(responses)\n",
    "    all_texts  = references + responses\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    tfidf      = vectorizer.fit_transform(all_texts)\n",
    "    ref_vecs   = tfidf[:len(references)]\n",
    "    resp_vecs  = tfidf[len(references):]\n",
    "    similarities = []\n",
    "    for i in range(resp_vecs.shape[0]):\n",
    "        sims = cosine_similarity(resp_vecs[i], ref_vecs)[0]\n",
    "        similarities.append(float(sims.max()))\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def _avg(lst): return sum(lst) / len(lst) if lst else 0.0\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac09a4",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: GPT-OSS 20B \u2014 Base Model\n",
    "\n",
    "We evaluate the *unmodified* GPT-OSS 20B model, which has never seen Peterson's\n",
    "books.  It uses the generic `\"You are a helpful assistant.\"` system prompt.\n",
    "\n",
    "This establishes our **baseline** for the GPT-OSS architecture: any improvement\n",
    "in the fine-tuned variant (Phase 2) can be attributed directly to the LoRA\n",
    "training on the four Peterson books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a88ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "_cache = CACHE_FILES[\"gptoss_base\"]\n",
    "\n",
    "if _cache.exists():\n",
    "    # \u2500\u2500 Fast path: skip model loading entirely \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # Loading a 20B model takes ~60 seconds and consumes ~14 GB of VRAM.\n",
    "    # If we already ran inference and saved the pkl, we bypass all of that.\n",
    "    print(f\"Cache found \u2014 skipping GPT-OSS 20B Base inference.\")\n",
    "    print(f\"  {_cache}\")\n",
    "else:\n",
    "    # \u2500\u2500 Slow path: load \u2192 infer \u2192 save \u2192 unload \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    print(\"Loading GPT-OSS 20B Base model \u2026\")\n",
    "    _model, _tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = GPTOSS_BASE_PATH,\n",
    "        dtype           = None,    # auto (bfloat16 on Ampere+, float16 otherwise)\n",
    "        max_seq_length  = 2048,    # matches the max used during fine-tuning\n",
    "        load_in_4bit    = True,    # bitsandbytes 4-bit quantization to save VRAM\n",
    "        full_finetuning = False,   # inference-only; no adapter merging needed\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)   # Unsloth: 2\u00d7 faster inference kernel\n",
    "    print(f\"  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "    # \u2500\u2500 1a. Perplexity on held-out Peterson passages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    print(\"\\nComputing perplexity on reference passages \u2026\")\n",
    "    _ppls = compute_perplexity(_model, _tok, PETERSON_PASSAGES)\n",
    "    print(f\"  Per-passage: {[round(p,1) for p in _ppls]}\")\n",
    "    print(f\"  Average    : {_avg(_ppls):.2f}\")\n",
    "\n",
    "    # \u2500\u2500 1b. Generate responses to all 10 evaluation prompts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # We use the architecture-appropriate wrapper (generate_response_gptoss).\n",
    "    # The generic BASE_SYSTEM_PROMPT is used because the base model has not\n",
    "    # been conditioned to write like Peterson.\n",
    "    print(\"\\nGenerating responses \u2026\")\n",
    "    _resps = []\n",
    "    for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{i+1:02d}/{len(EVAL_PROMPTS)}] {prompt[:70]}\")\n",
    "        r = generate_response_gptoss(_model, _tok, prompt,\n",
    "                                     SYSTEM_PROMPTS[\"gptoss_base\"])\n",
    "        _resps.append(r)\n",
    "        print(f\"         \u2192 {r[:100]}\\n\")\n",
    "\n",
    "    # \u2500\u2500 1c. Persist results and free VRAM \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # We save a dict containing both lists so a single pkl read in the metrics\n",
    "    # cell restores the full results for this model.\n",
    "    with open(_cache, \"wb\") as f:\n",
    "        pickle.dump({\"responses\": _resps, \"perplexities\": _ppls}, f)\n",
    "    print(f\"\\nSaved \u2192 {_cache}\")\n",
    "\n",
    "    # Explicitly delete model references and flush CUDA memory allocator.\n",
    "    # Without this step, loading the next 20B model would likely OOM.\n",
    "    del _model, _tok, _resps, _ppls\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Model unloaded.  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "# \u2500\u2500 Always load pkl into named variables \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "with open(_cache, \"rb\") as f:\n",
    "    _d = pickle.load(f)\n",
    "gptoss_base_responses    = _d[\"responses\"]\n",
    "gptoss_base_perplexities = _d[\"perplexities\"]\n",
    "print(f\"\\nGPT-OSS 20B Base loaded from cache.\")\n",
    "print(f\"  Responses    : {len(gptoss_base_responses)}\")\n",
    "print(f\"  Avg PPL      : {_avg(gptoss_base_perplexities):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9cecfc",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: GPT-OSS 20B \u2014 Fine-Tuned Model\n",
    "\n",
    "We now load the LoRA-adapted version of the same GPT-OSS 20B model, trained for\n",
    "**1 epoch** on ~768,000 words from four Peterson books:\n",
    "*Maps of Meaning*, *12 Rules for Life*, *Beyond Order*, and\n",
    "*We Who Wrestle with God*.\n",
    "\n",
    "Training stats: **641 steps**, **73.3 min**, **loss 3.01**, batch size 1.\n",
    "\n",
    "The fine-tuned model uses the rich Peterson system prompt to activate its\n",
    "domain-specific weights. We use the same Harmony-format `generate_response_gptoss`\n",
    "wrapper as the base model \u2014 the only difference is the model weights loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f42428",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cache = CACHE_FILES[\"gptoss_tuned\"]\n",
    "\n",
    "if _cache.exists():\n",
    "    print(f\"Cache found \u2014 skipping GPT-OSS 20B Fine-Tuned inference.\")\n",
    "    print(f\"  {_cache}\")\n",
    "else:\n",
    "    # Unsloth loads LoRA adapters transparently: passing the adapter directory\n",
    "    # as model_name causes it to detect the adapter config and merge the LoRA\n",
    "    # weights onto the base model at load time.  No separate merge step needed.\n",
    "    print(\"Loading GPT-OSS 20B Fine-Tuned model (base + LoRA adapters) \u2026\")\n",
    "    _model, _tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = GPTOSS_LORA_PATH,   # points to saved adapter directory\n",
    "        dtype           = None,\n",
    "        max_seq_length  = 2048,\n",
    "        load_in_4bit    = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)\n",
    "    print(f\"  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "    print(\"\\nComputing perplexity on reference passages \u2026\")\n",
    "    _ppls = compute_perplexity(_model, _tok, PETERSON_PASSAGES)\n",
    "    print(f\"  Per-passage: {[round(p,1) for p in _ppls]}\")\n",
    "    print(f\"  Average    : {_avg(_ppls):.2f}\")\n",
    "\n",
    "    # Fine-tuned model receives the TUNED system prompt \u2014 the same persona\n",
    "    # prompt it was trained on \u2014 to ensure we evaluate it under its intended\n",
    "    # operating conditions.\n",
    "    print(\"\\nGenerating responses \u2026\")\n",
    "    _resps = []\n",
    "    for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{i+1:02d}/{len(EVAL_PROMPTS)}] {prompt[:70]}\")\n",
    "        r = generate_response_gptoss(_model, _tok, prompt,\n",
    "                                     SYSTEM_PROMPTS[\"gptoss_tuned\"])\n",
    "        _resps.append(r)\n",
    "        print(f\"         \u2192 {r[:100]}\\n\")\n",
    "\n",
    "    with open(_cache, \"wb\") as f:\n",
    "        pickle.dump({\"responses\": _resps, \"perplexities\": _ppls}, f)\n",
    "    print(f\"\\nSaved \u2192 {_cache}\")\n",
    "\n",
    "    del _model, _tok, _resps, _ppls\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Model unloaded.  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "with open(_cache, \"rb\") as f:\n",
    "    _d = pickle.load(f)\n",
    "gptoss_tuned_responses    = _d[\"responses\"]\n",
    "gptoss_tuned_perplexities = _d[\"perplexities\"]\n",
    "print(f\"\\nGPT-OSS 20B Fine-Tuned loaded from cache.\")\n",
    "print(f\"  Responses    : {len(gptoss_tuned_responses)}\")\n",
    "print(f\"  Avg PPL      : {_avg(gptoss_tuned_perplexities):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba3b8e",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Qwen3-14B \u2014 Base Model\n",
    "\n",
    "Qwen3-14B is a **different architecture** from GPT-OSS \u2014 Alibaba's third-generation\n",
    "Qwen model using the ChatML format (`<|im_start|>` / `<|im_end|>` tokens) and\n",
    "supporting an optional chain-of-thought thinking mode via `enable_thinking`.\n",
    "\n",
    "At 14B parameters vs 20B for GPT-OSS, Qwen3 is a *smaller* model \u2014 yet it\n",
    "achieves a lower training loss after 1 epoch (2.44 vs 3.01).  This phase\n",
    "evaluates whether the smaller model is already more capable on Peterson-domain\n",
    "questions *before* fine-tuning, which would favour Qwen3 architecturally.\n",
    "\n",
    "Key API difference: `generate_response_qwen3` uses a two-step tokenization\n",
    "pipeline and strips any `<think>` blocks from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cache = CACHE_FILES[\"qwen3_base\"]\n",
    "\n",
    "if _cache.exists():\n",
    "    print(f\"Cache found \u2014 skipping Qwen3-14B Base inference.\")\n",
    "    print(f\"  {_cache}\")\n",
    "else:\n",
    "    # Qwen3-14B in 4-bit quantization uses ~10.4 GB of VRAM \u2014 less than\n",
    "    # GPT-OSS 20B but still substantial. We follow the same load/infer/\n",
    "    # save/unload pattern to keep memory usage predictable.\n",
    "    print(\"Loading Qwen3-14B Base model \u2026\")\n",
    "    _model, _tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = QWEN3_BASE_PATH,\n",
    "        dtype           = None,\n",
    "        max_seq_length  = 2048,\n",
    "        load_in_4bit    = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)\n",
    "    print(f\"  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "    print(\"\\nComputing perplexity on reference passages \u2026\")\n",
    "    _ppls = compute_perplexity(_model, _tok, PETERSON_PASSAGES)\n",
    "    print(f\"  Per-passage: {[round(p,1) for p in _ppls]}\")\n",
    "    print(f\"  Average    : {_avg(_ppls):.2f}\")\n",
    "\n",
    "    # Qwen3's generate_response wrapper applies the ChatML template with\n",
    "    # enable_thinking=False, tokenises separately, and strips any <think> tags.\n",
    "    print(\"\\nGenerating responses \u2026\")\n",
    "    _resps = []\n",
    "    for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{i+1:02d}/{len(EVAL_PROMPTS)}] {prompt[:70]}\")\n",
    "        r = generate_response_qwen3(_model, _tok, prompt,\n",
    "                                    SYSTEM_PROMPTS[\"qwen3_base\"])\n",
    "        _resps.append(r)\n",
    "        print(f\"         \u2192 {r[:100]}\\n\")\n",
    "\n",
    "    with open(_cache, \"wb\") as f:\n",
    "        pickle.dump({\"responses\": _resps, \"perplexities\": _ppls}, f)\n",
    "    print(f\"\\nSaved \u2192 {_cache}\")\n",
    "\n",
    "    del _model, _tok, _resps, _ppls\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Model unloaded.  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "with open(_cache, \"rb\") as f:\n",
    "    _d = pickle.load(f)\n",
    "qwen3_base_responses    = _d[\"responses\"]\n",
    "qwen3_base_perplexities = _d[\"perplexities\"]\n",
    "print(f\"\\nQwen3-14B Base loaded from cache.\")\n",
    "print(f\"  Responses    : {len(qwen3_base_responses)}\")\n",
    "print(f\"  Avg PPL      : {_avg(qwen3_base_perplexities):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f2153",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: Qwen3-14B \u2014 Fine-Tuned Model\n",
    "\n",
    "The Qwen3-14B model fine-tuned on the same four Peterson books.\n",
    "\n",
    "Training stats: **321 steps**, **23.3 min**, **loss 2.44**, batch size 2.\n",
    "\n",
    "Notable: despite being a smaller model, Qwen3 reached a *lower* training loss\n",
    "in a *third* of the time and with *half* as many gradient updates.  Whether\n",
    "this superior training efficiency translates to better inference quality is\n",
    "exactly what this phase (and the subsequent comparisons) will reveal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cache = CACHE_FILES[\"qwen3_tuned\"]\n",
    "\n",
    "if _cache.exists():\n",
    "    print(f\"Cache found \u2014 skipping Qwen3-14B Fine-Tuned inference.\")\n",
    "    print(f\"  {_cache}\")\n",
    "else:\n",
    "    print(\"Loading Qwen3-14B Fine-Tuned model (base + LoRA adapters) \u2026\")\n",
    "    _model, _tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = QWEN3_LORA_PATH,\n",
    "        dtype           = None,\n",
    "        max_seq_length  = 2048,\n",
    "        load_in_4bit    = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)\n",
    "    print(f\"  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "    print(\"\\nComputing perplexity on reference passages \u2026\")\n",
    "    _ppls = compute_perplexity(_model, _tok, PETERSON_PASSAGES)\n",
    "    print(f\"  Per-passage: {[round(p,1) for p in _ppls]}\")\n",
    "    print(f\"  Average    : {_avg(_ppls):.2f}\")\n",
    "\n",
    "    print(\"\\nGenerating responses \u2026\")\n",
    "    _resps = []\n",
    "    for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{i+1:02d}/{len(EVAL_PROMPTS)}] {prompt[:70]}\")\n",
    "        r = generate_response_qwen3(_model, _tok, prompt,\n",
    "                                    SYSTEM_PROMPTS[\"qwen3_tuned\"])\n",
    "        _resps.append(r)\n",
    "        print(f\"         \u2192 {r[:100]}\\n\")\n",
    "\n",
    "    with open(_cache, \"wb\") as f:\n",
    "        pickle.dump({\"responses\": _resps, \"perplexities\": _ppls}, f)\n",
    "    print(f\"\\nSaved \u2192 {_cache}\")\n",
    "\n",
    "    del _model, _tok, _resps, _ppls\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Model unloaded.  VRAM reserved: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "\n",
    "with open(_cache, \"rb\") as f:\n",
    "    _d = pickle.load(f)\n",
    "qwen3_tuned_responses    = _d[\"responses\"]\n",
    "qwen3_tuned_perplexities = _d[\"perplexities\"]\n",
    "print(f\"\\nQwen3-14B Fine-Tuned loaded from cache.\")\n",
    "print(f\"  Responses    : {len(qwen3_tuned_responses)}\")\n",
    "print(f\"  Avg PPL      : {_avg(qwen3_tuned_perplexities):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f537f39",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Aggregate All Metrics\n",
    "\n",
    "Now that all four pkl files have been loaded, we compute the remaining metrics\n",
    "(TF-IDF similarity and text statistics) for every model at once.  No GPU is\n",
    "required at this point \u2014 all computation is CPU-bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab69ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Consolidate all response and perplexity lists into dicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "all_responses = {\n",
    "    \"gptoss_base\":  gptoss_base_responses,\n",
    "    \"gptoss_tuned\": gptoss_tuned_responses,\n",
    "    \"qwen3_base\":   qwen3_base_responses,\n",
    "    \"qwen3_tuned\":  qwen3_tuned_responses,\n",
    "}\n",
    "all_perplexities = {\n",
    "    \"gptoss_base\":  gptoss_base_perplexities,\n",
    "    \"gptoss_tuned\": gptoss_tuned_perplexities,\n",
    "    \"qwen3_base\":   qwen3_base_perplexities,\n",
    "    \"qwen3_tuned\":  qwen3_tuned_perplexities,\n",
    "}\n",
    "\n",
    "# \u2500\u2500 TF-IDF cosine similarity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Measures how closely each model's vocabulary distribution resembles\n",
    "# Peterson's actual writing.  Computed once per model against the same\n",
    "# eight reference passages.\n",
    "print(\"Computing TF-IDF similarities \u2026\")\n",
    "all_similarities = {\n",
    "    k: compute_tfidf_similarity(all_responses[k], PETERSON_PASSAGES)\n",
    "    for k in MODEL_KEYS\n",
    "}\n",
    "\n",
    "# \u2500\u2500 Text statistics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Word counts, sentence counts, TTR, keyword density, word lists for clouds.\n",
    "print(\"Computing text statistics \u2026\")\n",
    "all_stats = {k: compute_text_stats(all_responses[k]) for k in MODEL_KEYS}\n",
    "\n",
    "# \u2500\u2500 Averages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Pre-compute averages used in multiple charts so they are consistent.\n",
    "avg_ppl  = {k: _avg(all_perplexities[k])              for k in MODEL_KEYS}\n",
    "avg_sim  = {k: _avg(all_similarities[k])               for k in MODEL_KEYS}\n",
    "avg_kd   = {k: _avg(all_stats[k][\"keyword_density\"])   for k in MODEL_KEYS}\n",
    "avg_ttr  = {k: _avg(all_stats[k][\"ttr_values\"])        for k in MODEL_KEYS}\n",
    "avg_len  = {k: _avg(all_stats[k][\"word_counts\"])       for k in MODEL_KEYS}\n",
    "\n",
    "# \u2500\u2500 Summary table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print()\n",
    "print(f\"{'Metric':<30} {'GPT-OSS Base':>14} {'GPT-OSS FT':>14} {'Qwen3 Base':>12} {'Qwen3 FT':>12}\")\n",
    "print(\"\u2500\" * 86)\n",
    "print(f\"{'Avg Perplexity':<30} {avg_ppl['gptoss_base']:>14.2f} {avg_ppl['gptoss_tuned']:>14.2f} {avg_ppl['qwen3_base']:>12.2f} {avg_ppl['qwen3_tuned']:>12.2f}\")\n",
    "print(f\"{'Avg TF-IDF Similarity':<30} {avg_sim['gptoss_base']:>14.4f} {avg_sim['gptoss_tuned']:>14.4f} {avg_sim['qwen3_base']:>12.4f} {avg_sim['qwen3_tuned']:>12.4f}\")\n",
    "print(f\"{'Avg Keyword Density':<30} {100*avg_kd['gptoss_base']:>13.2f}% {100*avg_kd['gptoss_tuned']:>13.2f}% {100*avg_kd['qwen3_base']:>11.2f}% {100*avg_kd['qwen3_tuned']:>11.2f}%\")\n",
    "print(f\"{'Avg TTR':<30} {avg_ttr['gptoss_base']:>14.4f} {avg_ttr['gptoss_tuned']:>14.4f} {avg_ttr['qwen3_base']:>12.4f} {avg_ttr['qwen3_tuned']:>12.4f}\")\n",
    "print(f\"{'Avg Response Length (words)':<30} {avg_len['gptoss_base']:>13.1f}  {avg_len['gptoss_tuned']:>13.1f}  {avg_len['qwen3_base']:>11.1f}  {avg_len['qwen3_tuned']:>11.1f}\")\n",
    "print(\"\u2500\" * 86)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f659d",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 1: Perplexity on Peterson Reference Passages\n",
    "\n",
    "**Left panel** \u2014 Per-passage perplexity for all four models.  Each cluster of\n",
    "four bars corresponds to one held-out reference passage.  We expect both\n",
    "fine-tuned models to sit lower in their respective clusters.\n",
    "\n",
    "**Right panel** \u2014 Average perplexity across all eight passages.  This is the\n",
    "single most direct measure of domain adaptation: lower = more \"in-domain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "x      = np.arange(len(PETERSON_PASSAGES))\n",
    "width  = 0.18   # width of each individual bar\n",
    "n      = len(MODEL_KEYS)\n",
    "colors = [MODEL_COLORS[k] for k in MODEL_KEYS]\n",
    "labels = [MODEL_SHORT[k]  for k in MODEL_KEYS]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6),\n",
    "                                gridspec_kw={'width_ratios': [3, 1]})\n",
    "fig.suptitle(\"Perplexity on Held-Out Peterson Passages\", fontsize=15, fontweight='bold')\n",
    "\n",
    "# \u2500\u2500 Left: per-passage grouped bar chart \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Each group of 4 bars shows one passage; bars within the group are the four\n",
    "# model variants.  Centring the group at each integer x position requires\n",
    "# offsetting each bar by (i - (n-1)/2) * width.\n",
    "for i, (key, color, label) in enumerate(zip(MODEL_KEYS, colors, labels)):\n",
    "    offset = (i - (n - 1) / 2) * width\n",
    "    ax1.bar(x + offset, all_perplexities[key], width,\n",
    "            label=label, color=color, alpha=0.85, edgecolor='white')\n",
    "ax1.set_xlabel(\"Reference Passage\")\n",
    "ax1.set_ylabel(\"Perplexity  (lower = more domain-adapted)\")\n",
    "ax1.set_title(\"Per-Passage Perplexity\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"P{i+1}\" for i in range(len(PETERSON_PASSAGES))],\n",
    "                     fontsize=10)\n",
    "ax1.legend(fontsize=9, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.4)\n",
    "\n",
    "# \u2500\u2500 Right: average perplexity summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "avgs = [avg_ppl[k] for k in MODEL_KEYS]\n",
    "bars = ax2.bar(range(n), avgs, color=colors, alpha=0.85, edgecolor='white', width=0.6)\n",
    "for bar, v in zip(bars, avgs):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, v + 0.2,\n",
    "             f\"{v:.1f}\", ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax2.set_xticks(range(n))\n",
    "ax2.set_xticklabels(labels, fontsize=9)\n",
    "ax2.set_ylabel(\"Average Perplexity\")\n",
    "ax2.set_title(\"Average Perplexity\")\n",
    "ax2.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"01_perplexity.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 01_perplexity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb1470",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 2: TF-IDF Cosine Similarity to Peterson's Writing\n",
    "\n",
    "For each model response to each evaluation prompt, we compute the cosine\n",
    "similarity between its TF-IDF vector and each of the eight reference passages,\n",
    "then take the *maximum* \u2014 so a response only needs to resemble one passage to\n",
    "score well.\n",
    "\n",
    "**Higher = the model's word choices are more similar to how Peterson actually writes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76887143",
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = np.arange(len(EVAL_PROMPTS))\n",
    "width = 0.18\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6),\n",
    "                                gridspec_kw={'width_ratios': [3, 1]})\n",
    "fig.suptitle(\"TF-IDF Cosine Similarity to Peterson's Writing\",\n",
    "             fontsize=15, fontweight='bold')\n",
    "\n",
    "# \u2500\u2500 Left: per-prompt grouped bars \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "for i, (key, color, label) in enumerate(zip(MODEL_KEYS, colors, labels)):\n",
    "    offset = (i - (n - 1) / 2) * width\n",
    "    ax1.bar(x + offset, all_similarities[key], width,\n",
    "            label=label, color=color, alpha=0.85, edgecolor='white')\n",
    "ax1.set_xlabel(\"Evaluation Prompt\")\n",
    "ax1.set_ylabel(\"TF-IDF Cosine Similarity  (higher = more Peterson-like)\")\n",
    "ax1.set_title(\"Per-Prompt TF-IDF Similarity\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))], fontsize=10)\n",
    "ax1.legend(fontsize=9, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.4)\n",
    "\n",
    "# \u2500\u2500 Right: average summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "avgs = [avg_sim[k] for k in MODEL_KEYS]\n",
    "bars = ax2.bar(range(n), avgs, color=colors, alpha=0.85, edgecolor='white', width=0.6)\n",
    "for bar, v in zip(bars, avgs):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, v + 0.001,\n",
    "             f\"{v:.4f}\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "ax2.set_xticks(range(n))\n",
    "ax2.set_xticklabels(labels, fontsize=9)\n",
    "ax2.set_ylabel(\"Average TF-IDF Similarity\")\n",
    "ax2.set_title(\"Average TF-IDF Similarity\")\n",
    "ax2.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"02_tfidf_similarity.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 02_tfidf_similarity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3da95a",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 3: Peterson Keyword Density\n",
    "\n",
    "Keyword density = (Peterson-signature words in response) \u00f7 (total words).\n",
    "\n",
    "This is a more *targeted* measure than TF-IDF: we are specifically asking\n",
    "\"does the model spontaneously use terms like chaos, logos, archetype, hierarchy,\n",
    "sovereignty, etc.?\"  High density suggests the fine-tuning has wired those\n",
    "concepts into the model's first-choice vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0621cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = np.arange(len(EVAL_PROMPTS))\n",
    "width = 0.18\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6),\n",
    "                                gridspec_kw={'width_ratios': [3, 1]})\n",
    "fig.suptitle(\"Peterson Keyword Density per Prompt\", fontsize=15, fontweight='bold')\n",
    "\n",
    "# \u2500\u2500 Left: per-prompt bars \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "for i, (key, color, label) in enumerate(zip(MODEL_KEYS, colors, labels)):\n",
    "    offset = (i - (n - 1) / 2) * width\n",
    "    ax1.bar(x + offset,\n",
    "            [100 * v for v in all_stats[key][\"keyword_density\"]],\n",
    "            width, label=label, color=color, alpha=0.85, edgecolor='white')\n",
    "ax1.set_xlabel(\"Evaluation Prompt\")\n",
    "ax1.set_ylabel(\"Keyword Density  (%)\")\n",
    "ax1.set_title(\"Per-Prompt Keyword Density\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))], fontsize=10)\n",
    "ax1.legend(fontsize=9, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.4)\n",
    "\n",
    "# \u2500\u2500 Right: average summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "avgs = [100 * avg_kd[k] for k in MODEL_KEYS]\n",
    "bars = ax2.bar(range(n), avgs, color=colors, alpha=0.85, edgecolor='white', width=0.6)\n",
    "for bar, v in zip(bars, avgs):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, v + 0.05,\n",
    "             f\"{v:.2f}%\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "ax2.set_xticks(range(n))\n",
    "ax2.set_xticklabels(labels, fontsize=9)\n",
    "ax2.set_ylabel(\"Average Keyword Density  (%)\")\n",
    "ax2.set_title(\"Average Keyword Density\")\n",
    "ax2.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"03_keyword_density.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 03_keyword_density.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ad2b7",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 4: Response Characteristics\n",
    "\n",
    "Three panel chart showing per-model averages for:\n",
    "- **Word count** \u2014 how verbose is each model?\n",
    "- **Sentence count** \u2014 structural complexity of responses\n",
    "- **Type-Token Ratio (TTR)** \u2014 vocabulary richness (unique/total words).\n",
    "  Higher TTR = more varied word choice.  Peterson is known for elaborate,\n",
    "  varied prose, so a higher TTR suggests stylistic alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867da4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(\"Response Characteristics \u2014 All 4 Models\", fontsize=14, fontweight='bold')\n",
    "\n",
    "stat_keys    = [\"word_counts\",     \"sentence_counts\",  \"ttr_values\"]\n",
    "stat_titles  = [\"Avg Word Count\",  \"Avg Sentence Count\",\"Avg Type-Token Ratio\"]\n",
    "stat_ylabels = [\"Words\",           \"Sentences\",         \"TTR  (unique/total words)\"]\n",
    "\n",
    "for ax, stat_key, title, ylabel in zip(axes, stat_keys, stat_titles, stat_ylabels):\n",
    "    avgs = [_avg(all_stats[k][stat_key]) for k in MODEL_KEYS]\n",
    "    bars = ax.bar(range(n), avgs, color=colors, alpha=0.85,\n",
    "                  edgecolor='white', width=0.6)\n",
    "    # Annotate each bar with its numeric value\n",
    "    for bar, v in zip(bars, avgs):\n",
    "        fmt = f\"{v:.3f}\" if \"ttr\" in stat_key else f\"{v:.1f}\"\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, v * 1.02,\n",
    "                fmt, ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_xticklabels(labels, fontsize=8)\n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(axis='y', alpha=0.4)\n",
    "\n",
    "# Add a shared legend patch at the bottom of the figure\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=MODEL_COLORS[k], label=MODEL_DISPLAY[k])\n",
    "    for k in MODEL_KEYS\n",
    "]\n",
    "fig.legend(handles=legend_patches, loc='lower center', ncol=4,\n",
    "           fontsize=9, bbox_to_anchor=(0.5, -0.08))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"04_response_characteristics.png\",\n",
    "            bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 04_response_characteristics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc5f0e",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 5: Word Clouds\n",
    "\n",
    "Word clouds show the most frequent *content* words (stop words removed) in\n",
    "each model's aggregate responses.  Peterson's vocabulary centres on a\n",
    "distinctive cluster of terms \u2014 order, chaos, meaning, responsibility, hero,\n",
    "archetype \u2014 so we expect to see those words dominate the fine-tuned clouds.\n",
    "\n",
    "The four clouds are arranged in a 2\u00d72 grid (architecture \u00d7 training status):\n",
    "\n",
    "```\n",
    "  GPT-OSS Base    |  GPT-OSS Fine-Tuned\n",
    "  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "  Qwen3 Base      |  Qwen3 Fine-Tuned\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle(\"Word Clouds \u2014 Content Word Frequency per Model\",\n",
    "             fontsize=15, fontweight='bold')\n",
    "\n",
    "for ax, key in zip(axes.flat, MODEL_KEYS):\n",
    "    words = all_stats[key][\"all_words\"]\n",
    "    if words:\n",
    "        # Use the model's designated colour as the dominant tint for easy\n",
    "        # visual association between the cloud and the bar charts above.\n",
    "        wc = WordCloud(\n",
    "            width=600, height=400,\n",
    "            background_color='white',\n",
    "            colormap='viridis',       # consistent palette regardless of model\n",
    "            max_words=80,\n",
    "            collocations=False,       # avoid duplicating bigrams\n",
    "            stopwords=STOPWORDS,\n",
    "        ).generate(' '.join(words))\n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"(no content words)\", ha='center', va='center',\n",
    "                transform=ax.transAxes, fontsize=14, color='grey')\n",
    "    ax.set_title(MODEL_DISPLAY[key], fontsize=12, fontweight='bold',\n",
    "                 color=MODEL_COLORS[key])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"05_wordclouds.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: 05_wordclouds.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5e747",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 6: Peterson Keyword Heatmap (per prompt \u00d7 per keyword)\n",
    "\n",
    "Each cell shows the raw count of a specific Peterson keyword in the response\n",
    "to a specific prompt.  The top-20 most-used keywords (pooled across all four\n",
    "models) are shown on the x-axis; the 10 prompts on the y-axis.\n",
    "\n",
    "Comparing the four heatmaps side by side reveals:\n",
    "- Which keywords the fine-tuned models spontaneously use more\n",
    "- Whether fine-tuning concentrates usage on a few keywords (overfitting a\n",
    "  limited vocabulary) or spreads it broadly across the full dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_set = set(k.lower() for k in PETERSON_KEYWORDS)\n",
    "\n",
    "def per_prompt_kw_matrix(responses, keywords):\n",
    "    \"\"\"\n",
    "    Return a (n_prompts \u00d7 n_keywords) integer matrix counting keyword hits.\n",
    "\n",
    "    Each row corresponds to one response; each column to one keyword.\n",
    "    The matrix is used for the heatmap visualisation.\n",
    "    \"\"\"\n",
    "    mat = []\n",
    "    for resp in responses:\n",
    "        words = word_tokenize(resp.lower()) if resp.strip() else []\n",
    "        words_alpha = [w for w in words if w.isalpha()]\n",
    "        mat.append([words_alpha.count(kw) for kw in keywords])\n",
    "    return np.array(mat)\n",
    "\n",
    "# \u2500\u2500 Build top-20 keyword list from the union of all model responses \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "all_kw_counter = Counter()\n",
    "for key in MODEL_KEYS:\n",
    "    for resp in all_responses[key]:\n",
    "        for w in word_tokenize(resp.lower()):\n",
    "            if w in kw_set:\n",
    "                all_kw_counter[w] += 1\n",
    "top20 = [kw for kw, _ in all_kw_counter.most_common(20)]\n",
    "\n",
    "if not top20:\n",
    "    print(\"No Peterson keywords found across any model \u2014 skipping heatmap.\")\n",
    "else:\n",
    "    # Compute keyword matrices for all four models\n",
    "    matrices = {k: per_prompt_kw_matrix(all_responses[k], top20) for k in MODEL_KEYS}\n",
    "    vmax = max(m.max() for m in matrices.values() if m.size > 0) or 1\n",
    "\n",
    "    p_labels = [f\"Q{i+1}: {EVAL_PROMPTS[i][:25]}\u2026\" for i in range(len(EVAL_PROMPTS))]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14), sharey=True)\n",
    "    fig.suptitle(\"Peterson Keyword Usage Heatmap \u2014 All 4 Models\",\n",
    "                 fontsize=15, fontweight='bold')\n",
    "\n",
    "    for ax, key in zip(axes.flat, MODEL_KEYS):\n",
    "        mat = matrices[key]\n",
    "        im = ax.imshow(mat, aspect='auto', cmap='YlOrRd', vmin=0, vmax=vmax)\n",
    "        ax.set_xticks(range(len(top20)))\n",
    "        ax.set_xticklabels(top20, rotation=45, ha='right', fontsize=8)\n",
    "        ax.set_yticks(range(len(p_labels)))\n",
    "        ax.set_yticklabels(p_labels, fontsize=8)\n",
    "        ax.set_title(MODEL_DISPLAY[key], fontsize=11, fontweight='bold',\n",
    "                     color=MODEL_COLORS[key])\n",
    "        ax.set_xlabel(\"Peterson Keyword\")\n",
    "        # Annotate non-zero cells with their count value\n",
    "        for i in range(mat.shape[0]):\n",
    "            for j in range(mat.shape[1]):\n",
    "                if mat[i, j] > 0:\n",
    "                    ax.text(j, i, str(mat[i, j]), ha='center', va='center',\n",
    "                            fontsize=7,\n",
    "                            color='white' if mat[i, j] > vmax * 0.6 else 'black')\n",
    "        plt.colorbar(im, ax=ax, label=\"Keyword count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"06_keyword_heatmap.png\", bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved: 06_keyword_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd1a32",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure 7: Radar / Spider Chart \u2014 Overall \"Peterson-Likeness\"\n",
    "\n",
    "The radar chart summarises all five metrics simultaneously for all four models\n",
    "on a single normalised scale [0.1, 1.0].  All metrics are oriented so that\n",
    "**larger = more Peterson-like**:\n",
    "\n",
    "| Spoke | Raw metric | Orientation |\n",
    "|-------|-----------|-------------|\n",
    "| Perplexity (inverted) | Lower PPL \u2192 higher score | inverted |\n",
    "| TF-IDF Similarity | Higher cos-sim \u2192 higher score | normal |\n",
    "| Keyword Density | Higher % \u2192 higher score | normal |\n",
    "| Vocabulary Richness (TTR) | Higher TTR \u2192 higher score | normal |\n",
    "| Response Length | Longer responses \u2192 higher score | normal |\n",
    "\n",
    "Normalisation uses min\u2013max across all four models simultaneously, so the\n",
    "chart shows *relative* differences rather than absolute magnitudes.  A larger\n",
    "polygon area means the model scores more consistently across all five axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f65e7a",
   "metadata": {},
   "outputs": [],
   "source": "def norm4(values: list[float], higher_better: bool = True) -> list[float]:\n    \"\"\"\n    Normalise four metric values to [0.1, 1.0].\n\n    Min-max normalisation is applied across all four values so that the\n    best model always reaches 1.0 and the worst always reaches 0.1 on each\n    axis.  Setting higher_better=False inverts the scale so that metrics\n    where a lower value is better (like perplexity) still point \"outward\"\n    on the radar chart.\n    \"\"\"\n    lo, hi = min(values), max(values)\n    if abs(hi - lo) < 1e-9:\n        return [0.5] * len(values)   # all models equal on this axis\n    normed = [(v - lo) / (hi - lo) for v in values]\n    if not higher_better:\n        normed = [1.0 - v for v in normed]   # invert so outward = better\n    return [0.1 + 0.9 * v for v in normed]\n\n# \u2500\u2500 Build radar axes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Each entry is (axis_label, [score_per_model]) where scores are in MODEL_KEYS order.\nradar_metrics = [\n    (\"Perplexity\\n(inverted)\",\n     norm4([avg_ppl[k]  for k in MODEL_KEYS], higher_better=False)),\n    (\"TF-IDF\\nSimilarity\",\n     norm4([avg_sim[k]  for k in MODEL_KEYS], higher_better=True)),\n    (\"Keyword\\nDensity\",\n     norm4([avg_kd[k]   for k in MODEL_KEYS], higher_better=True)),\n    (\"Vocabulary\\nRichness (TTR)\",\n     norm4([avg_ttr[k]  for k in MODEL_KEYS], higher_better=True)),\n    (\"Response\\nLength\",\n     norm4([avg_len[k]  for k in MODEL_KEYS], higher_better=True)),\n]\n\n# Close the radar polygon by repeating the first point\nspoke_labels = [m[0] for m in radar_metrics] + [radar_metrics[0][0]]\nangles       = np.linspace(0, 2 * np.pi, len(spoke_labels), endpoint=True)\n\nfig, ax = plt.subplots(figsize=(9, 9), subplot_kw=dict(polar=True))\nfig.suptitle(\n    \"Radar Summary: Peterson-Likeness Across All Metrics\\n(all 4 model variants)\",\n    fontsize=14, fontweight='bold', y=1.02)\n\nfor i, key in enumerate(MODEL_KEYS):\n    # Extract this model's score on each axis, then close the polygon\n    values = [m[1][i] for m in radar_metrics] + [radar_metrics[0][1][i]]\n    ax.plot(angles, values, color=MODEL_COLORS[key], lw=2.5,\n            label=MODEL_DISPLAY[key])\n    ax.fill(angles, values, color=MODEL_COLORS[key], alpha=0.08)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(spoke_labels[:-1], size=11)\nax.set_yticks([0.25, 0.5, 0.75, 1.0])\nax.set_yticklabels(['0.25', '0.5', '0.75', '1.0'], size=7, color='grey')\nax.set_ylim(0, 1)\nax.legend(loc='upper right', bbox_to_anchor=(1.45, 1.15), fontsize=10)\n\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / \"07_radar_summary.png\", bbox_inches='tight', dpi=150)\nplt.show()\nprint(\"Saved: 07_radar_summary.png\")"
  },
  {
   "cell_type": "markdown",
   "id": "c3182de6",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Full Metric Summary Table\n",
    "\n",
    "The table below shows raw metric values for all four models plus the\n",
    "intra-architecture fine-tuning improvement (base \u2192 fine-tuned) for each metric.\n",
    "\n",
    "Symbol key:\n",
    "- `\u25b2 \u2713` \u2014 value moved in the *beneficial* direction\n",
    "- `\u25bc \u2717` \u2014 value moved in the *wrong* direction\n",
    "- `\u2014`   \u2014 informational metric (no \"correct\" direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_change(bv: float, tv: float, higher_better: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Format a percentage change from base value (bv) to tuned value (tv).\n",
    "\n",
    "    Returns a string like '+12.3% \u25b2 \u2713' when the change is in the expected\n",
    "    direction, or '-5.1% \u25bc \u2717' when it is not.  An improvement is:\n",
    "    - decrease for lower-is-better metrics (like perplexity)\n",
    "    - increase for higher-is-better metrics (like TF-IDF, keyword density)\n",
    "    \"\"\"\n",
    "    if abs(bv) < 1e-9:\n",
    "        return \"N/A\"\n",
    "    pct    = 100 * (tv - bv) / bv\n",
    "    is_up  = pct > 0\n",
    "    # improved = moved in the desired direction\n",
    "    ok     = (is_up == higher_better)\n",
    "    arrow  = \"\u25b2\" if is_up else \"\u25bc\"\n",
    "    check  = \"\u2713\" if ok else \"\u2717\"\n",
    "    return f\"{pct:+.1f}% {arrow} {check}\"\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"Metric\":              \"Avg Perplexity  (\u2193 better)\",\n",
    "        \"GPT-OSS Base\":        f\"{avg_ppl['gptoss_base']:.2f}\",\n",
    "        \"GPT-OSS Fine-Tuned\":  f\"{avg_ppl['gptoss_tuned']:.2f}\",\n",
    "        \"GPT-OSS \u0394\":           pct_change(avg_ppl['gptoss_base'], avg_ppl['gptoss_tuned'],\n",
    "                                           higher_better=False),\n",
    "        \"Qwen3 Base\":          f\"{avg_ppl['qwen3_base']:.2f}\",\n",
    "        \"Qwen3 Fine-Tuned\":    f\"{avg_ppl['qwen3_tuned']:.2f}\",\n",
    "        \"Qwen3 \u0394\":             pct_change(avg_ppl['qwen3_base'], avg_ppl['qwen3_tuned'],\n",
    "                                           higher_better=False),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\":              \"Avg TF-IDF Similarity  (\u2191 better)\",\n",
    "        \"GPT-OSS Base\":        f\"{avg_sim['gptoss_base']:.4f}\",\n",
    "        \"GPT-OSS Fine-Tuned\":  f\"{avg_sim['gptoss_tuned']:.4f}\",\n",
    "        \"GPT-OSS \u0394\":           pct_change(avg_sim['gptoss_base'], avg_sim['gptoss_tuned']),\n",
    "        \"Qwen3 Base\":          f\"{avg_sim['qwen3_base']:.4f}\",\n",
    "        \"Qwen3 Fine-Tuned\":    f\"{avg_sim['qwen3_tuned']:.4f}\",\n",
    "        \"Qwen3 \u0394\":             pct_change(avg_sim['qwen3_base'], avg_sim['qwen3_tuned']),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\":              \"Avg Keyword Density  (\u2191 better)\",\n",
    "        \"GPT-OSS Base\":        f\"{100*avg_kd['gptoss_base']:.2f}%\",\n",
    "        \"GPT-OSS Fine-Tuned\":  f\"{100*avg_kd['gptoss_tuned']:.2f}%\",\n",
    "        \"GPT-OSS \u0394\":           pct_change(avg_kd['gptoss_base'], avg_kd['gptoss_tuned']),\n",
    "        \"Qwen3 Base\":          f\"{100*avg_kd['qwen3_base']:.2f}%\",\n",
    "        \"Qwen3 Fine-Tuned\":    f\"{100*avg_kd['qwen3_tuned']:.2f}%\",\n",
    "        \"Qwen3 \u0394\":             pct_change(avg_kd['qwen3_base'], avg_kd['qwen3_tuned']),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\":              \"Avg Type-Token Ratio  (\u2191 better)\",\n",
    "        \"GPT-OSS Base\":        f\"{avg_ttr['gptoss_base']:.4f}\",\n",
    "        \"GPT-OSS Fine-Tuned\":  f\"{avg_ttr['gptoss_tuned']:.4f}\",\n",
    "        \"GPT-OSS \u0394\":           pct_change(avg_ttr['gptoss_base'], avg_ttr['gptoss_tuned']),\n",
    "        \"Qwen3 Base\":          f\"{avg_ttr['qwen3_base']:.4f}\",\n",
    "        \"Qwen3 Fine-Tuned\":    f\"{avg_ttr['qwen3_tuned']:.4f}\",\n",
    "        \"Qwen3 \u0394\":             pct_change(avg_ttr['qwen3_base'], avg_ttr['qwen3_tuned']),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\":              \"Avg Response Length  (\u2014  informational)\",\n",
    "        \"GPT-OSS Base\":        f\"{avg_len['gptoss_base']:.1f} words\",\n",
    "        \"GPT-OSS Fine-Tuned\":  f\"{avg_len['gptoss_tuned']:.1f} words\",\n",
    "        \"GPT-OSS \u0394\":           f\"{avg_len['gptoss_tuned']-avg_len['gptoss_base']:+.1f} words\",\n",
    "        \"Qwen3 Base\":          f\"{avg_len['qwen3_base']:.1f} words\",\n",
    "        \"Qwen3 Fine-Tuned\":    f\"{avg_len['qwen3_tuned']:.1f} words\",\n",
    "        \"Qwen3 \u0394\":             f\"{avg_len['qwen3_tuned']-avg_len['qwen3_base']:+.1f} words\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "col_order = [\"Metric\",\n",
    "             \"GPT-OSS Base\", \"GPT-OSS Fine-Tuned\", \"GPT-OSS \u0394\",\n",
    "             \"Qwen3 Base\",   \"Qwen3 Fine-Tuned\",   \"Qwen3 \u0394\"]\n",
    "df = df[col_order]\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(\"FULL METRIC SUMMARY \u2014 Cross-Architecture Comparison  (Jordan Peterson Fine-Tuning)\")\n",
    "print(\"=\" * 110)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 110)\n",
    "print(\"\\n\u2713 = change in expected direction  |  \u2717 = no improvement  |  \u25b2/\u25bc = direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec8f65",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Side-by-Side Response Comparison\n",
    "\n",
    "Quantitative metrics tell only part of the story.  Here we print the raw\n",
    "responses from all four models for the first three prompts so you can directly\n",
    "read and compare their wording, depth, and stylistic alignment with Peterson.\n",
    "\n",
    "Look for:\n",
    "- Does the fine-tuned model add Peterson-specific vocabulary (chaos, order, logos)?\n",
    "- Does any model regurgitate book passages rather than generating structured answers?\n",
    "- Is the response coherent and on-topic, or does it drift?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(3, len(EVAL_PROMPTS))):\n",
    "    print(\"\u2501\" * 100)\n",
    "    print(f\"PROMPT {i+1}: {EVAL_PROMPTS[i]}\")\n",
    "    print(\"\u2501\" * 100)\n",
    "    for key in MODEL_KEYS:\n",
    "        resp = all_responses[key][i]\n",
    "        label = MODEL_DISPLAY[key]\n",
    "        print(f\"\\n[{label}]\")\n",
    "        print(resp if resp.strip() else \"(empty response)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abddf71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saved figures:\")\n",
    "for f in sorted(FIGURES_DIR.iterdir()):\n",
    "    print(f\"  {f.name}  ({f.stat().st_size / 1024:.0f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c8949",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "### What perplexity tells us\n",
    "\n",
    "Perplexity is the most unambiguous signal of domain adaptation.  A model that\n",
    "has truly learned Peterson's writing will assign higher probability to his actual\n",
    "sentences \u2014 producing lower perplexity on held-out passages.  We expect both\n",
    "fine-tuned models to score lower than their respective base models; the\n",
    "*magnitude* of that drop tells us how thoroughly the training signal was absorbed.\n",
    "\n",
    "### Interpreting the vocabulary metrics (TF-IDF, keyword density, TTR)\n",
    "\n",
    "These three metrics can *decrease* after fine-tuning even when training is\n",
    "working correctly.  The reason is the **passage-regurgitation phenomenon**:\n",
    "a model trained for only 1 epoch sometimes learns to output book passages\n",
    "verbatim.  A passage is a sequence of continuous prose, not a curated answer\n",
    "\u2014 it naturally has *lower* keyword concentration and a different vocabulary\n",
    "distribution than a well-structured response.  If this is occurring, you will\n",
    "see it clearly in the side-by-side comparison above (responses starting with\n",
    "fragments like \"the world, and\u2026\" or \"\u2026is the place of\u2026\").\n",
    "\n",
    "### Architecture comparison\n",
    "\n",
    "| | GPT-OSS 20B | Qwen3-14B |\n",
    "|--|-------------|-----------|\n",
    "| Parameters | 20B | 14B |\n",
    "| Training loss (1 epoch) | 3.01 | **2.44** |\n",
    "| Training time | 73.3 min | **23.3 min** |\n",
    "| Batch size | 1 | **2** |\n",
    "| VRAM peak | 13.8 GB | **13.4 GB** |\n",
    "\n",
    "Qwen3-14B achieved a substantially *lower* training loss while being a smaller\n",
    "model that trained three times faster.  This is consistent with Qwen3's\n",
    "architecture improvements and its stronger base capabilities at 14B parameters.\n",
    "Whether the numerical training-loss advantage translates into *perceptually*\n",
    "better responses depends on whether the model has learned generalisation\n",
    "(answering questions in Peterson's style) versus memorisation (reproducing passages).\n",
    "\n",
    "### Next steps to improve inference quality\n",
    "\n",
    "1. **Train for more epochs** \u2014 3 epochs typically shifts the model from passage\n",
    "   reproduction toward genuine stylistic transfer.\n",
    "2. **Enable sampling at inference** \u2014 use `temperature=0.7, top_p=0.8` (Qwen3\n",
    "   recommended defaults) instead of greedy decoding to produce more natural answers.\n",
    "3. **Try Qwen3 thinking mode** \u2014 `enable_thinking=True` at inference engages the\n",
    "   chain-of-thought reasoning module, which may produce substantively different and\n",
    "   potentially deeper responses for complex Peterson-themed questions.\n",
    "4. **Longer LoRA rank** \u2014 the fine-tuning used `r=16`; increasing to `r=32` gives\n",
    "   the adapter more capacity to capture Peterson's stylistic patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}