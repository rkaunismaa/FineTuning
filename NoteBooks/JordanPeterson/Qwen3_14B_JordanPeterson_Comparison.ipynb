{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1b1406",
   "metadata": {},
   "source": [
    "# Comparing Base vs. Fine-Tuned Qwen3-14B: Jordan Peterson Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a quantitative comparison between:\n",
    "- **Base model**: `unsloth/Qwen3-14B-unsloth-bnb-4bit` ‚Äî Qwen3-14B with no domain specialization\n",
    "- **Fine-tuned model**: The same base with LoRA adapters trained on ~768,000 words from four Jordan Peterson books\n",
    "\n",
    "It is the companion to `GPT_OSS_20B_JordanPeterson_Comparison.ipynb` and uses the identical evaluation framework, allowing meaningful cross-model comparisons.\n",
    "\n",
    "### How This Notebook Differs from the GPT-OSS Comparison\n",
    "\n",
    "| Aspect | GPT-OSS 20B | Qwen3-14B |\n",
    "|--------|-------------|-----------|\n",
    "| Chat template | Harmony (`reasoning_effort`) | **ChatML** (`enable_thinking`) |\n",
    "| Inference API | `apply_chat_template(return_dict=True)` | **`apply_chat_template` ‚Üí text ‚Üí tokenize** |\n",
    "| Thinking mode | Via `reasoning_effort=\"low\"` | Via **`enable_thinking=False`** |\n",
    "| Expected base perplexity | Higher (larger model, different training) | Different baseline |\n",
    "| Fine-tune loss | 3.01 | **2.44** (better ‚Äî smaller model, larger batch) |\n",
    "\n",
    "### Metrics (Same as GPT-OSS Comparison)\n",
    "\n",
    "| Metric | What It Measures | Expected Result |\n",
    "|--------|-----------------|----------------|\n",
    "| **Perplexity** | How surprised the model is by Peterson's actual writing | Fine-tuned = lower |\n",
    "| **TF-IDF Similarity** | Vocabulary overlap with real Peterson passages | Fine-tuned = higher |\n",
    "| **Keyword Density** | Frequency of Peterson's characteristic vocabulary | Fine-tuned = higher |\n",
    "| **Type-Token Ratio** | Vocabulary richness | Fine-tuned ‚âà higher |\n",
    "| **Response Length** | Words per response (Peterson is verbose) | Fine-tuned = longer |\n",
    "| **Word Distribution** | Word clouds of dominant vocabulary | Fine-tuned = Peterson-specific terms |\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "Qwen3-14B in 4-bit uses ~10-11 GB of VRAM. We still can't fit two models simultaneously (would require ~22 GB active at once, plus overhead), so we evaluate them sequentially, caching results to disk between phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ca51a",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0f3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc, math, pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt',     quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "BASE_MODEL_NAME = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\n",
    "LORA_MODEL_PATH = \"./outputs/qwen3_14b_jordan_peterson_lora\"\n",
    "CACHE_DIR       = Path(\"./comparison_cache_qwen3\")\n",
    "FIGURES_DIR     = Path(\"./comparison_figures_qwen3\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ‚îÄ‚îÄ Plot style ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 120,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#f8f8f8',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.4,\n",
    "    'font.size': 11,\n",
    "})\n",
    "BASE_COLOR  = '#4C72B0'   # blue  ‚Äî base model\n",
    "TUNED_COLOR = '#DD8452'   # orange ‚Äî fine-tuned model\n",
    "\n",
    "print(f\"PyTorch : {torch.__version__}\")\n",
    "print(f\"CUDA    : {torch.cuda.is_available()}  |  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM    : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Cache   : {CACHE_DIR.resolve()}\")\n",
    "print(f\"Figures : {FIGURES_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009ad9f",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Reference Data\n",
    "\n",
    "The same three reference datasets used in the GPT-OSS comparison ‚Äî held-out Peterson passages, evaluation prompts, and keyword dictionary ‚Äî ensuring the results are directly comparable across model families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "PETERSON_PASSAGES = [\n",
    "    \"The world can be validly construed as a forum for action, or as a place of things. \"\n",
    "    \"The former manner of interpretation ‚Äî more primordial, and less clearly understood ‚Äî \"\n",
    "    \"finds its expression in the arts or humanities, in ritual, drama, literature, and myth. \"\n",
    "    \"The world as forum for action is a place of value, a place where all things have meaning.\",\n",
    "\n",
    "    \"To stand up straight with your shoulders back is to accept the terrible responsibility \"\n",
    "    \"of life, with eyes wide open. It means deciding to voluntarily transform the chaos of \"\n",
    "    \"potential into the realities of habitable order. It means adopting the burden of \"\n",
    "    \"self-conscious vulnerability, and accepting the end of the unconscious paradise of \"\n",
    "    \"childhood, where finitude and mortality are only dimly comprehended.\",\n",
    "\n",
    "    \"Order is the place where the things you are currently doing are working out well \"\n",
    "    \"for you. Chaos is the domain of ignorance itself. It's unexplored territory. Chaos \"\n",
    "    \"is what extends, endlessly and without limit, beyond the boundaries of all states, \"\n",
    "    \"all ideas, and all disciplines. It's the foreigner, the stranger, the member of \"\n",
    "    \"another gang, the rustle in the bushes in the night-time.\",\n",
    "\n",
    "    \"The divine spark in man is the logos ‚Äî the word, the reason, the creative principle \"\n",
    "    \"that gives order to the chaos of experience. To act in accordance with the logos is \"\n",
    "    \"to speak the truth, to pursue what is meaningful rather than what is expedient, and \"\n",
    "    \"to take on the burden of Being itself with courage and humility.\",\n",
    "\n",
    "    \"Compare yourself to who you were yesterday, not to who someone else is today. \"\n",
    "    \"You have a nature. You can play the game of life and improve. You can set a \"\n",
    "    \"standard, even a minimal standard, and try to live it up to. You can improve \"\n",
    "    \"incrementally, moving forward step by step. You can judge your life against \"\n",
    "    \"what you know to be good, against what you should be.\",\n",
    "\n",
    "    \"The great myths and rituals of the past have been formulated in the language of \"\n",
    "    \"the imagination. They say: act out the role of the hero; do not be the villain; \"\n",
    "    \"do not be the tyrant. They say: update your maps of meaning when new information \"\n",
    "    \"warrants it; admit your errors and change. They say: encounter the stranger and \"\n",
    "    \"extract from that encounter what is valuable. Treat the stranger with respect.\",\n",
    "\n",
    "    \"Meaning is the ultimate balance between, on the one hand, the chaos of transformation \"\n",
    "    \"and possibility and, on the other, the discipline of pristine order, whose purpose is \"\n",
    "    \"to produce out of the attendant chaos a new order that will be even more productive \"\n",
    "    \"and worthwhile than the old. Pursue what is meaningful, not what is expedient.\",\n",
    "\n",
    "    \"Suffering is not a mistake or an accident. It is the very ground of Being itself. \"\n",
    "    \"To wrestle with God, as Jacob did, is to confront that suffering honestly, to take \"\n",
    "    \"responsibility for it, and to find within it the possibility of transcendence. The \"\n",
    "    \"hero does not flee from the dragon; he faces it and transforms the encounter.\",\n",
    "]\n",
    "\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the relationship between order and chaos in human experience?\",\n",
    "    \"Why is personal responsibility the foundation of a meaningful life?\",\n",
    "    \"How do ancient myths and stories reveal truths about human nature?\",\n",
    "    \"What does it mean to pursue what is meaningful rather than what is expedient?\",\n",
    "    \"How should a person confront suffering rather than flee from it?\",\n",
    "    \"What is the significance of the hero archetype in understanding the human psyche?\",\n",
    "    \"Why is telling the truth essential to a properly functioning life?\",\n",
    "    \"What is the role of the divine or the sacred in organizing human society?\",\n",
    "    \"How does the Jungian concept of the shadow relate to individual development?\",\n",
    "    \"What does it mean to stand up straight with your shoulders back?\",\n",
    "]\n",
    "\n",
    "PETERSON_KEYWORDS = list(set([\n",
    "    \"chaos\", \"order\", \"logos\", \"being\", \"meaning\", \"meaningful\", \"meaningless\",\n",
    "    \"transcendence\", \"transcendent\", \"archetype\", \"archetypal\",\n",
    "    \"shadow\", \"anima\", \"animus\", \"unconscious\", \"consciousness\", \"psyche\",\n",
    "    \"individuation\", \"projection\",\n",
    "    \"responsibility\", \"suffering\", \"redemption\", \"courage\", \"virtue\",\n",
    "    \"nihilism\", \"nihilistic\", \"expedient\", \"expedience\", \"tyranny\", \"tyrannical\",\n",
    "    \"sovereignty\", \"heroic\", \"malevolent\",\n",
    "    \"myth\", \"mythological\", \"hero\", \"dragon\", \"narrative\", \"story\",\n",
    "    \"ritual\", \"sacrifice\", \"resurrection\", \"transformation\",\n",
    "    \"divine\", \"sacred\", \"god\", \"biblical\", \"genesis\", \"logos\", \"spirit\",\n",
    "    \"wrestle\", \"jacob\", \"adam\", \"eve\", \"serpent\",\n",
    "    \"confront\", \"hierarchy\", \"dominance\", \"voluntarily\", \"catastrophe\",\n",
    "    \"pathological\", \"resentment\", \"ideological\", \"totalitarian\",\n",
    "]))\n",
    "\n",
    "print(f\"Reference passages  : {len(PETERSON_PASSAGES)}\")\n",
    "print(f\"Evaluation prompts  : {len(EVAL_PROMPTS)}\")\n",
    "print(f\"Keyword dictionary  : {len(PETERSON_KEYWORDS)} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ab563",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Helper Functions\n",
    "\n",
    "The metric functions are identical to the GPT-OSS comparison notebook. The key difference is in `generate_response()`, which must use Qwen3's ChatML API:\n",
    "\n",
    "```python\n",
    "# GPT-OSS approach (reasoning_effort parameter, return_dict=True):\n",
    "inputs = tokenizer.apply_chat_template(messages, return_dict=True,\n",
    "             reasoning_effort=\"low\", ...).to(\"cuda\")\n",
    "\n",
    "# Qwen3 approach (enable_thinking parameter, two-step):\n",
    "text   = tokenizer.apply_chat_template(messages, enable_thinking=False, tokenize=False, ...)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "```\n",
    "\n",
    "Qwen3's `apply_chat_template` returns a plain string (not a dict), so we tokenize in a second step. We also strip any `<think>...</think>` blocks from the output to ensure the measured response text contains only the visible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40be42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, tokenizer, texts: list, max_length: int = 512) -> list:\n",
    "    \"\"\"\n",
    "    Compute perplexity of a language model on each text.\n",
    "\n",
    "    Perplexity = exp(average negative log-likelihood per token).\n",
    "    Lower perplexity = model is less surprised by the text = more domain-adapted.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    perplexities = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            enc = tokenizer(text, return_tensors=\"pt\",\n",
    "                            max_length=max_length, truncation=True).to(\"cuda\")\n",
    "            out = model(**enc, labels=enc[\"input_ids\"])\n",
    "            perplexities.append(math.exp(out.loss.item()))\n",
    "    return perplexities\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, prompt: str,\n",
    "                      system_prompt: str, max_new_tokens: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from a Qwen3 model for a given prompt.\n",
    "\n",
    "    Key difference from GPT-OSS: Qwen3's apply_chat_template returns a plain string,\n",
    "    not a tensor dict. We tokenize in a second step. We also pass enable_thinking=False\n",
    "    to suppress chain-of-thought and get direct Peterson-style responses.\n",
    "\n",
    "    Any residual <think>...</think> blocks are stripped from the output so that only\n",
    "    the visible response is measured.\n",
    "\n",
    "    Uses greedy decoding (do_sample=False) for deterministic, fair comparison.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "    ]\n",
    "    # Step 1: Apply chat template ‚Üí plain text string\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize             = False,\n",
    "        add_generation_prompt= True,\n",
    "        enable_thinking      = False,   # Non-thinking mode: direct response\n",
    "    )\n",
    "    # Step 2: Tokenize the formatted string\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens    = max_new_tokens,\n",
    "            do_sample         = False,   # Greedy ‚Äî fully deterministic\n",
    "            temperature       = 1.0,\n",
    "            repetition_penalty= 1.1,\n",
    "        )\n",
    "\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response   = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Strip any <think>...</think> blocks (may appear even in non-thinking mode)\n",
    "    response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "def compute_text_stats(texts: list) -> dict:\n",
    "    \"\"\"\n",
    "    Compute text statistics over a list of responses.\n",
    "\n",
    "    Every text always contributes exactly one entry to each output list\n",
    "    (empty texts get 0 counts) so that per-prompt plots never have length mismatches.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    kw_set     = set(k.lower() for k in PETERSON_KEYWORDS)\n",
    "\n",
    "    word_counts, sentence_counts, ttr_values, keyword_density = [], [], [], []\n",
    "    keyword_counts = Counter()\n",
    "    all_words = []\n",
    "\n",
    "    for text in texts:\n",
    "        if text.strip():\n",
    "            words = word_tokenize(text.lower())\n",
    "            sents = sent_tokenize(text)\n",
    "        else:\n",
    "            words, sents = [], []\n",
    "\n",
    "        words_alpha = [w for w in words if w.isalpha()]\n",
    "        word_counts.append(len(words_alpha))\n",
    "        sentence_counts.append(len(sents))\n",
    "        ttr_values.append(len(set(words_alpha)) / max(len(words_alpha), 1))\n",
    "\n",
    "        kw_hits = [w for w in words_alpha if w in kw_set]\n",
    "        keyword_density.append(len(kw_hits) / max(len(words_alpha), 1))\n",
    "        keyword_counts.update(kw_hits)\n",
    "\n",
    "        content_words = [w for w in words_alpha if w not in stop_words and len(w) > 2]\n",
    "        all_words.extend(content_words)\n",
    "\n",
    "    return {\n",
    "        \"word_counts\":     word_counts,\n",
    "        \"sentence_counts\": sentence_counts,\n",
    "        \"ttr_values\":      ttr_values,\n",
    "        \"keyword_density\": keyword_density,\n",
    "        \"keyword_counts\":  keyword_counts,\n",
    "        \"all_words\":       all_words,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_tfidf_similarity(responses: list, references: list) -> list:\n",
    "    \"\"\"\n",
    "    TF-IDF cosine similarity: how similar each response is to Peterson's actual writing.\n",
    "    Returns one similarity score per response (0.0 for empty responses).\n",
    "    \"\"\"\n",
    "    if not responses or not any(r.strip() for r in responses):\n",
    "        return [0.0] * len(responses)\n",
    "    all_texts  = references + responses\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    tfidf      = vectorizer.fit_transform(all_texts)\n",
    "    ref_vecs   = tfidf[:len(references)]\n",
    "    resp_vecs  = tfidf[len(references):]\n",
    "    return [float(cosine_similarity(resp_vecs[i], ref_vecs)[0].max())\n",
    "            for i in range(resp_vecs.shape[0])]\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84746090",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Evaluate the Base Model\n",
    "\n",
    "We load `unsloth/Qwen3-14B-unsloth-bnb-4bit`, compute perplexity on Peterson's passages, generate responses to all 10 evaluation prompts, save results to `comparison_cache_qwen3/`, then fully unload the model before loading the fine-tuned version.\n",
    "\n",
    "**System prompt for base model**: A generic helpful-assistant prompt ‚Äî the same framing a user would naturally provide to an untrained model.\n",
    "\n",
    "**System prompt for fine-tuned model**: The Peterson-expert persona the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e16b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "BASE_SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "\n",
    "TUNED_SYSTEM_PROMPT = (\n",
    "    \"You are an AI assistant that has been trained on the complete works of Jordan B. Peterson, \"\n",
    "    \"a Canadian clinical psychologist, professor, and author. You speak with deep knowledge of \"\n",
    "    \"psychology, philosophy, mythology, religion, and personal responsibility. Your responses \"\n",
    "    \"reflect Peterson's writing style, intellectual depth, and interdisciplinary approach to \"\n",
    "    \"understanding human nature and meaning.\"\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ Cache check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "_base_cache_exists = (CACHE_DIR / \"base_results.pkl\").exists()\n",
    "\n",
    "if _base_cache_exists:\n",
    "    print(\"Base model cache found ‚Äî skipping inference.\")\n",
    "    print(f\"  (Delete {CACHE_DIR / 'base_results.pkl'} to force re-run)\")\n",
    "else:\n",
    "    print(\"Loading BASE model (Qwen3-14B 4-bit)‚Ä¶\")\n",
    "    base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name    = BASE_MODEL_NAME,\n",
    "        dtype         = None,\n",
    "        max_seq_length= 2048,\n",
    "        load_in_4bit  = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(base_model)\n",
    "    print(f\"Base model loaded. VRAM: {torch.cuda.memory_reserved()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0de142",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _base_cache_exists:\n",
    "    print(\"Computing base model perplexity on Peterson passages‚Ä¶\")\n",
    "    base_perplexities = compute_perplexity(base_model, base_tokenizer, PETERSON_PASSAGES)\n",
    "    for i, (txt, ppl) in enumerate(zip(PETERSON_PASSAGES, base_perplexities)):\n",
    "        print(f\"  Passage {i+1}: PPL = {ppl:.2f}  |  '{txt[:55]}‚Ä¶'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65fba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _base_cache_exists:\n",
    "    print(\"Generating base model responses‚Ä¶\\n\")\n",
    "    base_responses = []\n",
    "    for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{i+1}/{len(EVAL_PROMPTS)}] {prompt[:70]}\")\n",
    "        resp = generate_response(base_model, base_tokenizer, prompt, BASE_SYSTEM_PROMPT)\n",
    "        base_responses.append(resp)\n",
    "        print(f\"         ‚Üí {resp[:100]}‚Ä¶\\n\")\n",
    "    print(f\"Done. {len(base_responses)} responses collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _base_cache_exists:\n",
    "    base_results = {\"perplexities\": base_perplexities, \"responses\": base_responses}\n",
    "    with open(CACHE_DIR / \"base_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(base_results, f)\n",
    "    print(f\"Saved. Avg PPL: {sum(base_perplexities)/len(base_perplexities):.2f}  \"\n",
    "          f\"| Responses: {len(base_responses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _base_cache_exists:\n",
    "    del base_model, base_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    free = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1e9\n",
    "    print(f\"Base model unloaded. VRAM free: {free:.1f} GB\")\n",
    "else:\n",
    "    print(\"Base model not loaded (used cache).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54106822",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Evaluate the Fine-Tuned Model\n",
    "\n",
    "We load the LoRA-adapted Qwen3-14B from the `outputs/` directory. Unsloth loads the base weights and merges the LoRA adapters transparently ‚Äî the resulting model behaves like a single fine-tuned model from the caller's perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443cddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tuned_cache_exists = (CACHE_DIR / \"tuned_results.pkl\").exists()\n",
    "\n",
    "if _tuned_cache_exists:\n",
    "    print(\"Fine-tuned model cache found ‚Äî skipping inference.\")\n",
    "    print(f\"  (Delete {CACHE_DIR / 'tuned_results.pkl'} to force re-run)\")\n",
    "else:\n",
    "    print(\"Loading FINE-TUNED model (base + LoRA adapters)‚Ä¶\")\n",
    "    tuned_model, tuned_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name    = LORA_MODEL_PATH,\n",
    "        dtype         = None,\n",
    "        max_seq_length= 2048,\n",
    "        load_in_4bit  = True,\n",
    "        full_finetuning = False,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(tuned_model)\n",
    "    print(f\"Fine-tuned model loaded. VRAM: {torch.cuda.memory_reserved()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c822e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _tuned_cache_exists:\n",
    "    print(\"Computing fine-tuned model perplexity‚Ä¶\")\n",
    "    tuned_perplexities = compute_perplexity(tuned_model, tuned_tokenizer, PETERSON_PASSAGES)\n",
    "    for i, (txt, ppl) in enumerate(zip(PETERSON_PASSAGES, tuned_perplexities)):\n",
    "        print(f\"  Passage {i+1}: PPL = {ppl:.2f}  |  '{txt[:55]}‚Ä¶'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _tuned_cache_exists:\n",
    "    print(\"Generating fine-tuned model responses‚Ä¶\\n\")\n",
    "    tuned_responses = []\n",
    "    for i, prompt in enumerate(EVAL_PROMPTS):\n",
    "        print(f\"  [{i+1}/{len(EVAL_PROMPTS)}] {prompt[:70]}\")\n",
    "        resp = generate_response(tuned_model, tuned_tokenizer, prompt, TUNED_SYSTEM_PROMPT)\n",
    "        tuned_responses.append(resp)\n",
    "        print(f\"         ‚Üí {resp[:100]}‚Ä¶\\n\")\n",
    "    print(f\"Done. {len(tuned_responses)} responses collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bec718",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _tuned_cache_exists:\n",
    "    tuned_results = {\"perplexities\": tuned_perplexities, \"responses\": tuned_responses}\n",
    "    with open(CACHE_DIR / \"tuned_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tuned_results, f)\n",
    "    print(f\"Saved. Avg PPL: {sum(tuned_perplexities)/len(tuned_perplexities):.2f}  \"\n",
    "          f\"| Responses: {len(tuned_responses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1321b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _tuned_cache_exists:\n",
    "    del tuned_model, tuned_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Fine-tuned model unloaded. Beginning analysis‚Ä¶\")\n",
    "else:\n",
    "    print(\"Fine-tuned model not loaded (used cache). Beginning analysis‚Ä¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693c1ac",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Compute All Derived Metrics\n",
    "\n",
    "We load both result sets from cache and compute all derived metrics in one place. Re-running this and subsequent cells is fast (no GPU required) ‚Äî only Steps 4 and 5 use the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f311df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CACHE_DIR / \"base_results.pkl\",  \"rb\") as f: base_results  = pickle.load(f)\n",
    "with open(CACHE_DIR / \"tuned_results.pkl\", \"rb\") as f: tuned_results = pickle.load(f)\n",
    "\n",
    "base_perplexities  = base_results[\"perplexities\"]\n",
    "tuned_perplexities = tuned_results[\"perplexities\"]\n",
    "base_responses     = base_results[\"responses\"]\n",
    "tuned_responses    = tuned_results[\"responses\"]\n",
    "\n",
    "print(f\"Base  responses : {len(base_responses)}  |  non-empty: {sum(1 for r in base_responses  if r.strip())}\")\n",
    "print(f\"Tuned responses : {len(tuned_responses)} |  non-empty: {sum(1 for r in tuned_responses if r.strip())}\")\n",
    "print()\n",
    "\n",
    "print(\"Computing text statistics‚Ä¶\")\n",
    "base_stats  = compute_text_stats(base_responses)\n",
    "tuned_stats = compute_text_stats(tuned_responses)\n",
    "\n",
    "print(\"Computing TF-IDF similarity‚Ä¶\")\n",
    "base_similarities  = compute_tfidf_similarity(base_responses,  PETERSON_PASSAGES)\n",
    "tuned_similarities = compute_tfidf_similarity(tuned_responses, PETERSON_PASSAGES)\n",
    "\n",
    "# ‚îÄ‚îÄ Summary table ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def _avg(lst): return sum(lst) / max(len(lst), 1)\n",
    "\n",
    "rows = {\n",
    "    \"Metric\": [\n",
    "        \"Avg Perplexity (‚Üì better)\",\n",
    "        \"Avg TF-IDF Similarity (‚Üë better)\",\n",
    "        \"Avg Keyword Density % (‚Üë better)\",\n",
    "        \"Avg Type-Token Ratio (‚Üë better)\",\n",
    "        \"Avg Response Length (words)\",\n",
    "    ],\n",
    "    \"Base Model\": [\n",
    "        f\"{_avg(base_perplexities):.2f}\",\n",
    "        f\"{_avg(base_similarities):.4f}\",\n",
    "        f\"{100*_avg(base_stats['keyword_density']):.2f}%\",\n",
    "        f\"{_avg(base_stats['ttr_values']):.4f}\",\n",
    "        f\"{_avg(base_stats['word_counts']):.1f}\",\n",
    "    ],\n",
    "    \"Fine-Tuned Model\": [\n",
    "        f\"{_avg(tuned_perplexities):.2f}\",\n",
    "        f\"{_avg(tuned_similarities):.4f}\",\n",
    "        f\"{100*_avg(tuned_stats['keyword_density']):.2f}%\",\n",
    "        f\"{_avg(tuned_stats['ttr_values']):.4f}\",\n",
    "        f\"{_avg(tuned_stats['word_counts']):.1f}\",\n",
    "    ],\n",
    "}\n",
    "print(\"\\n\" + pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bc7f8",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Visualization ‚Äî Perplexity\n",
    "\n",
    "**Perplexity** measures how \"surprised\" the model is by Peterson's actual sentences. A fine-tuned model that has learned his vocabulary and sentence patterns assigns higher probability to his words, yielding lower perplexity. This is the most direct measure of domain adaptation.\n",
    "\n",
    "$$\\text{PPL}(\\text{text}) = \\exp\\!\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(w_i | w_1,\\ldots,w_{i-1})\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(\"Perplexity on Jordan Peterson Reference Passages (Qwen3-14B)\",\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "x = np.arange(len(PETERSON_PASSAGES))\n",
    "w = 0.35\n",
    "labels = [f\"P{i+1}\" for i in range(len(PETERSON_PASSAGES))]\n",
    "\n",
    "ax = axes[0]\n",
    "bb = ax.bar(x - w/2, base_perplexities,  w, label=\"Base\",        color=BASE_COLOR,  alpha=0.85)\n",
    "bt = ax.bar(x + w/2, tuned_perplexities, w, label=\"Fine-Tuned\",  color=TUNED_COLOR, alpha=0.85)\n",
    "for bar in bb: ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+.3,\n",
    "                       f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=7, color=BASE_COLOR)\n",
    "for bar in bt: ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+.3,\n",
    "                       f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=7, color=TUNED_COLOR)\n",
    "ax.set_xticks(x); ax.set_xticklabels(labels)\n",
    "ax.set_xlabel(\"Peterson Passage\"); ax.set_ylabel(\"Perplexity  (lower = better)\")\n",
    "ax.set_title(\"Per-Passage Perplexity\"); ax.legend()\n",
    "\n",
    "ax2 = axes[1]\n",
    "avg_b = _avg(base_perplexities); avg_t = _avg(tuned_perplexities)\n",
    "bars  = ax2.bar([\"Base Model\", \"Fine-Tuned\"], [avg_b, avg_t],\n",
    "                color=[BASE_COLOR, TUNED_COLOR], alpha=0.85, width=0.45)\n",
    "for bar, val in zip(bars, [avg_b, avg_t]):\n",
    "    ax2.text(bar.get_x()+bar.get_width()/2, bar.get_height()+.1,\n",
    "             f\"{val:.2f}\", ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "pct = 100 * (avg_b - avg_t) / avg_b\n",
    "ax2.annotate(f\"{pct:+.1f}%\\nimprovement\",\n",
    "             xy=(1, avg_t), xytext=(0.5, (avg_b+avg_t)/2), fontsize=10, ha='center',\n",
    "             color='green' if pct > 0 else 'red', fontweight='bold',\n",
    "             arrowprops=dict(arrowstyle='->', color='green' if pct > 0 else 'red', lw=1.5))\n",
    "ax2.set_ylabel(\"Average Perplexity  (lower = better)\")\n",
    "ax2.set_title(\"Average Perplexity Across All Passages\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"01_perplexity.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(f\"Base: {avg_b:.2f}  |  Fine-tuned: {avg_t:.2f}  |  Change: {pct:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad1c2e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Visualization ‚Äî TF-IDF Semantic Similarity\n",
    "\n",
    "**TF-IDF cosine similarity** measures how similar each model's response vocabulary is to Peterson's actual writing. Rare, distinctive words (chaos, logos, archetype, sovereignty) get higher TF-IDF weight than common words, so this metric rewards using Peterson's characteristic vocabulary rather than generic language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95484112",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(\"TF-IDF Semantic Similarity to Peterson's Actual Writing (Qwen3-14B)\",\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "prompt_labels = [f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))]\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(prompt_labels, base_similarities,  'o-', color=BASE_COLOR,  label=\"Base\",       lw=2, ms=7)\n",
    "ax.plot(prompt_labels, tuned_similarities, 's-', color=TUNED_COLOR, label=\"Fine-Tuned\", lw=2, ms=7)\n",
    "ax.fill_between(range(len(prompt_labels)), base_similarities,  alpha=0.15, color=BASE_COLOR)\n",
    "ax.fill_between(range(len(prompt_labels)), tuned_similarities, alpha=0.15, color=TUNED_COLOR)\n",
    "ax.set_xticks(range(len(prompt_labels))); ax.set_xticklabels(prompt_labels)\n",
    "ax.set_xlabel(\"Evaluation Prompt\"); ax.set_ylabel(\"Cosine Similarity  (higher = better)\")\n",
    "ax.set_title(\"Per-Prompt Similarity\"); ax.legend(); ax.set_ylim(0, 1)\n",
    "for i, (b, t) in enumerate(zip(base_similarities, tuned_similarities)):\n",
    "    ax.annotate(f\"{b:.2f}\", (i, b), textcoords=\"offset points\", xytext=(0, 8),\n",
    "                fontsize=7, ha='center', color=BASE_COLOR)\n",
    "    ax.annotate(f\"{t:.2f}\", (i, t), textcoords=\"offset points\", xytext=(0, -14),\n",
    "                fontsize=7, ha='center', color=TUNED_COLOR)\n",
    "\n",
    "ax2 = axes[1]\n",
    "bp = ax2.boxplot([base_similarities, tuned_similarities], labels=[\"Base\", \"Fine-Tuned\"],\n",
    "                 patch_artist=True, medianprops=dict(color='black', lw=2))\n",
    "bp['boxes'][0].set_facecolor(BASE_COLOR);  bp['boxes'][0].set_alpha(0.7)\n",
    "bp['boxes'][1].set_facecolor(TUNED_COLOR); bp['boxes'][1].set_alpha(0.7)\n",
    "ax2.set_ylabel(\"Cosine Similarity\"); ax2.set_title(\"Similarity Distribution\"); ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"02_tfidf_similarity.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "avg_b2 = _avg(base_similarities); avg_t2 = _avg(tuned_similarities)\n",
    "print(f\"Base: {avg_b2:.4f}  |  Fine-tuned: {avg_t2:.4f}  |  Change: {100*(avg_t2-avg_b2)/max(avg_b2,1e-6):+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037518c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Visualization ‚Äî Peterson Keyword Density\n",
    "\n",
    "**Keyword density** measures what fraction of words in each response belong to Peterson's characteristic vocabulary (~60 terms: chaos, order, meaning, hero, archetype, responsibility, logos, suffering‚Ä¶). This directly tests whether the model has adopted his conceptual vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edde620",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle(\"Peterson Keyword Density (Qwen3-14B)\", fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "base_kd  = [v * 100 for v in base_stats['keyword_density']]\n",
    "tuned_kd = [v * 100 for v in tuned_stats['keyword_density']]\n",
    "ql = [f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))]\n",
    "x  = np.arange(len(ql)); w = 0.35\n",
    "\n",
    "ax = axes[0]\n",
    "ax.bar(x - w/2, base_kd,  w, label=\"Base\",       color=BASE_COLOR,  alpha=0.85)\n",
    "ax.bar(x + w/2, tuned_kd, w, label=\"Fine-Tuned\", color=TUNED_COLOR, alpha=0.85)\n",
    "ax.set_xticks(x); ax.set_xticklabels(ql)\n",
    "ax.set_xlabel(\"Evaluation Prompt\"); ax.set_ylabel(\"Keyword Density (%)\")\n",
    "ax.set_title(\"Fraction of Response Words in Peterson's Vocabulary\"); ax.legend()\n",
    "\n",
    "ax2 = axes[1]\n",
    "all_kw  = set(base_stats['keyword_counts']) | set(tuned_stats['keyword_counts'])\n",
    "top_kw  = sorted(all_kw,\n",
    "                 key=lambda k: base_stats['keyword_counts'].get(k, 0) +\n",
    "                               tuned_stats['keyword_counts'].get(k, 0),\n",
    "                 reverse=True)[:15]\n",
    "bkc = [base_stats['keyword_counts'].get(k, 0)  for k in top_kw]\n",
    "tkc = [tuned_stats['keyword_counts'].get(k, 0) for k in top_kw]\n",
    "y   = np.arange(len(top_kw))\n",
    "ax2.barh(y + 0.2, bkc, 0.4, label=\"Base\",       color=BASE_COLOR,  alpha=0.85)\n",
    "ax2.barh(y - 0.2, tkc, 0.4, label=\"Fine-Tuned\", color=TUNED_COLOR, alpha=0.85)\n",
    "ax2.set_yticks(y); ax2.set_yticklabels(top_kw)\n",
    "ax2.set_xlabel(\"Uses Across All Responses\"); ax2.set_title(\"Top Peterson Keywords Used\")\n",
    "ax2.legend(); ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"03_keyword_density.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(f\"Avg keyword density ‚Äî Base: {_avg(base_kd):.2f}%  |  Fine-tuned: {_avg(tuned_kd):.2f}%  \"\n",
    "      f\"|  Change: {_avg(tuned_kd)-_avg(base_kd):+.2f}pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a0ab6",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Visualization ‚Äî Response Characteristics\n",
    "\n",
    "**Type-Token Ratio (TTR)** measures vocabulary richness: what fraction of the words in a response are unique. Peterson's writing is known for its elaborate, varied vocabulary. **Response length** reflects verbosity ‚Äî Peterson's explanations tend to be extensive and multi-layered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Response Characteristics: Vocabulary Richness & Length (Qwen3-14B)\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "ql2 = [f\"Q{i+1}\" for i in range(len(EVAL_PROMPTS))]\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.plot(ql2, base_stats['ttr_values'],  'o-', color=BASE_COLOR,  label=\"Base\",       lw=2, ms=7)\n",
    "ax.plot(ql2, tuned_stats['ttr_values'], 's-', color=TUNED_COLOR, label=\"Fine-Tuned\", lw=2, ms=7)\n",
    "ax.set_ylabel(\"Type-Token Ratio\"); ax.set_title(\"Vocabulary Richness (TTR) per Prompt\")\n",
    "ax.set_ylim(0, 1); ax.legend(); ax.set_xlabel(\"Prompt\")\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "bp = ax2.boxplot([base_stats['ttr_values'], tuned_stats['ttr_values']],\n",
    "                 labels=[\"Base\", \"Fine-Tuned\"], patch_artist=True,\n",
    "                 medianprops=dict(color='black', lw=2))\n",
    "bp['boxes'][0].set_facecolor(BASE_COLOR);  bp['boxes'][0].set_alpha(0.7)\n",
    "bp['boxes'][1].set_facecolor(TUNED_COLOR); bp['boxes'][1].set_alpha(0.7)\n",
    "ax2.set_ylabel(\"Type-Token Ratio\"); ax2.set_title(\"TTR Distribution\"); ax2.set_ylim(0, 1)\n",
    "\n",
    "x2 = np.arange(len(ql2)); w2 = 0.35\n",
    "ax3 = axes[1, 0]\n",
    "ax3.bar(x2 - w2/2, base_stats['word_counts'],  w2, label=\"Base\",       color=BASE_COLOR,  alpha=0.85)\n",
    "ax3.bar(x2 + w2/2, tuned_stats['word_counts'], w2, label=\"Fine-Tuned\", color=TUNED_COLOR, alpha=0.85)\n",
    "ax3.set_xticks(x2); ax3.set_xticklabels(ql2)\n",
    "ax3.set_xlabel(\"Prompt\"); ax3.set_ylabel(\"Words in Response\")\n",
    "ax3.set_title(\"Response Length (Word Count) per Prompt\"); ax3.legend()\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "bp2 = ax4.boxplot([base_stats['word_counts'], tuned_stats['word_counts']],\n",
    "                  labels=[\"Base\", \"Fine-Tuned\"], patch_artist=True,\n",
    "                  medianprops=dict(color='black', lw=2))\n",
    "bp2['boxes'][0].set_facecolor(BASE_COLOR);  bp2['boxes'][0].set_alpha(0.7)\n",
    "bp2['boxes'][1].set_facecolor(TUNED_COLOR); bp2['boxes'][1].set_alpha(0.7)\n",
    "ax4.set_ylabel(\"Words in Response\"); ax4.set_title(\"Response Length Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"04_response_characteristics.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Avg TTR   ‚Äî Base: {_avg(base_stats['ttr_values']):.4f}  \"\n",
    "      f\"|  Fine-tuned: {_avg(tuned_stats['ttr_values']):.4f}\")\n",
    "print(f\"Avg words ‚Äî Base: {_avg(base_stats['word_counts']):.1f}  \"\n",
    "      f\"|  Fine-tuned: {_avg(tuned_stats['word_counts']):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab48a18",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Visualization ‚Äî Word Clouds\n",
    "\n",
    "Word clouds give an immediate visual of each model's dominant vocabulary (stop words removed). The base model should show generic language; the fine-tuned model should prominently feature Peterson's vocabulary: *meaning, chaos, order, responsibility, suffering, hero, myth, logos*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3acf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_wc = set(STOPWORDS) | {'also', 'one', 'may', 'much', 'even', 'way', 'well',\n",
    "                             'get', 'make', 'like', 'us', 'would', 'could', 'time',\n",
    "                             'thing', 'things', 'many', 'something', 'often'}\n",
    "\n",
    "def make_wc(words, title, ax):\n",
    "    freq = Counter(w for w in words if w.lower() not in stop_wc and len(w) > 2)\n",
    "    if not freq:\n",
    "        ax.text(0.5, 0.5, \"(no content words)\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(title); return\n",
    "    wc = WordCloud(width=800, height=450, background_color='white',\n",
    "                   colormap='Blues' if 'Base' in title else 'Oranges',\n",
    "                   max_words=80, prefer_horizontal=0.8, stopwords=stop_wc,\n",
    "                   min_font_size=8).generate_from_frequencies(freq)\n",
    "    ax.imshow(wc, interpolation='bilinear'); ax.axis('off')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold', pad=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Word Clouds: Most Frequent Content Words in Responses (Qwen3-14B)\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "make_wc(base_stats['all_words'],  \"Base Model\",        axes[0])\n",
    "make_wc(tuned_stats['all_words'], \"Fine-Tuned Model\",  axes[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"05_wordclouds.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d3308",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Visualization ‚Äî Keyword Heatmap\n",
    "\n",
    "This heatmap shows, for each evaluation prompt (row), how many Peterson keywords appeared in that model's response (column). Warmer colors indicate more keyword usage ‚Äî the fine-tuned model's heatmap should be noticeably warmer if domain adaptation has taken hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_set = set(k.lower() for k in PETERSON_KEYWORDS)\n",
    "\n",
    "def per_prompt_kw_matrix(responses, keywords):\n",
    "    mat = []\n",
    "    for resp in responses:\n",
    "        words = word_tokenize(resp.lower()) if resp.strip() else []\n",
    "        words_alpha = [w for w in words if w.isalpha()]\n",
    "        mat.append([words_alpha.count(kw) for kw in keywords])\n",
    "    return np.array(mat)\n",
    "\n",
    "all_kw_c = Counter()\n",
    "for r in base_responses + tuned_responses:\n",
    "    for w in word_tokenize(r.lower()):\n",
    "        if w in kw_set:\n",
    "            all_kw_c[w] += 1\n",
    "\n",
    "top20 = [kw for kw, _ in all_kw_c.most_common(20)]\n",
    "\n",
    "if top20:\n",
    "    bm = per_prompt_kw_matrix(base_responses,  top20)\n",
    "    tm = per_prompt_kw_matrix(tuned_responses, top20)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "    fig.suptitle(\"Peterson Keyword Usage per Prompt ‚Äî Heatmap (Qwen3-14B)\",\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    p_labels = [f\"Q{i+1}: {EVAL_PROMPTS[i][:28]}‚Ä¶\" for i in range(len(EVAL_PROMPTS))]\n",
    "    vmax = max(bm.max(), tm.max(), 1)\n",
    "\n",
    "    for ax, mat, title in [(axes[0], bm, \"Base Model\"), (axes[1], tm, \"Fine-Tuned Model\")]:\n",
    "        im = ax.imshow(mat, aspect='auto', cmap='YlOrRd', vmin=0, vmax=vmax)\n",
    "        ax.set_xticks(range(len(top20))); ax.set_xticklabels(top20, rotation=45, ha='right', fontsize=9)\n",
    "        ax.set_yticks(range(len(p_labels))); ax.set_yticklabels(p_labels, fontsize=8)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold'); ax.set_xlabel(\"Peterson Keyword\")\n",
    "        for i in range(mat.shape[0]):\n",
    "            for j in range(mat.shape[1]):\n",
    "                if mat[i, j] > 0:\n",
    "                    ax.text(j, i, str(mat[i, j]), ha='center', va='center', fontsize=7,\n",
    "                            color='white' if mat[i, j] > vmax * 0.6 else 'black')\n",
    "        plt.colorbar(im, ax=ax, label=\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"06_keyword_heatmap.png\", bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Peterson keywords found ‚Äî skipping heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6c96e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Side-by-Side Response Comparison\n",
    "\n",
    "Quantitative metrics capture aggregate behavior, but it's equally important to read the actual responses. Here we display the first three prompts answered by both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(3, len(EVAL_PROMPTS))):\n",
    "    print(\"‚îÅ\" * 90)\n",
    "    print(f\"PROMPT {i+1}: {EVAL_PROMPTS[i]}\")\n",
    "    print(\"‚îÅ\" * 90)\n",
    "    print(f\"\\nüîµ BASE MODEL:\")\n",
    "    print(base_responses[i]  if base_responses[i].strip()  else \"(empty response)\")\n",
    "    print(f\"\\nüü† FINE-TUNED MODEL:\")\n",
    "    print(tuned_responses[i] if tuned_responses[i].strip() else \"(empty response)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc83a9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 14: Radar Chart Summary\n",
    "\n",
    "A radar chart lets us compare both models across all five metrics simultaneously. All metrics are normalized to [0.1, 1.0] so that the area of each polygon represents overall \"Peterson-likeness\". A larger orange area = more domain-adapted model.\n",
    "\n",
    "All metrics are oriented so that **larger = more Peterson-like**:\n",
    "- Perplexity is inverted (lower perplexity ‚Üí higher radar score)\n",
    "- All other metrics are already in \"higher = better\" direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e55a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(bv, tv, higher_better=True):\n",
    "    lo, hi = min(bv, tv), max(bv, tv)\n",
    "    if abs(hi - lo) < 1e-9: return 0.5, 0.5\n",
    "    bn = (bv - lo) / (hi - lo)\n",
    "    tn = (tv - lo) / (hi - lo)\n",
    "    if not higher_better: bn, tn = 1 - bn, 1 - tn\n",
    "    return 0.1 + 0.9 * bn, 0.1 + 0.9 * tn\n",
    "\n",
    "avg_b_ppl = _avg(base_perplexities);  avg_t_ppl = _avg(tuned_perplexities)\n",
    "avg_b_sim = _avg(base_similarities);  avg_t_sim = _avg(tuned_similarities)\n",
    "avg_b_kd  = _avg(base_stats['keyword_density']); avg_t_kd = _avg(tuned_stats['keyword_density'])\n",
    "avg_b_ttr = _avg(base_stats['ttr_values']);       avg_t_ttr= _avg(tuned_stats['ttr_values'])\n",
    "avg_b_len = _avg(base_stats['word_counts']);      avg_t_len= _avg(tuned_stats['word_counts'])\n",
    "\n",
    "metrics = [\n",
    "    (\"Perplexity\\n(inverted)\",     *norm(avg_b_ppl, avg_t_ppl, higher_better=False)),\n",
    "    (\"TF-IDF\\nSimilarity\",         *norm(avg_b_sim, avg_t_sim, higher_better=True)),\n",
    "    (\"Keyword\\nDensity\",           *norm(avg_b_kd,  avg_t_kd,  higher_better=True)),\n",
    "    (\"Vocabulary\\nRichness (TTR)\", *norm(avg_b_ttr, avg_t_ttr, higher_better=True)),\n",
    "    (\"Response\\nLength\",           *norm(avg_b_len, avg_t_len, higher_better=True)),\n",
    "]\n",
    "labels  = [m[0] for m in metrics] + [metrics[0][0]]\n",
    "base_v  = [m[1] for m in metrics] + [metrics[0][1]]\n",
    "tuned_v = [m[2] for m in metrics] + [metrics[0][2]]\n",
    "angles  = np.linspace(0, 2 * np.pi, len(labels), endpoint=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "fig.suptitle(\"Radar Summary: How Peterson-Like is Each Qwen3-14B Model?\",\n",
    "             fontsize=15, fontweight='bold', y=1.01)\n",
    "\n",
    "ax.plot(angles, base_v,  color=BASE_COLOR,  lw=2.5, label=\"Base Model\")\n",
    "ax.fill(angles, base_v,  color=BASE_COLOR,  alpha=0.15)\n",
    "ax.plot(angles, tuned_v, color=TUNED_COLOR, lw=2.5, label=\"Fine-Tuned Model\")\n",
    "ax.fill(angles, tuned_v, color=TUNED_COLOR, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1]); ax.set_xticklabels(labels[:-1], size=11)\n",
    "ax.set_yticklabels([]); ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.25, 0.5, 0.75, 1.0])\n",
    "ax.set_yticklabels(['0.25', '0.5', '0.75', '1.0'], size=7, color='grey')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.15), fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"07_radar_summary.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d83efb",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 15: Final Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_change(bv, tv, higher_better=True):\n",
    "    if abs(bv) < 1e-9: return \"N/A\"\n",
    "    pct = 100 * (tv - bv) / bv\n",
    "    symbol = \"‚ñ≤\" if (pct > 0) == higher_better else \"‚ñº\"\n",
    "    ok     = \"‚úì\" if (pct > 0) == higher_better else \"‚úó\"\n",
    "    return f\"{pct:+.1f}% {symbol} {ok}\"\n",
    "\n",
    "rows = [\n",
    "    {\"Metric\": \"Avg Perplexity on Peterson text (‚Üì better)\",\n",
    "     \"Direction\": \"‚Üì\", \"Base\": f\"{avg_b_ppl:.2f}\", \"Fine-Tuned\": f\"{avg_t_ppl:.2f}\",\n",
    "     \"Change\": pct_change(avg_b_ppl, avg_t_ppl, higher_better=False)},\n",
    "    {\"Metric\": \"Avg TF-IDF Similarity to Peterson (‚Üë better)\",\n",
    "     \"Direction\": \"‚Üë\", \"Base\": f\"{avg_b_sim:.4f}\", \"Fine-Tuned\": f\"{avg_t_sim:.4f}\",\n",
    "     \"Change\": pct_change(avg_b_sim, avg_t_sim)},\n",
    "    {\"Metric\": \"Avg Keyword Density (‚Üë better)\",\n",
    "     \"Direction\": \"‚Üë\", \"Base\": f\"{100*avg_b_kd:.2f}%\", \"Fine-Tuned\": f\"{100*avg_t_kd:.2f}%\",\n",
    "     \"Change\": pct_change(avg_b_kd, avg_t_kd)},\n",
    "    {\"Metric\": \"Avg Type-Token Ratio (‚Üë better)\",\n",
    "     \"Direction\": \"‚Üë\", \"Base\": f\"{avg_b_ttr:.4f}\", \"Fine-Tuned\": f\"{avg_t_ttr:.4f}\",\n",
    "     \"Change\": pct_change(avg_b_ttr, avg_t_ttr)},\n",
    "    {\"Metric\": \"Avg Response Length (informational)\",\n",
    "     \"Direction\": \"‚Äî\", \"Base\": f\"{avg_b_len:.1f} words\", \"Fine-Tuned\": f\"{avg_t_len:.1f} words\",\n",
    "     \"Change\": f\"{avg_t_len - avg_b_len:+.1f} words\"},\n",
    "]\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"=\" * 95)\n",
    "print(\"FINAL COMPARISON SUMMARY  ‚Äî  Qwen3-14B  (Jordan Peterson Fine-Tuning)\")\n",
    "print(\"=\" * 95)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 95)\n",
    "print(\"\\n‚úì = improvement in expected direction  |  ‚úó = no improvement  |  ‚ñ≤/‚ñº = direction of change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc873e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 16: Saved Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab5169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saved figures:\")\n",
    "for f in sorted(FIGURES_DIR.iterdir()):\n",
    "    print(f\"  {f.name}  ({f.stat().st_size / 1024:.0f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a117f98",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "### Interpreting the Metrics for Qwen3-14B\n",
    "\n",
    "**Perplexity** is the most direct indicator of domain adaptation. After fine-tuning on ~768,000 words of Peterson's writing, the model should assign higher probability to his actual sentences ‚Äî producing lower perplexity. The magnitude of improvement reflects how thoroughly the 14B model has internalized his patterns in a single epoch.\n",
    "\n",
    "**TF-IDF Similarity** tests whether the model's responses use Peterson's *distinctive* vocabulary ‚Äî not just common words, but the specific terms (chaos, logos, archetype, sovereignty) that characterize his writing across all four books.\n",
    "\n",
    "**Keyword Density** provides the most direct check: does the model use Peterson's signature words when answering his central questions?\n",
    "\n",
    "**Type-Token Ratio** captures the richness of the model's vocabulary ‚Äî a property Peterson is known for in his elaborate, multi-layered prose.\n",
    "\n",
    "### Qwen3-14B vs. GPT-OSS 20B: Training Comparison\n",
    "\n",
    "| Metric | GPT-OSS 20B | Qwen3-14B |\n",
    "|--------|-------------|-----------|\n",
    "| Training loss | 3.01 | **2.44** |\n",
    "| Training time | 73.3 min | **23.3 min** |\n",
    "| Batch size | 1 | **2** |\n",
    "| Optimizer steps | 641 | **321** |\n",
    "\n",
    "The Qwen3-14B model achieved a **lower training loss** despite being a smaller model and training faster. This does not necessarily mean better inference quality ‚Äî perplexity and response metrics will show the true story.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **One epoch**: More epochs would deepen style adoption. Three epochs typically produces noticeably more Peterson-like outputs.\n",
    "- **Greedy decoding**: Deterministic generation for fair comparison. Sampling (`temperature=0.7`, `top_p=0.8`) as recommended by Qwen3 for chat mode would likely produce richer responses.\n",
    "- **Thinking mode not tested**: We use `enable_thinking=False` throughout for a clean apples-to-apples comparison. Enabling thinking at inference might produce substantively different (and potentially better) outputs for the fine-tuned model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (.finetuning)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
